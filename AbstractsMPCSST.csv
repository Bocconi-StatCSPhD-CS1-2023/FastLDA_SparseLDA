ABSTRACTS
"fundamental frequency (f0) approximation from polyphonic music includes a tasks of multiple-f0, melody, vocal, and bass line estimation. historically these problems have been approached separately, and only recently, with the help of learning-based approaches. we present the multitask deep learning architecture that jointly estimates outputs considering various tasks including multiple-f0, melody, vocal and bass line estimation, and was trained with the help of the large, semi-automatically annotated dataset. we show that a multitask model outperforms its single-task counterparts, and explore a effect of various design decisions inside our approach, and show that it performs better or at least competitively when compared against strong baseline methods."
"this large-scale study, consisting of 24.5 million hand hygiene opportunities spanning 19 distinct facilities inside 10 different states, uses linear predictive models to expose factors that may affect hand hygiene compliance. we examine a use of features such as temperature, relative humidity, influenza severity, day/night shift, federal holidays and a presence of new residents inside predicting daily hand hygiene compliance. a results suggest that colder temperatures and federal holidays have an adverse effect on hand hygiene compliance rates, and that individual cultures and attitudes regarding hand hygiene seem to exist among facilities."
"we present a stability analysis of the plane couette flow which was stably stratified inside a vertical direction orthogonally to a horizontal shear. interest inside such the flow comes from geophysical and astrophysical applications where background shear and vertical stable stratification commonly coexist. we perform a linear stability analysis of a flow inside the domain which was periodic inside a stream-wise and vertical directions and confined inside a cross-stream direction. a stability diagram was constructed as the function of a reynolds number re and a froude number fr, which compares a importance of shear and stratification. we find that a flow becomes unstable when shear and stratification are of a same order (i.e. fr $\sim$ 1) and above the moderate value of a reynolds number re$\gtrsim$700. a instability results from the resonance mechanism already known inside a context of channel flows, considering instance a unstratified plane couette flow inside a shallow water approximation. a result was confirmed by fully non linear direct numerical simulations and to a best of our knowledge, constitutes a first evidence of linear instability inside the vertically stratified plane couette flow. we also report a study of the laboratory flow generated by the transparent belt entrained by two vertical cylinders and immersed inside the tank filled with salty water linearly stratified inside density. we observe a emergence of the robust spatio-temporal pattern close to a threshold values of f r and re indicated by linear analysis, and explore a accessible part of a stability diagram. with a support of numerical simulations we conclude that a observed pattern was the signature of a same instability predicted by a linear theory, although slightly modified due to streamwise confinement."
"we construct finite time blow-up solutions to a 2-dimensional harmonic map flow into a sphere $s^2$, \begin{align*} u_t & = \delta u + |\nabla u|^2 u \quad \text{in } \omega\times(0,t) \\ u &= \varphi \quad \text{on } \partial \omega\times(0,t) \\ u(\cdot,0) &= u_0 \quad \text{in } \omega , \end{align*} where $\omega$ was the bounded, smooth domain inside $\mathbb{r}^2$, $u: \omega\times(0,t)\to s^2$, $u_0:\bar\omega \to s^2$ was smooth, and $\varphi = u_0\big|_{\partial\omega}$. given any points $q_1,\ldots, q_k$ inside a domain, we find initial and boundary data so that a solution blows-up precisely at those points. a profile around each point was close to an asymptotically singular scaling of the 1-corrotational harmonic map. we build the continuation after blow-up as the $h^1$-weak solution with the finite number of discontinuities inside space-time by ""reverse bubbling"", which preserves a homotopy class of a solution after blow-up."
"planetary nebulae (pne) constitute an important tool to study a chemical evolution of a milky way and other galaxies, probing a nucleosynthesis processes, abundance gradients and a chemical enrichment of a interstellar medium. inside particular, galactic bulge pne (gbpne) have been extensively used inside a literature to study a chemical properties of this galactic structure. however, a presently available gbpne chemical composition studies are strongly biased, since they were focused on brighter objects, predominantly located inside galactic regions of low interstellar reddening. inside this work, we report physical parameters and abundances derived considering the sample of 17 high extinction pne located inside a inner 2\degr of a galactic bulge, based on low dispersion spectroscopy secured at a soar telescope with the help of a goodman spectrograph. a new data allow us to extend our database including faint objects, providing chemical compositions considering pne located inside this region of a bulge and an approximation considering a masses of their progenitors to explore a chemical enrichment history of a central region of a galactic bulge. a results show that there was an enhancement inside a n/o abundance ratio inside a galactic centre pne compared with pne located inside a outer regions of a galactic bulge. this may indicate recent episodes of star formation occurring near a galactic centre."
"with a recent advancements inside artificial intelligence (ai), various organizations and individuals started debating about a progress of ai as the blessing or the curse considering a future of a society. this paper conducts an investigation on how a public perceives a progress of ai by utilizing a data shared on twitter. specifically, this paper performs the comparative analysis on a understanding of users from two categories -- general ai-tweeters (ait) and a expert ai-tweeters (eait) who share posts about ai on twitter. our analysis revealed that users from both a categories express distinct emotions and interests towards ai. users from both a categories regard ai as positive and are optimistic about a progress of ai but a experts are more negative than a general ai-tweeters. characterization of users manifested that `london' was a popular location of users from where they tweet about ai. tweets posted by ait are highly retweeted than posts made by eait that reveals greater diffusion of information from ait."
"model-based optimization methods and discriminative learning methods have been a two dominant strategies considering solving various inverse problems inside low-level vision. typically, those two kinds of methods have their respective merits and drawbacks, e.g., model-based optimization methods are flexible considering handling different inverse problems but are usually time-consuming with sophisticated priors considering a purpose of good performance; inside a meanwhile, discriminative learning methods have fast testing speed but their application range was greatly restricted by a specialized task. recent works have revealed that, with a aid of variable splitting techniques, denoiser prior should be plugged inside as the modular part of model-based optimization methods to solve other inverse problems (e.g., deblurring). such an integration induces considerable advantage when a denoiser was obtained using discriminative learning. however, a study of integration with fast discriminative denoiser prior was still lacking. to this end, this paper aims to train the set of fast and effective cnn (convolutional neural network) denoisers and integrate them into model-based optimization method to solve other inverse problems. experimental results demonstrate that a learned set of denoisers not only achieve promising gaussian denoising results but also should be used as prior to deliver good performance considering various low-level vision applications."
"inside a present work the new controller called particle swarm optimization based state feedback gain controller has been proposed considering frequency regulation of the two area system and then its performance was compared with earlier designed controllers such as linear quadratic regulator proportional integral controller and integral controller. a performance comparison has been done considering a power system network comprising of two thermal power plants which are tie line connected. considering with the help of a optimal control based method such as lqr pi controller and computationally intelligent method such as pso based state feedback gain controller, a state space modeling of a system has been done. transfer function model considering a system was used considering finding a response of integral controller. inside an effective generation control scheme a change inside frequency should be minimum during a load variation. a proposed pso based state feedback gain controller technique has been found most effective considering improving a frequency response."
"we study a problem of community detection inside hypergraphs under the stochastic block model. similarly to how a stochastic block model inside graphs suggests studying spiked random matrices, our model motivates investigating statistical and computational limits of exact recovery inside the certain spiked tensor model. inside contrast with a matrix case, a spiked model naturally arising from community detection inside hypergraphs was different from a one arising inside a so-called tensor principal component analysis model. we investigate a effectiveness of algorithms inside a sum-of-squares hierarchy on these models. interestingly, our results suggest that these two apparently similar models exhibit significantly different computational to statistical gaps."
"graphene nanoribbons (gnrs) are ultra-narrow strips of graphene that have a potential to be used inside high-performance graphene-based semiconductor electronics. however, controlled growth of gnrs on dielectric substrates remains the challenge. here, we report a successful growth of gnrs directly on hexagonal boron nitride substrates with smooth edges and controllable widths with the help of chemical vapour deposition. a idea behind the method was based on the type of template growth that allows considering a in-plane epitaxy of mono-layered gnrs inside nano-trenches on hexagonal boron nitride with edges following the zigzag direction. a embedded gnr channels show excellent electronic properties, even at room temperature. such in-plane hetero-integration of gnrs, which was compatible with integrated circuit processing, creates the gapped channel with the width of the few benzene rings, enabling a development of digital integrated circuitry based on gnrs."
"with a rise inside militant activity and rogue behaviour inside oil and gas regions around a world, oil pipeline disturbances was on a increase leading to huge losses to multinational operators and a countries where such facilities exist. however, this situation should be averted if adequate predictive monitoring schemes are put inside place. we propose inside a first part of this paper, an artificial intelligence predictive monitoring system capable of predictive classification and pattern recognition of pipeline datasets. a predictive system was based on the highly sparse predictive deviant learning algorithm (p-dla) designed to synthesize the sequence of memory predictive clusters considering eventual monitoring, control and decision making. a dla (p-dla) was compared with the popular machine learning algorithm, a long short-term memory (lstm) which was based on the temporal version of a standard feed-forward back-propagation trained artificial neural networks (anns). a results of simulations study show impressive results and validates a sparse memory predictive idea behind the method which favours a sub-synthesis of the highly compressed and low dimensional knowledge discovery and information prediction scheme. it also shows that a proposed new idea behind the method was competitive with the well-known and proven ai idea behind the method such as a lstm."
"inside magnetohydrodynamic (mhd) turbulence, a large-scale magnetic field sets the preferred local direction considering a small-scale dynamics, altering a statistics of turbulence from a isotropic case. this happens even inside a absence of the total magnetic flux, since mhd turbulence forms randomly oriented large-scale domains of strong magnetic field. it was therefore customary to study small-scale magnetic plasma turbulence by assuming the strong background magnetic field relative to a turbulent fluctuations. this was done, considering example, inside reduced models of plasmas, such as reduced mhd, reduced-dimension kinetic models, gyrokinetics, etc., which make theoretical calculations easier and numerical computations cheaper. recently, however, it has become clear that a turbulent energy dissipation was concentrated inside a regions of strong magnetic field variations. the significant fraction of a energy dissipation may be localized inside very small volumes corresponding to a boundaries between strongly magnetized domains. inside these regions a reduced models are not applicable. this has important implications considering studies of particle heating and acceleration inside magnetic plasma turbulence. a goal of this work was to systematically investigate a relationship between local magnetic field variations and magnetic energy dissipation, and to understand its implications considering modeling energy dissipation inside realistic turbulent plasmas."
"the core challenge inside a analysis of experimental data was that a impact of some intervention was often not entirely captured by the single, well-defined outcome. instead there may be the large number of outcome variables that are potentially affected and of interest. inside this paper, we propose the data-driven idea behind the method rooted inside machine learning to a problem of testing effects on such groups of outcome variables. it was based on two simple observations. first, a 'false-positive' problem that the group of outcomes was similar to a concern of 'over-fitting,' which has been a focus of the large literature inside statistics and computer science. we should thus leverage sample-splitting methods from a machine-learning playbook that are designed to control over-fitting to ensure that statistical models express generalizable insights about treatment effects. a second simple observation was that a question whether treatment affects the group of variables was equivalent to a question whether treatment was predictable from these variables better than some trivial benchmark (provided treatment was assigned randomly). this formulation allows us to leverage data-driven predictors from a machine-learning literature to flexibly mine considering effects, rather than rely on more rigid approaches like multiple-testing corrections and pre-analysis plans. we formulate the specific methodology and present three kinds of results: first, our test was exactly sized considering a null hypothesis of no effect; second, the specific version was asymptotically equivalent to the benchmark joint wald test inside the linear regression; and third, this methodology should guide inference on where an intervention has effects. finally, we argue that our idea behind the method should naturally deal with typical features of real-world experiments, and be adapted to baseline balance checks."
"(bedt-tff)$_2$i$_3$ charge transfer salts are reported to show superconductivity and pressure induced quasi two-dimensional dirac cones at a fermi level. by performing state of a art ab initio calculations inside a framework of density functional theory, we investigate a structural and electronic properties of a three structural phases $\alpha$, $\beta$ and $\kappa$. \edit{we furthermore report about a irreducible representations of a corresponding electronic band structures, symmetry of their crystal structure, and discuss a origin of band crossings. additionally, we discuss a chemically induced strain inside $\kappa$-(bedt-ttf)$_2$i$_3$ achieved by replacing a iodine layer with other halogens: fluorine, bromine and chlorine. inside a case of $\kappa$-(bedt-ttf)$_2$f$_3$, we identify topologically protected crossings within a band structure. these crossings are forced to occur due to a non-symmorphic nature of a crystal.} a calculated electronic structures presented here are added to a organic materials database (omdb)."
"a purpose of this paper was to investigate a asymptotic behavior of automorphism groups of function fields when genus tends to infinity. motivated by applications inside coding and cryptography, we consider a maximum size of abelian subgroups of a automorphism group $\mbox{aut}(f/\mathbb{f}_q)$ inside terms of genus ${g_f}$ considering the function field $f$ over the finite field $\mathbb{f}_q$. although a whole group $\mbox{aut}(f/\mathbb{f}_q)$ could have size $\omega({g_f}^4)$, a maximum size $m_f$ of abelian subgroups of a automorphism group $\mbox{aut}(f/\mathbb{f}_q)$ was upper bounded by $4g_f+4$ considering $g_f\ge 2$. inside a present paper, we study a asymptotic behavior of $m_f$ by defining $m_q=\limsup_{{g_f}\rightarrow\infty}\frac{m_f \cdot \log_q m_f}{g_f}$, where $f$ runs through all function fields over $\mathbb{f}_q$. we show that $m_q$ lies between $2$ and $3$ (or $4$) considering odd characteristic (or considering even characteristic, respectively). this means that $m_f$ grows much more slowly than genus does asymptotically. a second part of this paper was to study a maximum size $b_f$ of subgroups of $\mbox{aut}(f/\mathbb{f}_q)$ whose order was coprime to $q$. a hurwitz bound gives an upper bound $b_f\le 84(g_f-1)$ considering every function field $f/\mathbb{f}_q$ of genus $g_f\ge 2$. we investigate a asymptotic behavior of $b_f$ by defining ${b_q}=\limsup_{{g_f}\rightarrow\infty}\frac{b_f}{g_f}$, where $f$ runs through all function fields over $\mathbb{f}_q$. although a hurwitz bound shows ${b_q}\le 84$, there are no lower bounds on $b_q$ inside literature. one does not even know if ${b_q}=0$. considering a first time, we show that ${b_q}\ge 2/3$ by explicitly constructing some towers of function fields inside this paper."
"different combinations of input parameters to filament identification algorithms, such as disperse and filfinder, produce numerous different output skeletons. a skeletons are the one pixel wide representation of a filamentary structure inside a original input image. however, these output skeletons may not necessarily be the good representation of that structure. furthermore, the given skeleton may not be as good the representation as another. previously there has been no mathematical `goodness-of-fit' measure to compare output skeletons to a input image. thus far this has been assessed visually, introducing visual bias. we propose a application of a mean structural similarity index (mssim) as the mathematical goodness-of-fit measure. we describe a use of a mssim to find a output skeletons most mathematically similar to a original input image (the optimum, or `best', skeletons) considering the given algorithm, and independently of a algorithm. this measure makes possible systematic parameter studies, aimed at finding a subset of input parameter values returning optimum skeletons. it should also be applied to a output of non-skeleton based filament identification algorithms, such as a hessian matrix method. a mssim removes a need to visually examine thousands of output skeletons, and eliminates a visual bias, subjectivity, and limited reproducibility inherent inside that process, representing the major improvement on existing techniques. importantly, it also allows further automation inside a post-processing of output skeletons, which was crucial inside this era of `big data'."
"purpose: a facial recess was the delicate structure that must be protected inside minimally invasive cochlear implant surgery. current research estimates a drill trajectory by with the help of endoscopy of a unique mastoid patterns. however, missing depth information limits available features considering the registration to preoperative ct data. therefore, this paper evaluates oct considering enhanced imaging of drill holes inside mastoid bone and compares oct data to original endoscopic images. methods: the catheter-based oct probe was inserted into the drill trajectory of the mastoid phantom inside the translation-rotation manner to acquire a inner surface state. a images are undistorted and stitched to create volumentric data of a drill hole. a mastoid cell pattern was segmented automatically and compared to ground truth. results: a mastoid pattern segmented on images acquired with oct show the similarity of j = 73.6 % to ground truth based on endoscopic images and measured with a jaccard metric. leveraged by additional depth information, automated segmentation tends to be more robust and fail-safe compared to endoscopic images. conclusion: a feasibility of with the help of the clinically approved oct probe considering imaging a drill hole inside cochlear implantation was shown. a resulting volumentric images provide additional information on a shape of caveties inside a bone structure, which will be useful considering image-to-patient registration and to approximate a drill trajectory. this will be another step towards safe minimally invasive cochlear implantation."
"inside this paper we study a distance-based docking problem of unmanned aerial vehicles (uavs) by with the help of the single landmark placed at an arbitrarily unknown position. to solve a problem, we propose an integrated estimation-control scheme to simultaneously achieve a relative localization and navigation tasks considering discrete-time integrators under bounded velocity: the nonlinear adaptive approximation scheme to approximate a relative position to a landmark, and the delicate control scheme to ensure both a convergence of a approximation and a asymptotic docking at a given landmark. the rigorous proof of convergence was provided by invoking a discrete-time lasalle's invariance principle, and we also validate our theoretical findings on quadcopters equipped with ultra-wideband ranging sensors and optical flow sensors inside the gps-less environment."
"helices of increased electron density should spontaneously form inside materials containing multiple, interacting density waves. although the macroscopic order parameter theory describing this behaviour has been proposed and experimentally tested, the detailed microscopic understanding of spiral electronic order inside any particular material was still lacking. here, we present a elemental chalcogens selenium and tellurium as model materials considering a development of chiral charge and orbital order. we formulate minimal models capturing a formation of spiral structures both inside terms of the macroscopic landau theory and the microscopic hamiltonian. both reproduce a known chiral crystal structure and are consistent with its observed thermal evolution and behaviour under applied pressure. a combination of microscopic and macroscopic frameworks allows us to distil a essential ingredients inside a emergence of helical charge order, and may serve as the guide to understanding spontaneous chirality both inside other specific materials and throughout materials classes."
"recent experiments revealed the striking asymmetry inside a phase diagram of a high temperature cuprate superconductors. a correlation effect seems strong inside a hole-doped systems and weak inside a electron-doped systems. on a other hand, the recent theoretical study shows that a interaction strengths (the hubbard u) are comparable inside these systems. therefore, it was difficult to explain this asymmetry by their interaction strengths. given this background, we analyze a one-particle spectrum of the single band model of the cuprate superconductor near a fermi level with the help of a dynamical mean field theory. we find a difference inside a ""visibility"" of a strong correlation effect between a hole- and electron-doped systems. this should explain a electron-hole asymmetry of a correlation strength without introducing a difference inside a interaction strength."
this paper focuses on detecting anomalies inside the digital video broadcasting (dvb) system from providers' perspective. we learn the probabilistic deterministic real timed automaton profiling benign behavior of encryption control inside a dvb control access system. this profile was used as the one-class classifier. anomalous items inside the testing sequence are detected when a sequence was not accepted by a learned model.
"this paper proposes the convolutional neural network (cnn)-based method that learns traffic as images and predicts large-scale, network-wide traffic speed with the high accuracy. spatiotemporal traffic dynamics are converted to images describing a time and space relations of traffic flow using the two-dimensional time-space matrix. the cnn was applied to a image following two consecutive steps: abstract traffic feature extraction and network-wide traffic speed prediction. a effectiveness of a proposed method was evaluated by taking two real-world transportation networks, a second ring road and north-east transportation network inside beijing, as examples, and comparing a method with four prevailing algorithms, namely, ordinary least squares, k-nearest neighbors, artificial neural network, and random forest, and three deep learning architectures, namely, stacked autoencoder, recurrent neural network, and long-short-term memory network. a results show that a proposed method outperforms other algorithms by an average accuracy improvement of 42.91% within an acceptable execution time. a cnn should train a model inside the reasonable time and, thus, was suitable considering large-scale transportation networks."
"inside this paper, we consider a usual linear regression model inside a case where a error process was assumed strictly stationary. we use the result from hannan, who proved the central limit theorem considering a usual least square estimator under general conditions on a design and on a error process. we show that considering the large class of designs, a asymptotic covariance matrix was as simple as a i.i.d. case. we then approximate a covariance matrix with the help of an estimator of a spectral density whose consistency was proved under very mild conditions. as an application, we show how to modify a usual fisher tests inside this dependent context, inside such the way that a type-$i$ error rate remains asymptotically correct, and we illustrate a performance of this procedure through different sets of simulations."
"the new technique was presented considering solving a problem of enforcing control limits inside power flow studies. as an added benefit, it greatly increases a achievable precision at nose points. a method was exemplified considering a case of mvar limits inside generators regulating voltage on both local and remote buses. based on a framework of a holomorphic embedding loadflow method (helm), it provides the rigorous solution to this fundamental problem by framing it inside terms of \emph{optimization}. the novel lagrangian formulation of power-flow, which was exact considering lossless networks, leads to the natural physics-based minimization criterion that yields a correct solution. considering networks with small losses, as was a case inside transmission, a ac power flow problem cannot be framed exactly inside terms of optimization, but a criterion still retains its ability to select a correct solution. this foundation then provides the way to design the helm scheme to solve considering a minimizing solution. although a use of barrier functions evokes interior point optimization, this method, like helm, was based on a analytic continuation of the germ (of the particular branch) of a algebraic curve representing a solutions of a system. inside this case, since a constraint equations given by limits result inside an unavoidable singularity at $s=1$, direct analytic continuation by means of standard padé approximation was fraught with numerical instabilities. this has been overcome by means of the new analytic continuation procedure, denominated padé-weierstrass, that exploits a covariant nature of a power flow equations under certain changes of variables. one colateral benefit of this procedure was that it should also be used when limits are not being enforced, inside order to increase a achievable numerical precision inside highly stressed cases."
"this comprehensive study of comet c/1995 o1 focuses first on investigating its orbital motion over the period of 17.6 yr (1993-2010). a comet was suggested to have approached jupiter to 0.005 au on -2251 november 7, inside general conformity with marsden's (1999) proposal of the jovian encounter nearly 4300 yr ago. a variations of sizable nongravitational effects with heliocentric distance correlate with a evolution of outgassing, asymmetric relative to perihelion. a future orbital period will shorten to ~1000 yr because of orbital-cascade resonance effects. we find that a sublimation curves of parent molecules are fitted with a type of the law used considering a nongravitational acceleration, determine their orbit-integrated mass loss, and conclude that a share of water ice is at most 57%, and possibly less than 50%, of a total outgassed mass. even though organic parent molecules (many still unidentified) had very low abundances relative to water individually, their high molar mass and sheer number made them, summarily, important potential mass contributors to a total production of gas. a mass loss of dust per orbit exceeded that of water ice by the factor of ~12, the dust loading high enough to imply the major role considering heavy organic molecules of low volatility inside accelerating a minuscule dust particles inside a expanding halos to terminal velocities as high as 0.7 km s^{-1}. inside part ii, a comet's nucleus will be modeled as the compact cluster of massive fragments to conform to a integrated nongravitational effect."
"a main objective of this paper was to address a instability and dynamical bifurcation of a dean problem. the nonlinear theory was obtained considering a dean problem, leading inside particular to rigorous justifications of a linear theory used by physicists, and a vortex structure. a main technical tools are a dynamic bifurcation theory [15] developed recently by ma and wang."
"we consider a weight spectrum of the class of quasi-perfect binary linear codes with code distance 4. considering example, extended hamming code and panchenko code are a known members of this class. also, it was known that inside many cases panchenko code has a minimal number of weight 4 codewords. we give exact recursive formulas considering a weight spectrum of quasi-perfect codes and their dual codes. as an example of application of a weight spectrum we derive the lower approximate considering a conditional probability of correction of erasure patterns of high weights (equal to or greater than code distance)."
"superconductivity has been a focus of enormous research effort since its discovery more than the century ago. yet, some features of this unique phenomenon remain poorly understood; prime among these was a connection between superconductivity and chemical/structural properties of materials. to bridge a gap, several machine learning schemes are developed herein to model a critical temperatures ($t_{\mathrm{c}}$) of a 12,000+ known superconductors available using a supercon database. materials are first divided into two classes based on their $t_{\mathrm{c}}$ values, above and below 10 k, and the classification model predicting this label was trained. a model uses coarse-grained features based only on a chemical compositions. it shows strong predictive power, with out-of-sample accuracy of about 92%. separate regression models are developed to predict a values of $t_{\mathrm{c}}$ considering cuprate, iron-based, and ""low-$t_{\mathrm{c}}$"" compounds. these models also demonstrate good performance, with learned predictors offering potential insights into a mechanisms behind superconductivity inside different families of materials. to improve a accuracy and interpretability of these models, new features are incorporated with the help of materials data from a aflow online repositories. finally, a classification and regression models are combined into the single integrated pipeline and employed to search a entire inorganic crystallographic structure database (icsd) considering potential new superconductors. we identify more than 30 non-cuprate and non-iron-based oxides as candidate materials."
"throughout a processing and analysis of survey data, the ubiquitous issue nowadays was that we are spoilt considering choice when we need to select the methodology considering some of its steps. a alternative methods usually fail and excel inside different data regions, and have various advantages and drawbacks, so the combination that unites a strengths of all while suppressing a weaknesses was desirable. we propose to use the two-level hierarchy of learners. its first level consists of training and applying a possible base methods on a first part of the known set. at a second level, we feed a output probability distributions from all base methods to the second learner trained on a remaining known objects. with the help of classification of variable stars and photometric redshift approximation as examples, we show that a hierarchical combination was capable of achieving general improvement over averaging-type combination methods, correcting systematics present inside all base methods, was easy to train and apply, and thus, it was the promising tool inside a astronomical ""big data"" era."
"vector quantization aims to form new vectors/matrices with shared values close to a original. it could compress data with acceptable information loss and could be of great usefulness inside areas like image processing, pattern recognition, and machine learning. inside this paper, a problem of vector quantization was examined from the new perspective, namely sparse least square optimization. specifically, inspired by a property of sparsity of lasso, the novel quantization algorithm based on $l_1$ least square was proposed and implemented. similar schemes with $l_1 + l_2$ combination penalization and $l_0$ regularization are simultaneously proposed. inside addition, to produce quantization results with given amount of quantized values(instead of penalization coefficient $\lambda$), this paper proposed an iterative sparse least square method and the cluster-based least square quantization method. it was also noticed that a later method was mathematically equivalent to an improved version of a existed clustering-based quantization algorithm, although a two algorithms originated from different intuitions. a algorithms proposed were tested under three scenarios of data and their computational performance, including information loss, time consumption and a distribution of a value of sparse vectors were compared and analyzed. a paper offers the new perspective to probe a area of vector quantization, and a algorithms proposed could offer better performance especially when a required post-quantization value amounts are not on the tiny scale."
"mechanistic modelling of animal movement was often formulated inside discrete time despite problems with scale invariance, such as handling irregularly timed observations. the natural solution was to formulate inside continuous time, yet uptake of this has been slow. this lack of implementation was often excused by the difficulty inside interpretation. here we aim to bolster usage by developing the continuous-time model with interpretable parameters, similar to those of popular discrete-time models that use turning angles and step lengths. movement was defined by the joint bearing and speed process, with parameters dependent on the continuous-time behavioural switching process, creating the flexible class of movement models. methodology was presented considering markov chain monte carlo inference given irregular observations, involving augmenting observed locations with the reconstruction of a underlying movement process. this was applied to well known gps data from elk (\emph{cervus elaphus}), which have previously been modelled inside discrete time. we demonstrate a interpretable nature of a continuous-time model, finding clear differences inside behaviour over time and insights into short term behaviour that could not have been obtained inside discrete time."
"this paper discusses a phenomenon of backreaction within a szekeres model. cosmological backreaction describes how a mean global evolution of a universe deviates from a friedmannian evolution. a analysis was based on models of the single cosmological environment and a global ensemble of a szekeres models (of a swiss-cheese-type and styrofoam-type). a obtained results show that non-linear growth of cosmic structures was associated with a growth of a spatial curvature $\omega_{\cal r}$ (in a flrw limit $\omega_{\cal r} \to \omega_k$). if averaged over global scales a result depends on a assumed global model of a universe. within a swiss-cheese model, which does have the fixed background, a volume average follows a evolution of a background, and a global spatial curvature averages out to zero (the background model was a $\lambda$cdm model, which was spatially flat). inside a styrofoam-type model, which does not have the fixed background, a mean evolution deviates from a spatially flat $\lambda$cdm model, and a mean spatial curvature evolves to from $\omega_{\cal r} =0 $ at a cmb to $\omega_{\cal r} \sim 0.1$ at $z =0$. if a styrofoam-type model correctly captures evolutionary features of a real universe then one should expect that inside our universe, a spatial curvature should build up (local growth of cosmic structures) and its mean global average should deviate from zero (backreaction). as the result, this paper predicts that a low-redshift universe should not be spatially flat (i.e. $\omega_k \ne 0$, even if inside a early universe $\omega_k = 0$) and therefore when analysing low-$z$ cosmological data one should keep $\omega_k$ as the free parameter and independent from a cmb constraints."
"inside this paper, we employ asymptotic analysis to determine information about small volume defects inside the known anisotropic scattering medium from far field scattering data. a location of a defects was reconstructed using a music algorithm from a range of a multi-static response matrix derived from a asymptotic expansion of a far field pattern inside a presence of small defects. since a same data determines a transmission eigenvalues corre- sponding to a perturbed media, we investigate how a presence of a defects changes a transmission eigenvalues and use this information to recover a strength of a small defects. we provide convergence results on transmission eigenvalues as a size of a defects tends to zero as well as derive a first correction term inside a asymptotic expansion of a simple transmission eigen- values. numerical examples are presented to show a viability of our imaging method."
"we present here the population structured model to describe a dynamics of macrophage cells. a model involves a interactions between modified ldl, monocytes/macrophages, cytokines and foam cells. a key assumption was that a individual macrophage dynamics depends on a amount of lipoproteins it has internalized. a obtained renewal equation was coupled with an ode describing a lipoprotein dynamics. we first prove global existence and uniqueness considering a nonlinear and nonlocal system. we then study long time asymp-totics inside the particular case describing silent plaques which undergo periodic rupture and repair. finally we study long time asymptotics considering a nonlinear renewal equation obtained when considering a steady state of a ode. and we prove that ...."
"this paper presents an explicit construction considering an $((n=2qt,k=2q(t-1),d=n-(q+1)), (\alpha = q(2q)^{t-1},\beta = \frac{\alpha}{q}))$ regenerating code over the field $\mathbb{f}_q$ operating at a minimum storage regeneration (msr) point. a msr code should be constructed to have rate $k/n$ as close to $1$ as desired, sub-packetization level $\alpha \leq r^{\frac{n}{r}}$ considering $r=(n-k)$, field size $q$ no larger than $n$ and where all code symbols should be repaired with a same minimum data download. this was a first-known construction of such an msr code considering $d<(n-1)$."
"structural properties of lacu$_{6-x}$ag$_{x}$ have been investigated with the help of neutron and x-ray diffraction, and resonant ultrasound spectroscopy (rus) measurements. diffraction measurements indicate the continuous structural transition from orthorhombic ($pnma$) to monoclinic ($p2_1/c$) structure. rus measurements show softening of natural frequencies at a structural transition, consistent with a elastic nature of a structural ground state. a structural transition temperatures inside lacu$_{6-x}$ag$_{x}$ decrease with ag composition until a monoclinic phase was completely suppressed at $x_c$ = 0.225. all of a evidence was consistent with a presence of an elastic quantum critical point inside lacu$_{6-x}$ag$_{x}$."
"we consider a problem of the robot learning a mechanical properties of objects through physical interaction with a object, and introduce the practical, data-efficient idea behind the method considering identifying a motion models of these objects. a proposed method utilizes the physics engine, where a robot seeks to identify a inertial and friction parameters of a object by simulating its motion under different values of a parameters and identifying those that result inside the simulation which matches a observed real motions. a problem was solved inside the bayesian optimization framework. a same framework was used considering both identifying a model of an object online and searching considering the policy that would minimize the given cost function according to a identified model. experimental results both inside simulation and with the help of the real robot indicate that a proposed method outperforms state-of-the-art model-free reinforcement learning approaches."
"the spacetime denotes the pure radiation field if its energy momentum tensor represents the situation inside which all a energy was transported inside one direction with a speed of light. inside 1989, wils and later inside 1997 ludwig and edgar studied a physical properties of pure radiation metrics, which are conformally related to the vacuum spacetime. inside a present paper we investigate a curvature properties of special type of pure radiation metrics presented by ludwig and edgar. it was shown that such the pure radiation spacetime was semisymmetric, ricci simple, $r$-space by venzi and its ricci tensor was riemann compatible. it was also proved that its conformal curvature 2-forms and ricci 1-forms are recurrent. we also present the pure radiation type metric and evaluate its curvature properties along with a form of its energy momentum tensor. it was interesting to note that such pure radiation type metric was $ein(3)$ and 3-quasi-einstein. we also find out a sufficient conditions considering which this metric represents the generalized pp-wave, pure radiation and perfect fluid. finally we made the comparison between a curvature properties of ludwig and edgar's pure radiation metric and pp-wave metrics."
"inside the recent letter, j. cardy, phys. rev. lett. \textbf{112}, 220401 (2014), a author made the very interesting observation that complete revivals of quantum states after quantum quench should happen inside the period which was the fraction of a system size. this was possible considering critical systems that should be described by minimal conformal field theories (cft) with central charge $c<1$. inside this article, we show that these complete revivals are impossible inside microscopic realizations of those minimal models. we will prove a absence of a mentioned complete revivals considering a critical transverse field ising chain analytically, and present numerical results considering a critical line of a xy chain. inside particular, considering a considered initial states, we will show that criticality has no significant effect inside partial revivals. we also comment on a applicability of quasi-particle picture to determine a period of a partial revivals qualitatively. inside particular, we detect the regime inside a phase diagram of a xy chain which one should not determine a period of a partial revivals with the help of a quasi-particle picture."
"we address a problem of algorithmic fairness: ensuring that sensitive variables do not unfairly influence a outcome of the classifier. we present an idea behind the method based on empirical risk minimization, which incorporates the fairness constraint into a learning problem. it encourages a conditional risk of a learned classifier to be approximately constant with respect to a sensitive variable. we derive both risk and fairness bounds that support a statistical consistency of our approach. we specify our idea behind the method to kernel methods and observe that a fairness requirement implies an orthogonality constraint which should be easily added to these methods. we further observe that considering linear models a constraint translates into the simple data preprocessing step. experiments indicate that a method was empirically effective and performs favorably against state-of-the-art approaches."
a effect of a canting of local anisotropy axes on a ground-state phase diagram and magnetization of the ferrimagnetic chain with regularly alternating ising and heisenberg spins was exactly examined inside an arbitrarily oriented magnetic field. it was shown that individual contributions of ising and heisenberg spins to a total magnetization basically depend on a spatial orientation of a magnetic field and a canting angle between two different local anisotropy axes of a ising spins.
"community structures are critical towards understanding not only a network topology but also how a network functions. however, how to evaluate a quality of detected community structures was still challenging and remains unsolved. a most widely used metric, normalized mutual information (nmi), is proved to have finite size effect, and its improved form rnmi has reverse finite size effect. cnmi was thus proposed and has neither finite size effect nor reverse finite size effect. however, inside this paper we show that cnmi violates a so-called proportionality assumption. inside addition, nmi-type metrics have a problem of ignoring importance of small communities. finally, they cannot be used to evaluate the single community of interest. inside this paper, we map a computed community labels to a ground-truth ones through integer linear programming, then use kappa index and f-score to evaluate a detected community structures. experimental results demonstrate a rationality of our method."
a object of a present paper was to study invariant submanifolds of (lcs)n-manifolds with respect to quarter symmetric metric connection. it was shown that a mean curvature of an invariant submanifold of (lcs)n-manifold with respect to quarter symmetric metric connection and levi-civita connection are equal. an example was constructed to illustrate a results of a paper. we also obtain some equivalent conditions of such notion.
"this dissertation was motivated by a need, inside today's globalist world, considering the precise way to enable governments, organisations and other regulatory bodies to evaluate a constraints they place on themselves and others. an organisation's modus operandi was enacting and fulfilling contracts between itself and its participants. yet, organisational contracts should respect external laws, such as those setting out data privacy rights and liberties. contracts should only be enacted by following contract law processes, which often require bilateral agreement and consideration. governments need to legislate whilst understanding today's context of national and international governance hierarchy where law makers shun isolationism and seek to influence one another. governments should avoid punishment by respecting constraints from international treaties and human rights charters. governments should only enact legislation by following their own, pre-existing, law making procedures. inside other words, institutions, such as laws and contracts are designed and enacted under constraints."
"inside this paper, we derive a second order approximate to a $2$-nd hessian type equation on the compact almost hermitian manifold."
"surface-assisted polymerization of molecular monomers into extended chains should be used as a seed of graphene nanoribbon (gnr) formation, resulting from the subsequent cyclo-dehydrogenation process. by means of valence-band photoemission and ab-initio density-functional theory (dft) calculations, we investigate a evolution of molecular states from monomer 10,10'-dibromo-9,9'bianthracene (dbba) precursors to polyanthryl polymers, and eventually to gnrs, as driven by a au(110) surface. a molecular orbitals and a energy level alignment at a metal-organic interface are studied inside depth considering a dbba precursors deposited at room temperature. on this basis, we should identify the spectral fingerprint of c-au interaction inside both dbba single-layer and polymerized chains obtained upon heating. furthermore, dft calculations aid us evidencing that gnrs interact more strongly than dbba and polyanthryl with a au(110) substrate, as the result of their flatter conformation."
"among asteroids there exist ambiguities inside their rotation period determinations. they are due to incomplete coverage of a rotation, noise and/or aliases resulting from gaps between separate lightcurves. to aid to remove such uncertainties, basic characteristic of a lightcurves resulting from constraints imposed by a asteroid shapes and geometries of observations should be identified. we simulated light variations of asteroids which shapes were modelled as gaussian random spheres, with random orientations of spin vectors and phase angles changed every $5^\circ$ from $0^\circ$ to $65^\circ$. this produced 1.4 mln lightcurves. considering each simulated lightcurve fourier analysis has been made and a harmonic of a highest amplitude is recorded. from a statistical point of view, all lightcurves observed at phase angles $\alpha < 30^\circ$, with peak-to-peak amplitudes $a>0.2$ mag are bimodal. second most frequently dominating harmonic was a first one, with a 3rd harmonic following right after. considering 1% of lightcurves with amplitudes $a < 0.1$ mag and phase angles $\alpha < 40^\circ$ 4th harmonic dominates."
"this paper focuses on best-arm identification inside multi-armed bandits with bounded rewards. we develop an algorithm that was the fusion of lil-ucb and kl-lucb, offering a best qualities of a two algorithms inside one method. this was achieved by proving the novel anytime confidence bound considering a mean of bounded distributions, which was a analogue of a lil-type bounds recently developed considering sub-gaussian distributions. we corroborate our theoretical results with numerical experiments based on a new yorker cartoon caption contest."
"lung nodule classification was the class imbalanced problem because nodules are found with much lower frequency than non-nodules. inside a class imbalanced problem, conventional classifiers tend to be overwhelmed by a majority class and ignore a minority class. we therefore propose cascaded convolutional neural networks to cope with a class imbalanced problem. inside a proposed approach, multi-stage convolutional neural networks that perform as single-sided classifiers filter out obvious non-nodules. successively, the convolutional neural network trained with the balanced data set calculates nodule probabilities. a proposed method achieved a sensitivity of 92.4\% and 94.5% at 4 and 8 false positives per scan inside free receiver operating characteristics (froc) curve analysis, respectively."
"training generative adversarial networks was unstable inside high-dimensions as a true data distribution tends to be concentrated inside the small fraction of a ambient space. a discriminator was then quickly able to classify nearly all generated samples as fake, leaving a generator without meaningful gradients and causing it to deteriorate after the point inside training. inside this work, we propose training the single generator simultaneously against an array of discriminators, each of which looks at the different random low-dimensional projection of a data. individual discriminators, now provided with restricted views of a input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to a generator throughout training. meanwhile, a generator learns to produce samples consistent with a full data distribution to satisfy all discriminators simultaneously. we demonstrate a practical utility of this idea behind the method experimentally, and show that it was able to produce image samples with higher quality than traditional training with the single discriminator."
"word embeddings have been found to capture the surprisingly rich amount of syntactic and semantic knowledge. however, it was not yet sufficiently well-understood how a relational knowledge that was implicitly encoded inside word embeddings should be extracted inside the reliable way. inside this paper, we propose two probabilistic models to address this issue. a first model was based on a common relations-as-translations view, but was cast inside the probabilistic setting. our second model was based on a much weaker assumption that there was the linear relationship between a vector representations of related words. compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what should and cannot be extracted from a word embedding."
"we present the simple method to incorporate syntactic information about a target language inside the neural machine translation system by translating into linearized, lexicalized constituency trees. an experiment on a wmt16 german-english news translation task resulted inside an improved bleu score when compared to the syntax-agnostic nmt baseline trained on a same dataset. an analysis of a translations from a syntax-aware system shows that it performs more reordering during translation inside comparison to a baseline. the small-scale human evaluation also showed an advantage to a syntax-aware system."
"data-target pairing was an important step towards multi-target localization considering a intelligent operation of unmanned systems. target localization plays the crucial role inside numerous applications, such as search, and rescue missions, traffic management and surveillance. a objective of this paper was to present an innovative target location learning approach, where numerous machine learning approaches, including k-means clustering and supported vector machines (svm), are used to learn a data pattern across the list of spatially distributed sensors. to enable a accurate data association from different sensors considering accurate target localization, appropriate data pre-processing was essential, which was then followed by a application of different machine learning algorithms to appropriately group data from different sensors considering a accurate localization of multiple targets. through simulation examples, a performance of these machine learning algorithms was quantified and compared."
"we present a detection of four far-infrared fine-structure oxygen lines, as well as strong upper limits considering a co(2-1) and [n ii] 205 um lines, inside 3c 368, the well-studied radio-loud galaxy at z = 1.131. these new oxygen lines, taken inside conjunction with previously observed neon and carbon fine-structure lines, suggest the powerful active galactic nucleus (agn), accompanied by vigorous and extended star formation. the starburst dominated by o8 stars, with an age of ~6.5 myr, provides the good fit to a fine-structure line data. this estimated age of a starburst makes it nearly concurrent with a latest episode of agn activity, suggesting the link between a growth of a supermassive black hole and stellar population inside this source. we do not detect a co(2-1) line, down to the level twelve times lower than a expected value considering star forming galaxies. this lack of co line emission was consistent with recent star formation activity if a star-forming molecular gas has low metallicity, was highly fractionated (such that co was photodissociated through much of a clouds), or was chemically very young (such that co has not yet had time to form). it was also possible, though we argue unlikely, that a ensemble of fine structure lines are emitted from a region heated by a agn."
"regularization occurs when a output the learner produces was less variable than a linguistic data they observed. inside an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias inside cognition: the domain-general source based on cognitive load and the domain-specific source triggered by linguistic stimuli. both of these factors modulate how frequency information was encoded and produced, but only a production-side modulations result inside regularization (i.e. cause learners to eliminate variation from a observed input). we formalize a definition of regularization as a reduction of entropy and find that entropy measures are better at identifying regularization behavior than frequency-based analyses. with the help of our experimental data and the model of cultural transmission, we generate predictions considering a amount of regularity that would develop inside each experimental condition if a artificial language were transmitted over several generations of learners. here we find that a effect of cognitive constraints should become more complex when put into a context of cultural evolution: although learning biases certainly carry information about a course of language evolution, we should not expect the one-to-one correspondence between a micro-level processes that regularize linguistic datasets and a macro-level evolution of linguistic regularity."
"efficient stochastic simulation algorithms are of paramount importance to a study of spreading phenomena on complex networks. with the help of insights and analytical results from network science, we discuss how a structure of contacts affects a efficiency of current algorithms. we show that algorithms believed to require $\mathcal{o}(\log n)$ or even $\mathcal{o}(1)$ operations per update---where $n$ was a number of nodes---display instead the polynomial scaling considering networks that are either dense or sparse and heterogeneous. this significantly affects a required computation time considering simulations on large networks. to circumvent a issue, we propose the node-based method combined with the composition and rejection algorithm, the sampling scheme that has an average-case complexity of $\mathcal{o} [\log(\log n)]$ per update considering general networks. this systematic idea behind the method was first set-up considering markovian dynamics, but should also be adapted to the number of non-markovian processes and should enhance considerably a study of the wide range of dynamics on networks."
"this article was concerned with causal structures, which are defined as the field of tangentially non-degenerate projective hypersurfaces inside a projectivized tangent bundle of the manifold. a local equivalence problem of causal structures on manifolds of dimension at least four was solved with the help of cartan's method of equivalence, leading to an $\{e\}$-structure over some principal bundle. it was shown that these structures correspond to parabolic geometries of type $(d_n,p_{1,2})$ and $(b_{n-1},p_{1,2})$, when $n\geq 4$, and $(d_3,p_{1,2,3})$. a essential local invariants are determined and interpreted geometrically. several special classes of causal structures are considered including those that are the lift of pseudo-conformal structures and those referred to as causal structures with vanishing wsf curvature. the twistorial construction considering causal structures with vanishing wsf curvature was given."
"inside this work we introduce declarative statistics, the suite of declarative modelling tools considering statistical analysis. statistical constraints represent a key building block of declarative statistics. first, we introduce the range of relevant counting and matrix constraints and associated decompositions, some of which novel, that are instrumental inside a design of statistical constraints. second, we introduce the selection of novel statistical constraints and associated decompositions, which constitute the self-contained toolbox that should be used to tackle the wide range of problems typically encountered by statisticians. finally, we deploy these statistical constraints to the wide range of application areas drawn from classical statistics and we contrast our framework against established practices."
"a central parsec of a milky way hosts two puzzlingly young stellar populations, the tight isotropic distribution of b stars around sgra* (the s-stars) and the disk of ob stars extending to ~0.5pc. with the help of the modified version of sverre aarseth's direct summation code nbody6 we explore a scenario inside which the young star cluster migrates to a galactic centre within a lifetime of a ob disk population using dynamical friction. we find that star clusters massive and dense enough to reach a central parsec form the very massive star using physical collisions on the mass segregation timescale. we follow a evolution of a merger product with the help of a most up to date, yet conservative, mass loss recipes considering very massive stars. over the large range of initial conditions, we find that a very massive star expels most of its mass using the strong stellar wind, eventually collapsing to form the black hole of mass 20 - 400 m_sun, incapable of bringing massive stars to a galactic centre. no massive intermediate mass black hole should form inside this scenario. a presence of the star cluster inside a central ~10 pc within a last 15 myr would also leave the ~2 pc ring of massive stars, which was not currently observed. thus, we conclude that a star cluster migration model was highly unlikely to be a origin of either young population, and in-situ formation models or binary disruptions are favoured."
"chemical substitution during growth was the well-established method to manipulate electronic states of quantum materials, and leads to rich spectra of phase diagrams inside cuprate and iron-based superconductors. here we report the novel and generic strategy to achieve nonvolatile electron doping inside series of (i.e. 11 and 122 structures) fe-based superconductors by ionic liquid gating induced protonation at room temperature. accumulation of protons inside bulk compounds induces superconductivity inside a parent compounds, and enhances a tc largely inside some superconducting ones. furthermore, a existence of proton inside a lattice enables a first proton nuclear magnetic resonance (nmr) study to probe directly superconductivity. with the help of fes as the model system, our nmr study reveals an emergent high-tc phase with no coherence peak which was hard to measure by nmr with other isotopes. this novel electric-field-induced proton evolution opens up an avenue considering manipulation of competing electronic states (e.g. mott insulators), and may provide an innovative way considering the broad perspective of nmr measurements with greatly enhanced detecting resolution."
"inside this paper, we study a solvability of a nonlinear dirichlet problem with sum of a operators of independent non standard growths inside the bounded domain $\omega \subset \mathbb{r}^{n}$. we obtain sufficient conditions and show a existence of weak solutions of a considered problem by with the help of monotonicity and compactness methods together."
"a present paper was devoted to provide conditions considering a levi--malcev theorem to hold or not to hold (i.e. considering two levi subalgebras to be or not conjugate by an inner automorphism) inside a context of finite-dimensional leibniz algebras over the field of characteristic zero. particularly, inside a case of a field $\mathbb{c}$ of complex numbers, we consider all possible cases inside which levi subalgebras are conjugate and not conjugate."
"earlier definitions of capacity considering wireless networks, e.g., transport or transmission capacity, considering which exact theoretical results are known, are well suited considering ad hoc networks but are not directly applicable considering cellular wireless networks, where large-scale basestation (bs) coordination was not possible, and retransmissions/arq under a sinr model was the universal feature. inside this paper, cellular wireless networks, where both bs locations and mobile user (mu) locations are distributed as independent poisson point processes are considered, and each mu connects to its nearest bs. with arq, under a sinr model, a effective downlink rate of packet transmission was a reciprocal of a expected delay (number of retransmissions needed till success), which we use as our network capacity definition after scaling it with a bs density. exact characterization of this natural capacity metric considering cellular wireless networks was derived. a capacity was shown to first increase polynomially with a bs density inside a low bs density regime and then scale inverse exponentially with a increasing bs density. two distinct upper bounds are derived that are relevant considering a low and a high bs density regimes. the single power control strategy was shown to achieve a upper bounds inside both a regimes. this result was fundamentally different from a well known capacity results considering ad hoc networks, such as transport and transmission capacity that scale as a square root of a (high) bs density. our results show that a strong temporal correlations of sinrs with ppp distributed bs locations was limiting, and a realizable capacity inside cellular wireless networks inside high-bs density regime was much smaller than previously thought. the byproduct of our analysis shows that a capacity of a aloha strategy with retransmissions was zero."
"community structure was the commonly observed feature of real networks. a term refers to a presence inside the network of groups of nodes (communities) that feature high internal connectivity, but are poorly connected between each other. whereas a issue of community detection has been addressed inside several works, a problem of validating the partition of nodes as the good community structure considering the real network has received considerably less attention and remains an open issue. we propose the set of indices considering community structure validation of network partitions, which rely on concepts from network enrichment analysis. a proposed indices allow to compare a adequacy of different partitions of nodes as community structures. moreover, they should be employed to assess whether two networks share a same or similar community structures, and to evaluate a performance of different network clustering algorithms."
"inside this paper, we exploit the memory-augmented neural network to predict accurate answers to visual questions, even when those answers occur rarely inside a training set. a memory network incorporates both internal and external memory blocks and selectively pays attention to each training exemplar. we show that memory-augmented neural networks are able to maintain the relatively long-term memory of scarce training exemplars, which was important considering visual question answering due to a heavy-tailed distribution of answers inside the general vqa setting. experimental results on two large-scale benchmark datasets show a favorable performance of a proposed algorithm with the comparison to state of a art."
"biomedical text mining has become more important than ever as a number of biomedical documents rapidly grows. with a progress of machine learning, extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning was boosting a development of effective biomedical text mining models. however, as deep learning models require the large amount of training data, biomedical text mining with deep learning often fails due to a small sizes of training datasets inside biomedical fields. recent researches on learning contextualized language representation models from text corpora shed light on a possibility of leveraging the large number of unannotated biomedical text corpora. we introduce biobert (bidirectional encoder representations from transformers considering biomedical text mining), which was the domain specific language representation model pre-trained on large-scale biomedical corpora. based on a bert architecture, biobert effectively transfers a knowledge of large amount of biomedical texts into biomedical text mining models. while bert also shows competitive performances with previous state-of-the-art models, biobert significantly outperforms them on three representative biomedical text mining tasks including biomedical named entity recognition (1.86% absolute improvement), biomedical relation extraction (3.33% absolute improvement), and biomedical question answering (9.61% absolute improvement) with minimal task-specific architecture modifications. we make pre-trained weights of biobert freely available inside this https url, and source codes of fine-tuned models inside this https url."
"online portfolio selection research has so far focused mainly on minimizing regret defined inside terms of wealth growth. practical financial decision making, however, was deeply concerned with both wealth and risk. we consider online learning of portfolios of stocks whose prices are governed by arbitrary (unknown) stationary and ergodic processes, where a goal was to maximize wealth while keeping a conditional value at risk (cvar) below the desired threshold. we characterize a asymptomatically optimal risk-adjusted performance and present an investment strategy whose portfolios are guaranteed to achieve a asymptotic optimal solution while fulfilling a desired risk constraint. we also numerically demonstrate and validate a viability of our method on standard datasets."
"it is proven inside [b.-y. chen, f. dillen, j. van der veken and l. vrancken, curvature inequalities considering lagrangian submanifolds: a final solution, differ. geom. appl. 31 (2013), 808-819] that every lagrangian submanifold $m$ of the complex space form $\tilde m^{n}(4c)$ of constant holomorphic sectional curvature $4c$ satisfies a following optimal inequality: \begin{align*} \delta(2,n-2) \leq \frac{n^2(n-2)}{4(n-1)} h^2 + 2(n-2) c, \end{align*} where $h^2$ was a squared mean curvature and $\delta(2,n-2)$ was the $\delta$-invariant on $m$. inside this paper we classify lagrangian submanifolds of complex space forms $\tilde m^{n}(4c)$, $n \geq 5$, which satisfy a equality case of this inequality at every point."
the new idea behind the method to approximate population size based on the stratified link-tracing sampling design was presented. a method extends on a frank and snijders (1994) idea behind the method by allowing considering heterogeneity inside a initial sample selection procedure. rao-blackwell estimators and corresponding resampling approximations similar to that detailed inside vincent and thompson (2017) are explored. an empirical application was provided considering the hard-to-reach networked population. a results demonstrate that a idea behind the method has much potential considering application to such populations. supplementary materials considering this article are available online.
"this paper investigates a role of tutor feedback inside language learning with the help of computational models. we compare two dominant paradigms inside language learning: interactive learning and cross-situational learning - which differ primarily inside a role of social feedback such as gaze or pointing. we analyze a relationship between these two paradigms and propose the new mixed paradigm that combines a two paradigms and allows to test algorithms inside experiments that combine no feedback and social feedback. to deal with mixed feedback experiments, we develop new algorithms and show how they perform with respect to traditional knn and prototype approaches."
"given the moebius homeomorphism $f : \partial x \to \partial y$ between boundaries of proper, geodesically complete cat(-1) spaces $x,y$, we describe an extension $\hat{f} : x \to y$ of $f$, called a circumcenter map of $f$, which was constructed with the help of circumcenters of expanding sets. a extension $\hat{f}$ was shown to coincide with a $(1, \log 2)$-quasi-isometric extension constructed inside [biswas3], and was locally $1/2$-holder continuous. when $x,y$ are complete, simply connected manifolds with sectional curvatures $k$ satisfying $-b^2 \leq k \leq -1$ considering some $b \geq 1$ then a extension $\hat{f} : x \to y$ was the $(1, (1 - \frac{1}{b})\log 2)$-quasi-isometry. circumcenter extension of moebius maps was natural with respect to composition with isometries."
"the grand challenge of a 21st century cosmology was to accurately approximate a cosmological parameters of our universe. the major idea behind the method to estimating a cosmological parameters was to use a large-scale matter distribution of a universe. galaxy surveys provide a means to map out cosmic large-scale structure inside three dimensions. information about galaxy locations was typically summarized inside the ""single"" function of scale, such as a galaxy correlation function or power-spectrum. we show that it was possible to approximate these cosmological parameters directly from a distribution of matter. this paper presents a application of deep 3d convolutional networks to volumetric representation of dark-matter simulations as well as a results obtained with the help of the recently proposed distribution regression framework, showing that machine learning techniques are comparable to, and should sometimes outperform, maximum-likelihood point estimates with the help of ""cosmological models"". this opens a way to estimating a parameters of our universe with higher accuracy."
"we present an empirical idea behind the method considering interpreting gravitational wave signals of binary black hole mergers under a assumption that a underlying black hole population was sourced by remnants of stellar evolution. with the help of a observed relationship between galaxy mass and stellar metallicity, we predict a black hole count as the function of galaxy stellar mass. we show, considering example, that the galaxy like a milky way should host millions of $\sim 30~m_\odot$ black holes and dwarf satellite galaxies like draco should host $\sim 100$ such remnants, with weak dependence on a assumed imf and stellar evolution model. most low-mass black holes ($\sim10 m_\odot$) typically reside within massive galaxies ($m_\star \simeq 10^{11} m_\odot$) while massive black holes ($\sim 50~m_\odot$) typically reside within dwarf galaxies ($m_\odot \simeq 10^9 m_\odot$) today. if roughly $1\%$ of black holes are involved inside the binary black hole merger, then a reported merger rate densities from advanced ligo should be accommodated considering the range of merger timescales, and a detection of mergers with $> 50~m_\odot$ black holes should be expected within a next decade. identifying a host galaxy population of a mergers provides the way to constrain both a binary neutron star or black hole formation efficiencies and a merger timescale distributions; these events would be primarily localized inside dwarf galaxies if a merger timescale was short compared to a age of a universe and inside massive galaxies otherwise. as more mergers are detected, a prospect of identifying a host galaxy population, either directly through a detection of electromagnetic counterparts of binary neutron star mergers or indirectly through a anisotropy of a events, will become the realistic possibility."
"we consider continuous-time, finite-horizon, optimal quadratic control of semi-markov jump linear systems (s-mjls), and develop principled approximations through markov-like representations considering a holding-time distributions. we adopt the phase-type approximation considering holding times, which was known to be consistent, and translates the s-mjls into the specific mjls with partially observable modes (mjlspom), where a modes inside the cluster have a same dynamic, a same cost weighting matrices and a same control policy. considering the general mjlspom, we give necessary and sufficient conditions considering optimal (switched) linear controllers. when specialized to our particular mjlspom, we additionally establish a existence of optimal linear controller, as well as its optimality within a class of general controllers satisfying standard smoothness conditions. a known equivalence between phase-type distributions and positive linear systems allows to leverage existing modeling tools, but possibly with large computational costs. motivated by this, we propose matrix exponential approximation of holding times, resulting inside pseudo-mjlspom representation, i.e., where a transition rates could be negative. such the representation was of relatively low order, and maintains a same optimality conditions as considering a mjlspom representation, but could violate non-negativity of holding-time density functions. the two-step procedure consisting of the local pulling-up modification and the filtering technique was constructed to enforce non-negativity."
"inside this paper, the multi-agent coordination problem with steady-state regulation constraints was investigated considering the class of nonlinear systems. unlike existing leader-following coordination formulations, a reference signal was not given by the dynamic autonomous leader but determined as a optimal solution of the distributed optimization problem. furthermore, we consider the global constraint having noisy data observations considering a optimization problem, which implies that reference signal was not trivially available with existing optimization algorithms. to handle those challenges, we present the passivity-based analysis and design idea behind the method by with the help of only local objective function, local data observation and exchanged information from their neighbors. a proposed distributed algorithms are shown to achieve a optimal steady-state regulation by rejecting a unknown observation disturbances considering passive nonlinear agents, which are persuasive inside various practical problems. applications and simulation examples are then given to verify a effectiveness of our design."
"a second-order extended stability factorized runge-kutta-chebyshev (frkc2) class of explicit schemes considering a integration of large systems of pdes with diffusive terms was presented. frkc2 schemes are straightforward to implement through ordered sequences of forward euler steps with complex stepsizes, and easily parallelised considering large scale problems on distributed architectures. preserving 7 digits considering accuracy at 16 digit precision, a schemes are theoretically capable of maintaining internal stability at acceleration factors inside excess of 6000 with respect to standard explicit runge-kutta methods. a stability domains have approximately a same extents as those of rkc schemes, and are the third longer than those of rkl2 schemes. extension of frkc methods to fourth-order, by both complex splitting and butcher composition techniques, was discussed. the publicly available implementation of a frkc2 class of schemes may be obtained from maths.dit.ie/frkc"
"inverse problems, where inside broad sense a task was to learn from a noisy response about some unknown function, usually represented as a argument of some known functional form, has received wide attention inside a general scientific disciplines. how- ever, inside mainstream statistics such inverse problem paradigm does not seem to be as popular. inside this article we provide the brief overview of such problems from the statistical, particularly bayesian, perspective. we also compare and contrast a above class of problems with a perhaps more statistically familiar inverse regression problems, arguing that this class of problems contains a traditional class of inverse problems. inside course of our review we point out that a statistical literature was very scarce with respect to both a inverse paradigms, and substantial research work was still necessary to develop a fields."
"we consider a problem of finding and describing minimisers of a rayleigh quotient \[ \lambda_\infty \, :=\, \inf_{u\in \mathcal{w}^{2,\infty}(\omega)\setminus\{0\} }\frac{\|\delta u\|_{l^\infty(\omega)}}{\|u\|_{l^\infty(\omega)}}, \] where $\omega \subseteq \mathbb{r}^n$ was the bounded $c^{1,1}$ domain and $\mathcal{w}^{2,\infty}(\omega)$ was the class of weakly twice differentiable functions satisfying either $u=0$ or $u=|\mathrm{d} u|=0$ on $\partial \omega$. our first main result, obtained through approximation by $l^p$-problems as $p\to \infty$, was a existence of the minimiser $u_\infty \in \mathcal{w}^{2,\infty}(\omega)$ satisfying \[ \left\{ \begin{array}{ll} \delta u_\infty \, \in \, \lambda_\infty \mathrm{sgn}(f_\infty) & \text{ a.e. inside }\omega, \\ \delta f_\infty \, =\, \mu_\infty & \text{ inside }\mathcal{d}'(\omega), \end{array} \right. \] considering some $f_\infty\in l^1(\omega)\cap bv_{\text{loc}}(\omega)$ and the measure $\mu_\infty \in \mathcal{m}(\omega)$, considering either choice of boundary conditions. here sgn was a multi-valued sign function. we also study a dependence of a eigenvalue $\lambda_\infty$ on a domain, establishing a validity of the faber-krahn type inequality: among all $c^{1,1}$ domains with fixed measure, a ball was the strict minimiser of $\omega \mapsto \lambda_\infty(\omega)$. this result was shown to hold true considering either choice of boundary conditions and inside every dimension."
"we propose the new algorithm considering solving parabolic partial differential equations (pdes) and backward stochastic differential equations (bsdes) inside high dimension, by making an analogy between a bsde and reinforcement learning with a gradient of a solution playing a role of a policy function, and a loss function given by a error between a prescribed terminal condition and a solution of a bsde. a policy function was then approximated by the neural network, as was done inside deep reinforcement learning. numerical results with the help of tensorflow illustrate a efficiency and accuracy of a proposed algorithms considering several 100-dimensional nonlinear pdes from physics and finance such as a allen-cahn equation, a hamilton-jacobi-bellman equation, and the nonlinear pricing model considering financial derivatives."
"off-policy learning was key to scaling up reinforcement learning as it allows to learn about the target policy from a experience generated by the different behavior policy. unfortunately, it has been challenging to combine off-policy learning with function approximation and multi-step bootstrapping inside the way that leads to both stable and efficient algorithms. inside this work, we show that a \textsc{tree backup} and \textsc{retrace} algorithms are unstable with linear function approximation, both inside theory and inside practice with specific examples. based on our analysis, we then derive stable and efficient gradient-based algorithms with the help of the quadratic convex-concave saddle-point formulation. by exploiting a problem structure proper to these algorithms, we are able to provide convergence guarantees and finite-sample bounds. a applicability of our new analysis also goes beyond \textsc{tree backup} and \textsc{retrace} and allows us to provide new convergence rates considering a gtd and gtd2 algorithms without having recourse to projections or polyak averaging."
"we consider a time-dependent 2d ginzburg-landau equation inside a whole plane with terms modeling impurities and applied currents. a ginzburg-landau vortices are then subjected to three forces: their mutual repulsive coulomb-like interaction, a applied current pushing them inside the fixed direction, and a pinning force attracting them towards a impurities. a competition between a three was expected to lead to complicated glassy effects. we rigorously study a limit inside which a number of vortices $n_\epsilon$ blows up as a inverse ginzburg-landau parameter $\epsilon$ goes to $0$, and we derive using the modulated energy method fluid-like mean-field evolution equations. these results hold considering parabolic, conservative, and mixed-flow dynamics inside appropriate regimes of $n_\epsilon\uparrow\infty$. finally, we briefly discuss some natural homogenization questions raised by this study."
"considering hidden markov models one of a most popular estimates of a hidden chain was a viterbi path -- a path maximising a posterior probability. we consider the more general setting, called a pairwise markov model, where a joint process consisting of finite-state hidden regime and observation process was assumed to be the markov chain. we prove that under some conditions it was possible to extend a viterbi path to infinity considering almost every observation sequence which inside turn enables to define an infinite viterbi decoding of a observation process, called a viterbi process. this was done by constructing the block of observations, called the barrier, which ensures that a viterbi path goes trough the given state whenever this block occurs inside a observation sequence."
"we consider a problem of repeatedly solving the variant of a same dynamic programming problem inside successive trials. an instance of a type of problems we consider was to find the good binary search tree inside the changing environment.at a beginning of each trial, a learner probabilistically chooses the tree with a $n$ keys at a internal nodes and a $n+1$ gaps between keys at a leaves. a learner was then told a frequencies of a keys and gaps and was charged by a average search cost considering a chosen tree. a problem was online because a frequencies should change between trials. a goal was to develop algorithms with a property that their total average search cost (loss) inside all trials was close to a total loss of a best tree chosen inside hindsight considering all trials. a challenge, of course, was that a algorithm has to deal with exponential number of trees. we develop the general methodology considering tackling such problems considering the wide class of dynamic programming algorithms. our framework allows us to extend online learning algorithms like hedge and component hedge to the significantly wider class of combinatorial objects than is possible before."
"we introduce the hybrid metaphor considering a visualization of a reconciliations of co-phylogenetic trees, that are mappings among a nodes of two trees. a typical application was a visualization of a co-evolution of hosts and parasites inside biology. our strategy combines the space-filling and the node-link approach. differently from traditional methods, it guarantees an unambiguous and `downward' representation whenever a reconciliation was time-consistent (i.e., meaningful). we address a problem of a minimization of a number of crossings inside a representation, by giving the characterization of planar instances and by establishing a complexity of a problem. finally, we propose heuristics considering computing representations with few crossings."
"inside this paper we present solutions to three short comings of smoothed particles hydrodynamics (sph) encountered inside previous work when applying it to giant impacts. first we introduce the novel method to obtain accurate sph representations of the planet's equilibrium initial conditions based on equal area tessellations of a sphere. this allows one to imprint an arbitrary density and internal energy profile with very low noise which substantially reduces computation because these models require no relaxation prior to use. as the consequence one should significantly increase a resolution and more flexibly change a initial bodies to explore larger parts of a impact parameter space inside simulations. a second issue addressed was a proper treatment of a matter/vacuum boundary at the planet's surface with the modified sph density estimator that properly calculates a density stabilizing a models and avoiding an artificially low density atmosphere prior to impact. further we present the novel sph scheme that simultaneously conserves both energy and entropy considering an arbitrary equation of state. this prevents loss of entropy during a simulation and further assures that a material does not evolve into unphysical states. application of these modifications to impact simulations considering different resolutions up to $6.4 \cdot 10^6$ particles show the general agreement with prior result. however, we observe resolution dependent differences inside a evolution and composition of post collision ejecta. this strongly suggests that a use of more sophisticated equations of state also demands the large number of particles inside such simulations."
"statistical inference should be computationally prohibitive inside ultrahigh-dimensional linear models. correlation-based variable screening, inside which one leverages marginal correlations considering removal of irrelevant variables from a model prior to statistical inference, should be used to overcome this challenge. prior works on correlation-based variable screening either impose strong statistical priors on a linear model or assume specific post-screening inference methods. this paper first extends a analysis of correlation-based variable screening to arbitrary linear models and post-screening inference techniques. inside particular, ($i$) it shows that the condition---termed a screening condition---is sufficient considering successful correlation-based screening of linear models, and ($ii$) it provides insights into a dependence of marginal correlation-based screening on different problem parameters. numerical experiments confirm that these insights are not mere artifacts of analysis; rather, they are reflective of a challenges associated with marginal correlation-based variable screening. second, a paper explicitly derives a screening condition considering two families of linear models, namely, sub-gaussian linear models and arbitrary (random or deterministic) linear models. inside a process, it establishes that---under appropriate conditions---it was possible to reduce a dimension of an ultrahigh-dimensional, arbitrary linear model to almost a sample size even when a number of active variables scales almost linearly with a sample size."
"the networked control system (ncs) consisting of cascaded two-port communication channels between a plant and controller was modeled and analyzed. towards this end, a robust stability of the standard closed-loop system inside a presence of conelike perturbations on a system graphs was investigated. a underlying geometric insights are then exploited to analyze a two-port ncs. it was shown that a robust stability of a two-port ncs should be guaranteed when a nonlinear uncertainties inside a transmission matrices are sufficiently small inside norm. a stability condition, given inside a form of ""arcsin"" of a uncertainty bounds, was both necessary and sufficient."
"let $p\geq 5$ be the prime number, $g$ the split connected reductive group defined over the $p$-adic field, and $i_1$ the choice of pro-$p$-iwahori subgroup. let $c$ be an algebraically closed field of characteristic $p$ and $\mathcal{h}$ a pro-$p$-iwahori--hecke algebra over $c$ associated to $i_1$. inside this note, we compute a action of $\mathcal{h}$ on $\textrm{h}^1(i_1,c)$ and $\textrm{h}^{\textrm{top}}(i_1,c)$ when a root system of $g$ was irreducible. we also give some partial results inside a general case."
"high dimensional piecewise stationary graphical models represent the versatile class considering modelling time varying networks arising inside diverse application areas, including biology, economics, and social sciences. there has been recent work inside offline detection and approximation of regime changes inside a topology of sparse graphical models. however, a online setting remains largely unexplored, despite its high relevance to applications inside sensor networks and other engineering monitoring systems, as well as financial markets. to that end, this work introduces the novel scalable online algorithm considering detecting an unknown number of abrupt changes inside a inverse covariance matrix of sparse gaussian graphical models with small delay. a proposed algorithm was based upon monitoring a conditional log-likelihood of all nodes inside a network and should be extended to the large class of continuous and discrete graphical models. we also investigate asymptotic properties of our procedure under certain mild regularity conditions on a graph size, sparsity level, number of samples, and pre- and post-changes inside a topology of a network. numerical works on both synthetic and real data illustrate a good performance of a proposed methodology both inside terms of computational and statistical efficiency across numerous experimental settings."
"we argue that a standard graph laplacian was preferable considering spectral partitioning of signed graphs compared to a signed laplacian. simple examples demonstrate that partitioning based on signs of components of a leading eigenvectors of a signed laplacian may be meaningless, inside contrast to partitioning based on a fiedler vector of a standard graph laplacian considering signed graphs. we observe that negative eigenvalues are beneficial considering spectral partitioning of signed graphs, making a fiedler vector easier to compute."
"a world health organization (who) reported 1.25 million deaths yearly due to road traffic accidents worldwide and a number has been continuously increasing over a last few years. nearly fifth of these accidents are caused by distracted drivers. existing work of distracted driver detection was concerned with the small set of distractions (mostly, cell phone usage). unreliable ad-hoc methods are often used.in this paper, we present a first publicly available dataset considering driver distraction identification with more distraction postures than existing alternatives. inside addition, we propose the reliable deep learning-based solution that achieves the 90% accuracy. a system consists of the genetically-weighted ensemble of convolutional neural networks, we show that the weighted ensemble of classifiers with the help of the genetic algorithm yields inside the better classification confidence. we also study a effect of different visual elements inside distraction detection by means of face and hand localizations, and skin segmentation. finally, we present the thinned version of our ensemble that could achieve 84.64% classification accuracy and operate inside the real-time environment."
"recurrent neural networks (rnn) are widely used to solve the variety of problems and as a quantity of data and a amount of available compute have increased, so have model sizes. a number of parameters inside recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. a challenge was due to both a size of a model and a time it takes to evaluate it. inside order to deploy these rnns efficiently, we propose the technique to reduce a parameters of the network by pruning weights during a initial training of a network. at a end of training, a parameters of a network are sparse while accuracy was still close to a original dense neural network. a network size was reduced by 8x and a time required to train a model remains constant. additionally, we should prune the larger dense network to achieve better than baseline performance while still reducing a total number of parameters significantly. pruning rnns reduces a size of a model and should also aid achieve significant inference time speed-up with the help of sparse matrix multiply. benchmarks show that with the help of our technique model size should be reduced by 90% and speed-up was around 2x to 7x."
"very recently proximal policy optimization (ppo) algorithms have been proposed as first-order optimization methods considering effective reinforcement learning. while ppo was inspired by a same learning theory that justifies trust region policy optimization (trpo), ppo substantially simplifies algorithm design and improves data efficiency by performing multiple epochs of \emph{clipped policy optimization} from sampled data. although clipping inside ppo stands considering an important new mechanism considering efficient and reliable policy update, it may fail to adaptively improve learning performance inside accordance with a importance of each sampled state. to address this issue, the new surrogate learning objective featuring an adaptive clipping mechanism was proposed inside this paper, enabling us to develop the new algorithm, known as ppo-$\lambda$. ppo-$\lambda$ optimizes policies repeatedly based on the theoretical target considering adaptive policy improvement. meanwhile, destructively large policy update should be effectively prevented through both clipping and adaptive control of the hyperparameter $\lambda$ inside ppo-$\lambda$, ensuring high learning reliability. ppo-$\lambda$ enjoys a same simple and efficient design as ppo. empirically on several atari game playing tasks and benchmark control tasks, ppo-$\lambda$ also achieved clearly better performance than ppo."
"inside this paper we describe the deep learning system that has been designed and built considering a wassa 2017 emotion intensity shared task. we introduce the representation learning idea behind the method based on inner attention on top of an rnn. results show that our model offers good capabilities and was able to successfully identify emotion-bearing words to predict intensity without leveraging on lexicons, obtaining a 13th place among 22 shared task competitors."
"we propose the representation of a indian summer monsoon rainfall inside terms of the probabilistic model based on the markov random field, consisting of discrete state variables representing low and high rainfall at grid-scale and daily rainfall patterns across space and inside time. these discrete states are conditioned on observed daily gridded rainfall data from a period 2000-2007. a model gives us the set of 10 spatial patterns of daily monsoon rainfall over india, which are robust over the range of user-chosen parameters as well as coherent inside space and time. each day inside a monsoon season was assigned precisely one of a spatial patterns, that approximates a spatial distribution of rainfall on that day. such approximations are quite accurate considering nearly 95% of a days. remarkably, these patterns are representative (with similar accuracy) of a monsoon seasons from 1901 to 2000 as well. finally, we compare a proposed model with alternative approaches to extract spatial patterns of rainfall, with the help of empirical orthogonal functions as well as clustering algorithms such as k-means and spectral clustering."
"we present a highest spatial resolution alma observations to date of a class i protostar wl 17 inside a $\rho$ ophiuchus l1688 molecular cloud complex, which show that it has the 12 au hole inside a center of its disk. we consider whether wl 17 was actually the class ii disk being extincted by foreground material, but find that such models do not provide the good fit to a broadband sed and also require such high extinction that it would presumably arise from dense material close to a source such as the remnant envelope. self-consistent models of the disk embedded inside the rotating collapsing envelope should nicely reproduce both a alma 3 mm observations and a broadband sed of wl 17. this suggests that wl 17 was the disk inside a early stages of its formation, and yet even at this young age a inner disk has been depleted. although there are multiple pathways considering such the hole to be created inside the disk, if this hole were produced by a formation of planets it could place constraints on a timescale considering a growth of planetesimals inside protoplanetary disks."
"complex systems should be modelled at various levels of detail. ideally, causal models of a same system should be consistent with one another inside a sense that they agree inside their predictions of a effects of interventions. we formalise this notion of consistency inside a case of structural equation models (sems) by introducing exact transformations between sems. this provides the general language to consider, considering instance, a different levels of description inside a following three scenarios: (a) models with large numbers of variables versus models inside which a `irrelevant' or unobservable variables have been marginalised out; (b) micro-level models versus macro-level models inside which a macro-variables are aggregate features of a micro-variables; (c) dynamical time series models versus models of their stationary behaviour. our analysis stresses a importance of well specified interventions inside a causal modelling process and sheds light on a interpretation of cyclic sems."
"this paper was concerned with the discrete-time mean-field stochastic linear-quadratic optimal control problem arose from financial application. through matrix dynamical optimization method, the group of linear feedback controls was investigated. a problem was then reformulated as an operator stochastic linear-quadratic optimal control problem by the sequence of bounded linear operators over hilbert space, a optimal control with six algebraic riccati difference equations was obtained by backward induction. a two above approaches are proved to be coincided by a classical method of completing a square. finally, after discussing a solution of a problem under multidimensional noises, the financial application example was given."
"it was the great pleasure to be invited to join a chorus on this auspicious occasion to celebrate professor k. alex mueller's 90th birthday by professors annette bussman-holder, hugo keller, and antonio bianconi. as the student inside high temperature superconductivity, i am forever grateful to professor alex mueller and dr. georg bednorz ""for their important breakthrough inside a discovery of superconductivity inside a ceramic materials"" inside 1986 as described inside a citation of their 1987 nobel prize inside physics. it was this breakthrough discovery that has ushered inside a explosion of research activities inside high temperature superconductivity (hts) and has provided immense excitement inside hts science and technology inside a ensuing decades till now. alex has not been resting on his laurels and has continued to search considering a origin of a unusual high temperature superconductivity inside cuprates."
"we evaluate a performance of four machine learning methods considering modeling and predicting fcc solute diffusion barriers. more than 200 fcc solute diffusion barriers from previous density functional theory (dft) calculations served as our dataset to train four machine learning methods: linear regression (lr), decision tree (dt), gaussian kernel ridge regression (gkrr), and artificial neural network (ann). we separately optimize key physical descriptors favored by each method to model diffusion barriers. we also assess a ability of each method to extrapolate when faced with new hosts with limited known data. gkrr and ann were found to perform a best, showing 0.15 ev cross-validation errors and predicting impurity diffusion inside new hosts to within 0.2 ev when given only 5 data points from a host. we demonstrate a success of the combined dft + data mining idea behind the method towards solving materials science challenges and predict a diffusion barrier of all available impurities across all fcc hosts."
"we present the new preprocessing algorithm considering embedding a nodes of the given edge-weighted undirected graph into the euclidean space. a euclidean distance between any two nodes inside this space approximates a length of a shortest path between them inside a given graph. later, at runtime, the shortest path between any two nodes should be computed with a* search with the help of a euclidean distances as heuristic. our preprocessing algorithm, called fastmap, was inspired by a data mining algorithm of a same name and runs inside near-linear time. hence, fastmap was orders of magnitude faster than competing approaches that produce the euclidean embedding with the help of semidefinite programming. fastmap also produces admissible and consistent heuristics and therefore guarantees a generation of shortest paths. moreover, fastmap applies to general undirected graphs considering which many traditional heuristics, such as a manhattan distance heuristic, are not well defined. empirically, we demonstrate that a* search with the help of a fastmap heuristic was competitive with a* search with the help of other state-of-the-art heuristics, such as a differential heuristic."
"we introduce the multiple testing procedure (treebh) which addresses a challenge of controlling error rates at multiple levels of resolution. conceptually, we frame this problem as a selection of hypotheses which are organized hierarchically inside the tree structure. we describe the fast algorithm considering a proposed sequential procedure, and prove that it controls relevant error rates given certain assumptions on a dependence among a p-values. through simulations, we demonstrate that treebh offers a desired guarantees under the range of dependency structures (including one similar to that encountered inside genome-wide association studies) and that it has a potential of gaining power over alternative methods. we also introduce the modified version of treebh which we prove to control a relevant error rates under any dependency structure. we conclude with two case studies: we first analyze data collected as part of a genotype-tissue expression (gtex) project, which aims to characterize a genetic regulation of gene expression across multiple tissues inside a human body, and secondly, data examining a relationship between a gut microbiome and colorectal cancer."
we present measurements of a frequency transfer stability and analysis of a noise characteristics of an optical signal propagating over aerial suspended fiber links up to 153.6 km inside length. a measured frequency transfer stability over these links was on a order of 10^-11 at an integration time of one second dropping to 10^-12 considering integration times longer than 100 s. we show that wind-loading of a cable spans was a dominant source of short-timescale noise on a fiber links. we also report an attempt to stabilize a optical frequency transfer over these aerial links.
"pca was the classical statistical technique whose simplicity and maturity has seen it find widespread use as an anomaly detection technique. however, it was limited inside this regard by being sensitive to gross perturbations of a input, and by seeking the linear subspace that captures normal behaviour. a first issue has been dealt with by robust pca, the variant of pca that explicitly allows considering some data points to be arbitrarily corrupted, however, this does not resolve a second issue, and indeed introduces a new issue that one should no longer inductively find anomalies on the test set. this paper addresses both issues inside the single model, a robust autoencoder. this method learns the nonlinear subspace that captures a majority of data points, while allowing considering some data to have arbitrary corruption. a model was simple to train and leverages recent advances inside a optimisation of deep neural networks. experiments on the range of real-world datasets highlight a model's effectiveness."
"we study a dependence of galaxy clustering on atomic gas mass with the help of the sample of $\sim$16,000 galaxies with redshift inside a range of $0.0025<z<0.05$ and hi mass of $m_{\rm hi}>10^8m_{\odot}$, drawn from a 70% complete sample of a arecibo legacy fast alfa survey. we construct subsamples of galaxies with $m_{\rm hi}$ above different thresholds, and make volume-limited clustering measurements inside terms of three statistics: a projected two-point correlation function, a projected cross-correlation function with respect to the reference sample selected from a sloan digital sky survey, and a redshift-space monopole moment. inside contrast to previous studies, which found no/weak hi-mass dependence, we find both a clustering amplitude on scales above the few mpc and a bias factors to increase significantly with increasing hi mass considering subsamples with hi mass thresholds above $10^9m_{\odot}$. considering hi mass thresholds below $10^9m_{\odot}$, while a measurements have large uncertainties caused by a limited survey volume and sample size, a inferred galaxy bias factors are systematically lower than a minimum halo bias factor from mass-selected halo samples. a simple halo model, inside which galaxy content was only determined by halo mass, has difficulties inside interpreting a clustering measurements of a hi-selected samples. we extend a simple model by including a halo formation time as an additional parameter. the model that puts hi-rich galaxies into halos that formed late should reproduce a clustering measurements reasonably well. we present a implications of our best-fitting model on a correlation of hi mass with halo mass and formation time, as well as a halo occupation distributions and hi mass functions considering central and satellite galaxies. these results are compared with a predictions from semi-analytic galaxy formation models and hydrodynamic galaxy formation simulations."
"a paper provides global optimization algorithms considering two particularly difficult nonconvex problems raised by hybrid system identification: switching linear regression and bounded-error estimation. while most works focus on local optimization heuristics without global optimality guarantees or with guarantees valid only under restrictive conditions, a proposed idea behind the method always yields the solution with the certificate of global optimality. this idea behind the method relies on the branch-and-bound strategy considering which we devise lower bounds that should be efficiently computed. inside order to obtain scalable algorithms with respect to a number of data, we directly optimize a model parameters inside the continuous optimization setting without involving integer variables. numerical experiments show that a proposed algorithms offer the higher accuracy than convex relaxations with the reasonable computational burden considering hybrid system identification. inside addition, we discuss how bounded-error approximation was related to robust approximation inside a presence of outliers and exact recovery under sparse noise, considering which we also obtain promising numerical results."
we consider an arbitrary algebra of a class of brauer configuration algebras and calculate a dimension of a center by determining the $k$-basis.
"deep artificial neural networks require the large corpus of training data inside order to effectively learn, where collection of such training data was often expensive and laborious. data augmentation overcomes this issue by artificially inflating a training set with label preserving transformations. recently there has been extensive use of generic data augmentation to improve convolutional neural network (cnn) task performance. this study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate considering their data sets. various geometric and photometric schemes are evaluated on the coarse-grained data set with the help of the relatively simple cnn. experimental results, run with the help of 4-fold cross-validation and reported inside terms of top-1 and top-5 accuracy, indicate that cropping inside geometric augmentation significantly increases cnn task performance."
"a discovery of $\gamma$-ray emission from radio-loud narrow-line seyfert 1 (nlsy1) galaxies has questioned a need considering large black hole masses (> 10$^8$ m$_{\odot}$) to launch relativistic jets. we present near-infrared data of a $\gamma$-ray-emitting nlsy1 fbqs j1644+2619 that were collected with the help of a camera circe (canarias infrared camera experiment) at a 10.4-m gran telescopio canarias to investigate a structural properties of its host galaxy and to infer a black hole mass. a 2d surface brightness profile was modelled by a combination of the nuclear and the bulge component with the sérsic profile with index $n$ = 3.7, indicative of an elliptical galaxy. a structural parameters of a host are consistent with a correlations of effective radius and surface brightness against absolute magnitude measured considering elliptical galaxies. from a bulge luminosity, we estimated the black hole mass of (2.1$\pm$0.2) $\times$10$^8$ m$_{\odot}$, consistent with a values characterizing radio-loud active galactic nuclei."
"ultrafast laser excitation of the metal causes correlated, highly nonequilibrium dynamics of electronic and ionic degrees of freedom, which are however only poorly captured by a widely-used two-temperature model. here we develop an out-of-equilibrium theory that captures a full dynamic evolution of a electronic and phononic populations and provides the microscopic description of a transfer of energy delivered optically into electrons to a lattice. all essential nonequilibrium energy processes, such as electron-phonon and phonon-phonon interactions are taken into account. moreover, as all required quantities are obtained from first-principles calculations, a model gives an exact description of a relaxation dynamics without a need considering fitted parameters. we apply a model to fept and show that a detailed relaxation was out-of-equilibrium considering picoseconds."
"a social phenomenon of familiar strangers is identified by stanley milgram inside 1972 with the small-scale experiment. however, there has been limited research focusing on uncovering a phenomenon at the societal scale and simultaneously investigating a social relationships between familiar strangers. with a aid of a large-scale mobile phone records, we empirically show a existence of a relationship inside a country of andorra. built upon a temporal and spatial distributions, we investigate a mechanisms, especially collective temporal regularity and spatial structure that trigger this phenomenon. moreover, we explore a relationship between social distances on a communication network and a number of encounters and show that larger number of encounters indicates shorter social distances inside the social network. a understanding of a physical encounter network could have important implications to understand a phenomena such as epidemics spreading and information diffusion."
"imaging atmospheric cherenkov telescopes (iacts) represent the class of instruments which are dedicated to a ground-based observation of cosmic vhe gamma ray emission based on a detection of a cherenkov radiation produced inside a interaction of gamma rays with a earth atmosphere. one of a key elements of such instruments was the pixelized focal-plane camera consisting of photodetectors. to date, photomultiplier tubes (pmts) have been a common choice given their high photon detection efficiency (pde) and fast time response. recently, silicon photomultipliers (sipms) are emerging as an alternative. this rapidly evolving technology has strong potential to become superior to that based on pmts inside terms of pde, which would further improve a sensitivity of iacts, and see the price reduction per square millimeter of detector area. we are working to develop the sipm-based module considering a focal-plane cameras of a magic telescopes to probe this technology considering iacts with large focal plane cameras of an area of few square meters. we will describe a solutions we are exploring inside order to balance the competitive performance with the minimal impact on a overall magic camera design with the help of ray tracing simulations. we further present the comparative study of a overall light throughput based on monte carlo simulations and considering a properties of a major hardware elements of an iact."
"we consider inside this paper a regularity problem considering time-optimal trajectories of the single-input control-affine system on the n-dimensional manifold. we prove that, under generic conditions on a drift and a controlled vector field, any control u associated with an optimal trajectory was smooth out of the countable set of times. more precisely, there exists an integer k, only depending on a dimension n, such that a non-smoothness set of u was made of isolated points, accumulations of isolated points, and so on up to k-th order iterated accumulations."
"inside this paper, we introduce new classes of divergences by extending a definitions of a bregman divergence and a skew jensen divergence. these new divergence classes (g-bregman divergence and skew g-jensen divergence) satisfy some properties similar to a bregman or skew jensen divergence. we show these g-divergences include divergences which belong to the class of f-divergence (the hellinger distance, a chi-square divergence and a alpha-divergence inside addition to a kullback-leibler divergence). moreover, we derive an inequality between a g-bregman divergence and a skew g-jensen divergence and show this inequality was the generalization of lin's inequality."
"density functional theory and nonequilibrium green's function calculations have been used to explore spin-resolved transport through a high-spin state of an iron(ii)sulfur single molecular magnet. our results show that this molecule exhibits near-perfect spin filtering, where a spin-filtering efficiency was above 99%, as well as significant negative differential resistance centered at the low bias voltage. a rise inside a spin-up conductivity up to a bias voltage of 0.4 v was dominated by the conductive lowest unoccupied molecular orbital, and this was accompanied by the slight increase inside a magnetic moment of a fe atom. a subsequent drop inside a spin-up conductivity was because a conductive channel moves to a highest occupied molecular orbital which has the lower conductance contribution. this was accompanied by the drop inside a magnetic moment of a fe atom. these two exceptional properties, and a fact that a onset of negative differential resistance occurs at low bias voltage, suggests a potential of a molecule inside nanoelectronic and nanospintronic applications."
"controlling quasiparticle dynamics should improve a performance of superconducting devices. considering example, it has been demonstrated effective inside increasing lifetime and stability of superconducting qubits. here we study how to optimize a placement of normal-metal traps inside transmon-type qubits. when a trap size increases beyond the certain characteristic length, a details of a geometry and trap position, and even a number of traps, become important. we discuss considering some experimentally relevant examples how to shorten a decay time of a excess quasiparticle density. moreover, we show that the trap inside a vicinity of the josephson junction should reduce a steady-state quasiparticle density near that junction, thus suppressing a quasiparticle-induced relaxation rate of a qubit. such the trap also reduces a impact of fluctuations inside a generation rate of quasiparticles, rendering a qubit more stable."
"recent transport experiments inside a cuprate superconductors linked a opening of a pseudogap to the change inside electronic dispersion [s. badoux et al., nature 531, 210 (2015)]. transport measurements showed that a carrier density sharply changes from $x$ to $1+x$ at a pseudogap critical doping, inside accordance with a change from fermi arcs at low doping to the large hole fermi surface at high doping. a su(2) theory of cuprates shows that antiferromagnetic short range interactions cause a arising of both charge and superconducting orders, which are related by an su(2) symmetry. a fluctuations associated with this symmetry form the pseudogap phase. here we derive a renormalised electronic propagator under a su(2) dome, and calculate a spectral functions and transport quantities of a renormalised bands. we show that their evolution with doping matches both spectral and transport measurements."
"the classical and useful way to study controllability problems was a moment method developed by fattorini-russell, based on a construction of suitable biorthogonal families. several recent problems exhibit a same behaviour: a eigenvalues of a problem satisfy the uniform but rather 'bad' gap condition, and the rather 'good' but only asymptotic one. a goal of this work was to obtain general and precise upper and lower bounds considering biorthogonal families under these two gap conditions, and so to measure a influence of a 'bad' gap condition and a good influence of a 'good' asymptotic one. to achieve our goals, we extend some of a general results of fattorini-russell concerning biorthogonal families, with the help of complex analysis techniques developed by seidman, güichal, tenenbaum-tucsnak, and lissy."
"internet protocol (ip) addresses are frequently used as the method of locating web users by researchers inside several different fields. however, there are competing reports concerning a accuracy of those locations, and little research has been done inside manually comparing a ip geolocation databases and web page geographic information. this paper categorized web page from a yahoo search engine into twelve categories, ranging from 'blog' and 'news' to 'education' and 'governmental'. then we manually compared a mailing or street address of a web page's content creator with a geolocation results by a given ip address. we introduced the cartographic design method by creating kernel density maps considering visualizing a information landscape of web pages associated with specific keywords."
"we find evidence that a newly discovered fe-based superconductor kca$_2$fe$_4$as$_4$f$_2$ ($t_c~=~33.36(7)$~k) displays multigap superconductivity with line nodes. transverse field muon spin rotation ($\mu$sr) measurements show that a temperature dependence of a superfluid density does not have a expected behavior of the fully-gapped superconductor, due to a lack of saturation at low temperatures. moreover, a data cannot be well fitted with the help of either single band models or the multiband $s$-wave model, yet are well described by two-gap models with line nodes on either one or both of a gaps. meanwhile a zero-field $\mu$sr results indicate the lack of time reversal symmetry breaking inside a superconducting state, but suggest a presence of magnetic fluctuations. these results demonstrate the different route considering realizing nodal superconductivity inside iron-based superconductors. here a gap structure was drastically altered upon replacing one of a spacer layers, indicating a need to understand how a pairing state was tuned by changes of a asymmetry between a pnictogens located either side of a fe planes."
"galaxy evolution should be studied observationally by linking progenitor and descendant galaxies through an evolving cumulative number density selection. this procedure should reproduce a expected evolution of a median stellar mass from abundance matching. however, models predict an increasing scatter inside main progenitor masses at higher redshifts, which makes galaxy selection at a median mass unrepresentative. consequently, there was no guarantee that a evolution of other galaxy properties deduced from this selection are reliable. despite this concern, we show that this procedure approximately reproduces a evolution of a average stellar density profile of main progenitors of m = 10^11.5 msun galaxies, when applied to a eagle hydrodynamical simulation. at z > 3.5 a aperture masses disagree by about the factor two, but this discrepancy disappears when we include a expected scatter inside cumulative number densities. a evolution of a average density profile inside eagle broadly agrees with observations from ultravista and candels, suggesting an inside-out growth history considering these massive galaxies over 0 < z < 5. however, considering z < 2 a inside-out growth trend was stronger inside eagle. we conclude that cumulative number density matching gives reasonably accurate results when applied to a evolution of a mean density profile of massive galaxies."
"music was usually highly structured and it was still an open question how to design models which should successfully learn to recognize and represent musical structure. the fundamental problem was that structurally related patterns should have very distinct appearances, because a structural relationships are often based on transformations of musical material, like chromatic or diatonic transposition, inversion, retrograde, or rhythm change. inside this preliminary work, we study a potential of two unsupervised learning techniques - restricted boltzmann machines (rbms) and gated autoencoders (gaes) - to capture pre-defined transformations from constructed data pairs. we evaluate a models by with the help of a learned representations as inputs inside the discriminative task where considering the given type of transformation (e.g. diatonic transposition), a specific relation between two musical patterns must be recognized (e.g. an upward transposition of diatonic steps). furthermore, we measure a reconstruction error of models when reconstructing musical transformed patterns. lastly, we test a models inside an analogy-making task. we find that it was difficult to learn musical transformations with a rbm and that a gae was much more adequate considering this task, since it was able to learn representations of specific transformations that are largely content-invariant. we believe these results show that models such as gaes may provide a basis considering more encompassing music analysis systems, by endowing them with the better understanding of a structures underlying music."
"monolayer tellurium (te) or tellurene has been suggested by the recent theory as the new two-dimensional (2d) system with great electronic and optoelectronic promises. here we present an experimental study of epitaxial te deposited on highly oriented pyrolytic graphite (hopg) substrate by molecular-beam epitaxy. scanning tunneling microscopy of ultrathin layers of te reveals rectangular surface cells with a cell size consistent with a theoretically predicted beta-tellurene, whereas considering thicker films, a cell size was more consistent with that of a (10-10) surface of bulk te crystal. scanning tunneling spectroscopy measurements show a films are semiconductors with a energy bandgaps decreasing with increasing film thickness, and a gap narrowing occurs predominantly at a valance-band maximum (vbm). a latter was understood by strong coupling of states at a vbm but the weak coupling at conduction band minimum (cbm) as revealed by density functional theory calculations."
"eos family is created during the catastrophic impact about 1.3 gyr ago. rotation states of individual family members contain information about a history of a whole population. we aim to increase a number of asteroid shape models and rotation states within a eos collision family, as well as to revise previously published shape models from a literature. such results should be used to constrain theoretical collisional and evolution models of a family, or to approximate other physical parameters by the thermophysical modeling of a thermal infrared data. we use all available disk-integrated optical data (i.e., classical dense-in-time photometry obtained from public databases and through the large collaboration network as well as sparse-in-time individual measurements from the few sky surveys) as input considering a convex inversion method, and derive 3d shape models of asteroids together with their rotation periods and orientations of rotation axes. we present updated shape models considering 15 asteroids and new shape model determinations considering 16 asteroids. together with a already published models from a publicly available damit database, we compiled the sample of 56 eos family members with known shape models that we used inside our analysis of physical properties within a family. rotation states of asteroids smaller than ~20 km are heavily influenced by a yorp effect, whilst a large objects more or less retained their rotation state properties since a family creation. moreover, we also present the shape model and bulk density of asteroid (423) diotima, an interloper inside a eos family, based on a disk-resolved data obtained by a near infrared camera (nirc2) mounted on a w.m. keck ii telescope."
"we consider a paradigm of the black box ai system that makes life-critical decisions. we propose an ""arguing machines"" framework that pairs a primary ai system with the secondary one that was independently trained to perform a same task. we show that disagreement between a two systems, without any knowledge of underlying system design or operation, was sufficient to arbitrarily improve a accuracy of a overall decision pipeline given human supervision over disagreements. we demonstrate this system inside two applications: (1) an illustrative example of image classification and (2) on large-scale real-world semi-autonomous driving data. considering a first application, we apply this framework to image classification achieving the reduction from 8.0% to 2.8% top-5 error on imagenet. considering a second application, we apply this framework to tesla autopilot and demonstrate a ability to predict 90.4% of system disengagements that were labeled by human annotators as challenging and needing human supervision."
"when a stratospheric observatory considering infrared astronomy (sofia) is conceived and its first science cases defined, exoplanets had not been detected. later studies, however, showed that optical and near-infrared photometric and spectrophotometric follow-up observations during planetary transits and eclipses are feasible with sofia's instrumentation, inside particular with a hipo-flitecam and fpi+ optical and near infrared (nir) instruments. additionally, a airborne-based platform sofia has the number of unique advantages when compared to other ground- and space-based observatories inside this field of research. here we will outline these theoretical advantages, present some sample science cases and a results of two observations from sofia's first five observation cycles -- an observation of a hot jupiter hd 189733b with hipo and an observation of a super-earth gj 1214b with flipo and fpi+. based on these early products available to this science case, we evaluate sofia's potential and future perspectives inside a field of optical and infrared exoplanet spectrophotometry inside a stratosphere."
"we propose an orthogonal series density estimator considering complex surveys, where samples are neither independent nor identically distributed. a proposed estimator was proved to be design-unbiased and asymptotically design-consistent. a asymptotic normality was proved under both design and combined spaces. two data driven estimators are proposed based on a proposed oracle estimator. we show a efficiency of a proposed estimators inside simulation studies. the real survey data example was provided considering an illustration."
"the dirichlet $k$-partition of the domain $u \subseteq \mathbb{r}^d$ was the collection of $k$ pairwise disjoint open subsets such that a sum of their first laplace-dirichlet eigenvalues was minimal. the discrete version of dirichlet partitions has been posed on graphs with applications inside data analysis. both versions admit variational formulations: solutions are characterized by minimizers of a dirichlet energy of mappings from $u$ into the singular space $\sigma_k \subseteq \mathbb{r}^k$. inside this paper, we extend results of n.\ garcía trillos and d.\ slepčev to show that there exist solutions of a continuum problem arising as limits to solutions of the sequence of discrete problems. specifically, the sequence of points $\{x_i\}_{i \in \mathbb{n}}$ from $u$ was sampled i.i.d.\ with respect to the given probability measure $\nu$ on $u$ and considering all $n \in \mathbb{n}$, the geometric graph $g_n$ was constructed from a first $n$ points $x_1, x_2, \ldots, x_n$ and a pairwise distances between a points. with probability one with respect to a choice of points $\{x_i\}_{i \in \mathbb{n}}$, we show that as $n \to \infty$ a discrete dirichlet energies considering functions $g_n \to \sigma_k$ $\gamma$-converge to (a scalar multiple of) a continuum dirichlet energy considering functions $u \to \sigma_k$ with respect to the metric coming from a theory of optimal transport. this, along with the compactness property considering a aforementioned energies that we prove, implies a convergence of minimizers. when $\nu$ was a uniform distribution, our results also imply a statistical consistency statement that dirichlet partitions of geometric graphs converge to partitions of a sampled space inside a hausdorff sense."
"a processes that led to a formation of a planetary bodies inside a solar system are still not fully understood. with the help of a results obtained with a comprehensive suite of instruments on-board esa's rosetta mission, we present evidence that comet 67p/churyumov-gerasimenko likely formed through a gentle gravitational collapse of the bound clump of mm-sized dust aggregates (""pebbles""), intermixed with microscopic ice particles. this formation scenario leads to the cometary make-up that was simultaneously compatible with a global porosity, homogeneity, tensile strength, thermal inertia, vertical temperature profiles, sizes and porosities of emitted dust, and a steep increase inside water-vapour production rate with decreasing heliocentric distance, measured by a instruments on-board a rosetta spacecraft and a philae lander. our findings suggest that a pebbles observed to be abundant inside protoplanetary discs around young stars provide a building material considering comets and other minor bodies."
"inside this paper, we have discussed the quantum idea behind the method considering a all-pair multiclass classification problem. we have shown that a multiclass support vector machine considering big data classification with the quantum all-pair idea behind the method should be implemented inside logarithm runtime complexity on the quantum computer. inside an all-pair approach, there was one binary classification problem considering each pair of classes, and so there are k (k-1)/2 classifiers considering the k-class problem. as compared to a classical multiclass support vector machine that should be implemented with polynomial run time complexity, our idea behind the method exhibits exponential speed up inside a quantum version. a quantum all-pair algorithm should be used with other classification algorithms, and the speed up gain should be achieved as compared to their classical counterparts."
"we classify all cubic extensions of any field of arbitrary characteristic, up to isomorphism, using an explicit construction involving three fundamental types of cubic forms. we deduce the classification of any galois cubic extension of the field. a splitting and ramification of places inside the separable cubic extension of any global function field are completely determined, and precise riemann-hurwitz formulae are given. inside doing so, we determine a decomposition of any cubic polynomial over the finite field."
we present the family of easily computable upper bounds considering a holevo quantity of ensemble of quantum states depending on the reference state as the free parameter. these upper bounds are obtained by combining probabilistic and metric characteristics of a ensemble. we show that appropriate choice of a reference state gives tight upper bounds considering a holevo quantity which inside many cases improve existing estimates inside a literature. we also present upper bound considering a holevo quantity of the generalized ensemble of quantum states with finite average energy depending on metric divergence of a ensemble. a specification of this upper bound considering a multi-mode quantum oscillator was tight considering large energy. a above results are used to obtain tight upper bounds considering a holevo capacity of finite-dimensional and infinite-dimensional energy-constrained quantum channels depending on metric characteristics of a channel output.
"most previous work of centralities focuses on metrics of vertex importance and methods considering identifying powerful vertices, while related work considering edges was much lesser, especially considering weighted networks, due to a computational challenge. inside this paper, we propose to use a well-known kirchhoff index as a measure of edge centrality inside weighted networks, called $\theta$-kirchhoff edge centrality. a kirchhoff index of the network was defined as a sum of effective resistances over all vertex pairs. a centrality of an edge $e$ was reflected inside a increase of kirchhoff index of a network when a edge $e$ was partially deactivated, characterized by the parameter $\theta$. we define two equivalent measures considering $\theta$-kirchhoff edge centrality. both are global metrics and have the better discriminating power than commonly used measures, based on local or partial structural information of networks, e.g. edge betweenness and spanning edge centrality. despite a strong advantages of kirchhoff index as the centrality measure and its wide applications, computing a exact value of kirchhoff edge centrality considering each edge inside the graph was computationally demanding. to solve this problem, considering each of a $\theta$-kirchhoff edge centrality metrics, we present an efficient algorithm to compute its $\epsilon$-approximation considering all a $m$ edges inside nearly linear time inside $m$. a proposed $\theta$-kirchhoff edge centrality was a first global metric of edge importance that should be provably approximated inside nearly-linear time. moreover, according to a $\theta$-kirchhoff edge centrality, we present the $\theta$-kirchhoff vertex centrality measure, as well as the fast algorithm that should compute $\epsilon$-approximate kirchhoff vertex centrality considering all a $n$ vertices inside nearly linear time inside $m$."
"a lack of interpretability often makes black-box models difficult to be applied to many practical domains. considering this reason, a current work, from a black-box model input port, proposes to incorporate data-based prior information into a black-box soft-margin svm model to enhance its interpretability. a concept and incorporation mechanism of data-based prior information are successively developed, based on which a interpretable or partly interpretable svm optimization model was designed and then solved through handily rewriting a optimization problem as the nonlinear quadratic programming problem. an algorithm considering mining data-based linear prior information from data set was also proposed, which generates the linear expression with respect to two appropriate inputs identified from all inputs of system. at last, a proposed interpretability enhancement strategy was applied to eight benchmark examples considering effectiveness exhibition."
"diffusion and flow-driven instability, or transport-driven instability, was one of a central mechanisms to generate inhomogeneous gradient of concentrations inside spatially distributed chemical systems. however, verifying a transport-driven instability of reaction-diffusion-advection systems requires checking a jacobian eigenvalues of infinitely many fourier modes, which was computationally intractable. to overcome this limitation, this paper proposes mathematical optimization algorithms that determine a stability/instability of reaction-diffusion-advection systems by finite steps of algebraic calculations. specifically, a stability/instability analysis of fourier modes was formulated as the sum-of-squares (sos) optimization program, which was the class of convex optimization whose solvers are widely available as software packages. a optimization program was further extended considering facile computation of a destabilizing spatial modes. this extension allows considering predicting and designing a shape of concentration gradient without simulating a governing equations. a streamlined analysis process of self-organized pattern formation was demonstrated with the simple illustrative reaction model with diffusion and advection."
"graphs are an important tool to model data inside different domains, including social networks, bioinformatics and a world wide web. most of a networks formed inside these domains are directed graphs, where all a edges have the direction and they are not symmetric. betweenness centrality was an important index widely used to analyze networks. inside this paper, first given the directed network $g$ and the vertex $r \in v(g)$, we propose the new exact algorithm to compute betweenness score of $r$. our algorithm pre-computes the set $\mathcal{rv}(r)$, which was used to prune the huge amount of computations that do not contribute inside a betweenness score of $r$. time complexity of our exact algorithm depends on $|\mathcal{rv}(r)|$ and it was respectively $\theta(|\mathcal{rv}(r)|\cdot|e(g)|)$ and $\theta(|\mathcal{rv}(r)|\cdot|e(g)|+|\mathcal{rv}(r)|\cdot|v(g)|\log |v(g)|)$ considering unweighted graphs and weighted graphs with positive weights. $|\mathcal{rv}(r)|$ was bounded from above by $|v(g)|-1$ and inside most cases, it was the small constant. then, considering a cases where $\mathcal{rv}(r)$ was large, we present the simple randomized algorithm that samples from $\mathcal{rv}(r)$ and performs computations considering only a sampled elements. we show that this algorithm provides an $(\epsilon,\delta)$-approximation of a betweenness score of $r$. finally, we perform extensive experiments over several real-world datasets from different domains considering several randomly chosen vertices as well as considering a vertices with a highest betweenness scores. our experiments reveal that inside most cases, our algorithm significantly outperforms a most efficient existing randomized algorithms, inside terms of both running time and accuracy. our experiments also show that our proposed algorithm computes betweenness scores of all vertices inside a sets of sizes 5, 10 and 15, much faster and more accurate than a most efficient existing algorithms."
"changing institution was the scientist's key career decision, which plays an important role inside education, scientific productivity, and a generation of scientific knowledge. yet, our understanding of a factors influencing the relocation decision was very limited. inside this paper we investigate how a scientific profile of the scientist determines their decision to move (i.e., change institution). to this aim, we describe the scientist's profile by three main aspects: a scientist's recent scientific career, a quality of their scientific environment and a structure of their scientific collaboration network. we then design and implement the two-stage predictive model: first, we use data mining to predict which researcher will move inside a next year on a basis of their scientific profile; second we predict which institution they will choose by with the help of the novel social-gravity model, an adaptation of a traditional gravity model of human mobility. experiments on the massive dataset of scientific publications show that our idea behind the method performs well inside both a stages, resulting inside the 85% reduction of a prediction error with respect to a state-of-the-art approaches."
"we consider the stochastic nonlinear schrödinger equation with multiplicative noise inside an abstract framework that covers subcritical focusing and defocusing stochastic nls inside $h^1$ on compact manifolds and bounded domains. we construct the martingale solution with the help of the modified faedo-galerkin-method based on a littlewood-paley-decomposition. considering 2d manifolds with bounded geometry, we use strichartz estimates to show pathwise uniqueness."
"we explore the new mechanism to explain polarization phenomena inside opinion dynamics inside which agents evaluate alternative views on a basis of a social feedback obtained on expressing them. high support of a favored opinion inside a social environment, was treated as the positive feedback which reinforces a value associated to this opinion. inside connected networks of sufficiently high modularity, different groups of agents should form strong convictions of competing opinions. linking a social feedback process to standard equilibrium concepts we analytically characterize sufficient conditions considering a stability of bi-polarization. while previous models have emphasized a polarization effects of deliberative argument-based communication, our model highlights an affective experience-based route to polarization, without assumptions about negative influence or bounded confidence."
"with the help of the mix of numerical and analytic methods, we show that recent nmr $^{17}$o measurements provide detailed information about a structure of a charge-density wave (cdw) phase inside underdoped yba$_2$cu$_3$o$_{6+x}$. we perform bogoliubov-de gennes (bdg) calculations of both a local density of states and a orbitally resolved charge density, which are closely related to a magnetic and electric quadrupole contributions to a nmr spectrum, with the help of the microscopic model that is shown previously to agree closely with x-ray experiments. a bdg results reproduce qualitative features of a experimental spectrum extremely well. these results are interpreted inside terms of the generic ""hotspot"" model that allows one to trace a origins of a nmr lineshapes. we find that four quantities---the orbital character of a fermi surface at a hotspots, a fermi surface curvature at a hotspots, a cdw correlation length, and a magnitude of a subdominant cdw component---are key inside determining a lineshapes."
"the nonuniform neumann boundary-value problem was considered considering a poisson equation inside the thin $3d$ aneurysm-type domain that consists of thin curvilinear cylinders that are joined through an aneurysm of diameter $\mathcal{o}(\varepsilon).$ the rigorous procedure was developed to construct a complete asymptotic expansion considering a solution as a parameter $\varepsilon \to 0.$ a asymptotic expansion consists of the regular part that was located in of each cylinder, the boundary-layer part near a base of each cylinder, and an inner part discovered inside the neighborhood of a aneurysm. terms of a inner part of a asymptotics are special solutions of boundary-value problems inside an unbounded domain with different outlets at infinity. it turns out that they have polynomial growth at infinity. by matching these parts, we derive a limit problem $(\varepsilon =0)$ inside a corresponding graph and the recurrence procedure to determine all terms of a asymptotic expansion. energetic and uniform pointwise estimates are proved. these estimates allow us to observe a impact of a aneurysm."
"dimension reduction and visualization was the staple of data analytics. methods such as principal component analysis (pca) and multidimensional scaling (mds) provide low dimensional (ld) projections of high dimensional (hd) data while preserving an hd relationship between observations. traditional biplots assign meaning to a ld space of the pca projection by displaying ld axes considering a attributes. these axes, however, are specific to a linear projection used inside pca. mds projections, which allow considering arbitrary stress and dissimilarity functions, require special care when labeling a ld space. we propose an iterative scheme to plot an ld axis considering each attribute based on a user-specified stress and dissimilarity metrics. we discuss a details of our general biplot methodology, its relationship with pca-derived biplots, and provide examples with the help of real data."
"we consider a problem of decentralized consensus optimization, where a sum of $n$ smooth and strongly convex functions are minimized over $n$ distributed agents that form the connected network. inside particular, we consider a case that a communicated local decision variables among nodes are quantized inside order to alleviate a communication bottleneck inside distributed optimization. we propose a quantized decentralized gradient descent (qdgd) algorithm, inside which nodes update their local decision variables by combining a quantized information received from their neighbors with their local information. we prove that under standard strong convexity and smoothness assumptions considering a objective function, qdgd achieves the vanishing mean solution error under customary conditions considering quantizers. to a best of our knowledge, this was a first algorithm that achieves vanishing consensus error inside a presence of quantization noise. moreover, we provide simulation results that show tight agreement between our derived theoretical convergence rate and a numerical results."
a eigenstructure of a discrete fourier transform (dft) was examined and new systematic procedures to generate eigenvectors of a unitary dft are proposed. dft eigenvectors are suggested as user signatures considering data communication over a real adder channel (rac). a proposed multiuser communication system over a 2-user rac was detailed.
"we study a skip-thought model with neighborhood information as weak supervision. more specifically, we propose the skip-thought neighbor model to consider a adjacent sentences as the neighborhood. we train our skip-thought neighbor model on the large corpus with continuous sentences, and then evaluate a trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. both quantitative comparison and qualitative investigation are conducted. we empirically show that, our skip-thought neighbor model performs as well as a skip-thought model on evaluation tasks. inside addition, we found that, incorporating an autoencoder path inside our model didn't aid our model to perform better, while it hurts a performance of a skip-thought model."
several conjectural continued fractions found with a aid of various algorithms are published inside this paper.
"transferring solutions found by trajectory optimization to robotic hardware remains the challenging task. when a optimization fully exploits a provided model to perform dynamic tasks, a presence of unmodeled dynamics renders a motion infeasible on a real system. model errors should be the result of model simplifications, but also naturally arise when deploying a robot inside unstructured and nondeterministic environments. predominantly, compliant contacts and actuator dynamics lead to bandwidth limitations. bandwidth limits arising from compliant contacts and actuator dynamics tend to occur at high frequencies. while classical control methods provide tools to synthesize controllers that are robust to the class of model errors, such the notion was missing inside modern trajectory optimization, which was solved inside a time domain. we propose frequency-shaped cost functions to achieve robust solutions inside a context of optimal control considering legged robots. through simulation and hardware experiments we show that motion plans should be made compatible with bandwidth limits set by actuators and contact dynamics. a smoothness of a model predictive solutions should be continuously tuned without compromising a feasibility of a problem. experiments with a quadrupedal robot anymal, which was driven by high-compliant series elastic actuators, showed significantly better tracking performance of a planned motion, torque, and force trajectories and enabled a machine to walk robustly on ground with unmodeled compliance."
"we apply a tractor image modeling code to improve upon existing multi-band photometry considering a spitzer extragalactic representative volume survey (servs). servs consists of post-cryogenic spitzer observations at 3.6 and 4.5 micron over five well-studied deep fields spanning 18 square degrees. inside concert with data from ground-based near-infrared (nir) and optical surveys, servs aims to provide the census of a properties of massive galaxies out to z ~ 5. to accomplish this, we are with the help of a tractor to perform ""forced photometry."" this technique employs prior measurements of source positions and surface brightness profiles from the high-resolution fiducial band from a vista deep extragalactic observations (video) survey to model and fit a fluxes at lower-resolution bands. we discuss our implementation of a tractor over the square degree test region within a xmm-lss field with deep imaging inside 12 nir/optical bands. our new multi-band source catalogs offer the number of advantages over traditional position-matched catalogs, including 1) consistent source cross-identification between bands, 2) de-blending of sources that are clearly resolved inside a fiducial band but blended inside a lower-resolution servs data, 3) the higher source detection fraction inside each band, 4) the larger number of candidate galaxies inside a redshift range 5 < z < 6, and 5) the statistically significant improvement inside a photometric redshift accuracy as evidenced by a significant decrease inside a fraction of outliers compared to spectroscopic redshifts. thus, forced photometry with the help of a tractor offers the means of improving a accuracy of multi-band extragalactic surveys designed considering galaxy evolution studies. we will extend our application of this technique to a full servs footprint inside a future."
"gravitation, a universal attractive force, acts upon all matter (and radiation) relentlessly. left to itself, gravity would pull everything together and a universe would be nothing but the gigantic black hole. nature throws almost every bit of physics - rotation, magnetic field, heat, quantum effects and so on, at gravity to escape such the fate. inside this series of articles we shall explore systems where a eternal pull of gravity has been held off by one or another such means."
"we demonstrate the thermodynamic formulation to quantify defect formation energetics inside an insulator under high electric field. as the model system, we analyzed neutral oxygen vacancies (color centers) inside alkaline-earth-metal binary oxides with the help of density functional theory, berry phase calculations, and maximally localized wannier functions. work of polarization lowers a field dependent electric gibbs energy of formation of this defect. this was attributed mainly to a ease of polarizing a two electrons trapped inside a vacant site, and secondarily to a defect induced reduction inside bond stiffness and softening of phonon modes. a formulation and analysis have implications considering understanding a behavior of insulating oxides inside electronic, magnetic, catalytic, and electrocaloric devices under high electric field."
"a effects of mhd boundary layer flow of non-linear thermal radiation with convective heat transfer and non-uniform heat source/sink inside presence of thermophortic velocity and chemical reaction investigated inside this study. suitable similarity transformation are used to solve a partial ordinary differential equation of considered governing flow. runge-kutta fourth fifth order fehlberg method with shooting techniques are used to solved non-dimensional governing equations. a variation of different parameters such as thermophoretic parameter, chemical reaction parameter, non- uniform heat source/sink parameters are studied on velocity, temperature and concentration profiles, and are described by suitable graphs and tables. a obtained results are inside very well agreement with previous results."
"a sparsity and compressibility of finite-dimensional signals are of great interest inside fields such as compressed sensing. a notion of compressibility was also extended to infinite sequences of i.i.d. or ergodic random variables based on a observed error inside their nonlinear k-term approximation. inside this work, we use a entropy measure to study a compressibility of continuous-domain innovation processes (alternatively known as white noise). specifically, we define such the measure as a entropy limit of a doubly quantized (time and amplitude) process. this provides the tool to compare a compressibility of various innovation processes. it also allows us to identify an analogue of a concept of ""entropy dimension"" which is originally defined by rényi considering random variables. particular attention was given to stable and impulsive poisson innovation processes. here, our results recognize poisson innovations as a more compressible ones with an entropy measure far below that of stable innovations. while this result departs from a previous knowledge regarding a compressibility of fat-tailed distributions, our entropy measure ranks stable innovations according to their tail decay."
model predictive control (mpc) method was the class of advanced control techniques most widely applied inside industry. a major advantages of a mpc are its straightforward procedure which should be applied considering both linear and nonlinear system. this paper proposes a use of mpc considering voltage source converter (vsc) inside the high voltage direct current (hvdc) system. the mpc controller was modeled based on a state-space model of the single vsc-hvdc station including a dynamics of a main ac grid. the full scale nonlinear switching model of point-to-point connected vsc-based hvdc system was developed inside matlab/simulink association with simpower system to demonstrate a application of a proposed controller.
"automatic continuous time, continuous value assessment of the patient's pain from face video was highly sought after by a medical profession. despite a recent advances inside deep learning that attain impressive results inside many domains, pain approximation risks not being able to benefit from this due to a difficulty inside obtaining data sets of considerable size. inside this work we propose the combination of hand-crafted and deep-learned features that makes a most of deep learning techniques inside small sample settings. encoding shape, appearance, and dynamics, our method significantly outperforms a current state of a art, attaining the rmse error of less than 1 point on the 16-level pain scale, whilst simultaneously scoring the 67.3% pearson correlation coefficient between our predicted pain level time series and a ground truth."
"we study a behavior of the fundamental tool inside sparse statistical modeling --the best-subset selection procedure (aka ""best-subsets""). assuming that a underlying linear model was sparse, it was well known, both inside theory and inside practice, that a best-subsets procedure works extremely well inside terms of several statistical metrics (prediction, approximation and variable selection) when a signal to noise ratio (snr) was high. however, its performance degrades substantially when a snr was low -- it was outperformed inside predictive accuracy by continuous shrinkage methods, such as ridge regression and a lasso. we explain why this behavior should not come as the surprise, and contend that a original version of a classical best-subsets procedure was, perhaps, not designed to be used inside a low snr regimes. we propose the close cousin of best-subsets, namely, its $\ell_{q}$-regularized version, considering $q \in\{1, 2\}$, which (a) mitigates, to the large extent, a poor predictive performance of best-subsets inside a low snr regimes; (b) performs favorably and generally delivers the substantially sparser model when compared to a best predictive models available using ridge regression and a lasso. our estimator should be expressed as the solution to the mixed integer second order conic optimization problem and, hence, was amenable to modern computational tools from mathematical optimization. we explore a theoretical properties of a predictive capabilities of a proposed estimator and complement our findings using several numerical experiments."
"we consider the restless multi-armed bandit (rmab) inside which there are two types of arms, say the and b. each arm should be inside one of two states, say $0$ or $1.$ playing the type the arm brings it to state $0$ with probability one and not playing it induces state transitions with arm-dependent probabilities. whereas playing the type b arm leads it to state $1$ with probability $1$ and not playing it gets state that dependent on transition probabilities of arm. further, play of an arm generates the unit reward with the probability that depends on a state of a arm. a belief about a state of a arm should be calculated with the help of the bayesian update after every play. this rmab has been designed considering use inside recommendation systems where a user's preferences depend on a history of recommendations. this rmab should also be used inside applications like creating of playlists or placement of advertisements. inside this paper we formulate a long term reward maximization problem as infinite horizon discounted reward and average reward problem. we analyse a rmab by first studying discounted reward scenario. we show that it was whittle-indexable and then obtain the closed form expression considering a whittle index considering each arm calculated from a belief about its state and a parameters that describe a arm. we next analyse a average reward problem with the help of vanishing discounted idea behind the method and derive a closed form expression considering whittle index. considering the rmab to be useful inside practice, we need to be able to learn a parameters of a arms. we present an algorithm derived from thompson sampling scheme, that learns a parameters of a arms and also illustrate its performance numerically."
"a popular adjusted rand index (ari) was extended to a task of simultaneous clustering of a rows and columns of the given matrix. this new index called coclustering adjusted rand index (cari) remains convenient and competitive facing other indices. indeed, partitions with high number of clusters should be considered and it does not require any convention when a numbers of clusters inside partitions are different. experiments on simulated partitions are presented and a performance of this index to measure a agreement between two pairs of partitions was assessed. comparison with other indices was discussed."
"today, smartphone devices are owned by the large portion of a population and have become the very popular platform considering accessing a internet. smartphones provide a user with immediate access to information and services. however, they should easily expose a user to many privacy risks. applications that are installed on a device and entities with access to a device's internet traffic should reveal private information about a smartphone user and steal sensitive content stored on a device or transmitted by a device over a internet. inside this paper, we present the method to reveal various demographics and technical computer skills of smartphone users by their internet traffic records, with the help of machine learning classification models. we implement and evaluate a method on real life data of smartphone users and show that smartphone users should be classified by their gender, smoking habits, software programming experience, and other characteristics."
"we have developed an efficient active galactic nucleus (agn) selection method with the help of 18-band spectral energy distribution (sed) fitting inside mid-infrared (mid-ir). agns are often obscured by gas and dust, and those obscured agns tend to be missed inside optical, uv and soft x-ray observations. mid-ir light should aid us to recover them inside an obscuration free way with the help of their thermal emission. on a other hand, star-forming galaxies (sfg) also have strong pah emission features inside mid-ir. hence, establishing an accurate method to separate populations of agn and sfg was important. however, inside previous mid-ir surveys, only 3 or 4 filters were available, and thus a selection is limited. we combined akari's continuous 9 mid-ir bands with wise and spitzer data to create 18 mid-ir bands considering agn selection. among 4682 galaxies inside a akari nep deep field, 1388 are selected to be agn hosts, which implies an agn fraction of 29.6$\pm$0.8$\%$ (among them 47$\%$ are seyfert 1.8 and 2). comparing a result from sed fitting into wise and spitzer colour-colour diagram reveals that seyferts are often missed by previous studies. our result has been tested by stacking median magnitude considering each sample. with the help of x-ray data from chandra, we compared a result of our sed fitting with wise's colour box selection. we recovered more x-ray detected agn than previous methods by 20$\%$."
"we report constraints on a global $21$ cm signal due to neutral hydrogen at redshifts $14.8 \geq z \geq 6.5$. we derive our constraints from low foreground observations of a average sky brightness spectrum conducted with a edges high-band instrument between september $7$ and october $26$, $2015$. observations were calibrated by accounting considering a effects of antenna beam chromaticity, antenna and ground losses, signal reflections, and receiver parameters. we evaluate a consistency between a spectrum and phenomenological models considering a global $21$ cm signal. considering tanh-based representations of a ionization history during a epoch of reionization, we rule out, at $\geq2\sigma$ significance, models with duration of up to $\delta z = 1$ at $z\approx8.5$ and higher than $\delta z = 0.4$ across most of a observed redshift range under a usual assumption that a $21$ cm spin temperature was much larger than a temperature of a cosmic microwave background (cmb) during reionization. we also investigate the `cold' igm scenario that assumes perfect ly$\alpha$ coupling of a $21$ cm spin temperature to a temperature of a intergalactic medium (igm), but that a igm was not heated by early stars or stellar remants. under this assumption, we reject tanh-based reionization models of duration $\delta z \lesssim 2$ over most of a observed redshift range. finally, we explore and reject the broad range of gaussian models considering a $21$ cm absorption feature expected inside a first light era. as an example, we reject $100$ mk gaussians with duration (full width at half maximum) $\delta z \leq 4$ over a range $14.2\geq z\geq 6.5$ at $\geq2\sigma$ significance."
"we discuss a relative merits of optimistic and randomized approaches to exploration inside reinforcement learning. optimistic approaches presented inside a literature apply an optimistic boost to a value approximate at each state-action pair and select actions that are greedy with respect to a resulting optimistic value function. randomized approaches sample from among statistically plausible value functions and select actions that are greedy with respect to a random sample. prior computational experience suggests that randomized approaches should lead to far more statistically efficient learning. we present two simple analytic examples that elucidate why this was a case. inside principle, there should be optimistic approaches that fare well relative to randomized approaches, but that would require intractable computation. optimistic approaches that have been proposed inside a literature sacrifice statistical efficiency considering a sake of computational efficiency. randomized approaches, on a other hand, may enable simultaneous statistical and computational efficiency."
"recent successes inside reinforcement learning have lead to a development of complex controllers considering real-world robots. as these robots are deployed inside safety-critical applications and interact with humans, it becomes critical to ensure safety inside order to avoid causing harm. the first step inside this direction was to test a controllers inside simulation. to be able to do this, we need to capture what we mean by safety and then efficiently search a space of all behaviors to see if they are safe. inside this paper, we present an active-testing framework based on bayesian optimization. we specify safety constraints with the help of logic and exploit structure inside a problem inside order to test a system considering adversarial counter examples that violate a safety specifications. these specifications are defined as complex boolean combinations of smooth functions on a trajectories and, unlike reward functions inside reinforcement learning, are expressive and impose hard constraints on a system. inside our framework, we exploit regularity assumptions on individual functions inside form of the gaussian process (gp) prior. we combine these into the coherent optimization framework with the help of problem structure. a resulting algorithm was able to provably verify complex safety specifications or alternatively find counter examples. experimental results show that a proposed method was able to find adversarial examples quickly."
"an evaluation metric was an absolute necessity considering measuring a performance of any system and complexity of any data. inside this paper, we have discussed how to determine a level of complexity of code-mixed social media texts that are growing rapidly due to multilingual interference. inside general, texts written inside multiple languages are often hard to comprehend and analyze. at a same time, inside order to meet a demands of analysis, it was also necessary to determine a complexity of the particular document or the text segment. thus, inside a present paper, we have discussed a existing metrics considering determining a code-mixing complexity of the corpus, their advantages, and shortcomings as well as proposed several improvements on a existing metrics. a new index better reflects a variety and complexity of the multilingual document. also, a index should be applied to the sentence and seamlessly extended to the paragraph or an entire document. we have employed two existing code-mixed corpora to suit a requirements of our study."
"we present an overview of scenarios where a observed dark matter (dm) abundance consists of feebly interacting massive particles (fimps), produced non-thermally by a so-called freeze-in mechanism. inside contrast to a usual freeze-out scenario, frozen-in fimp dm interacts very weakly with a particles inside a visible sector and never attained thermal equilibrium with a baryon-photon fluid inside a early universe. instead of being determined by its annihilation strength, a dm abundance depends on a decay and annihilation strengths of particles inside equilibrium with a baryon-photon fluid, as well as couplings inside a dm sector. this makes frozen-in dm very difficult but not impossible to test. inside this review, we present a freeze-in mechanism and its variations considered inside a literature (dark freeze-out and reannihilation), compare them to a standard dm freeze-out scenario, discuss several aspects of model building, and pay particular attention to observational properties and general testability of such feebly interacting dm."
"inside a online multiple testing problem, p-values corresponding to different null hypotheses are observed one by one, and a decision of whether or not to reject a current hypothesis must be made immediately, after which a next p-value was observed. alpha-investing algorithms to control a false discovery rate (fdr), formulated by foster and stine, have been generalized and applied to many settings, including quality-preserving databases inside science and multiple a/b or multi-armed bandit tests considering internet commerce. this paper improves a class of generalized alpha-investing algorithms (gai) inside four ways: (a) we show how to uniformly improve a power of a entire class of monotone gai procedures by awarding more alpha-wealth considering each rejection, giving the win-win resolution to the recent dilemma raised by javanmard and montanari, (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be non-null, (c) we allow considering differing penalties considering false discoveries to indicate that some hypotheses may be more important than others, (d) we define the new quantity called a decaying memory false discovery rate (mem-fdr) that may be more meaningful considering truly temporal applications, and which alleviates problems that we describe and refer to as ""piggybacking"" and ""alpha-death"". our gai++ algorithms incorporate all four generalizations simultaneously, and reduce to more powerful variants of earlier algorithms when a weights and decay are all set to unity. finally, we also describe the simple method to derive new online fdr rules based on an estimated false discovery proportion."
"understanding a dynamics during freezing of nanofluid droplets was of importance from the fundamental and practical viewpoint. while a freezing of the water droplet has been extensively studied, little information was available about a characteristics of freezing the nanofluid droplet. here, we report unique shape changes observed during a freezing process of the nanofluid droplet. instead of forming the pointy tip on a frozen deionized water droplet, we found that a top of the frozen nanofluid droplet exhibits the flat plateau shape and such plateau becomes larger with increasing nanoparticle concentration. we ascribe this characteristic shape change to an outward marangoni flow which moves liquid from a droplet interior to its vertexes, thus engendering a volume redistribution and shape changes. moreover, we also observed the particle ring pattern formed during freezing of the nanofluid droplet, which was similar to a well-known coffee stain phenomenon occurring during evaporation of the water droplet with dispersed particles. we believe our reported phenomena are not only of fundamental interest but have potential applications inside a freezing of nanofluids considering energy storage and developing nanofluid based new microfabrication methods."
"we report an extensive study on a zero field ground state of the powder sample of a pyrochlore $\mathrm{yb_2ti_2o_7}$. the sharp heat capacity anomaly that labels the low temperature phase transition inside this material was observed at 280 mk. neutron diffraction shows that the \emph{quasi-collinear} ferromagnetic order develops below $t_\mathrm{c}$ with the magnetic moment of $0.87(2)\mu_\mathrm{b}$. high resolution inelastic neutron scattering measurements show, below a phase transition temperature, sharp gapped low-lying magnetic excitations coexisting with the remnant quasielastic contribution likely associated with persistent spin fluctuations. moreover, the broad inelastic continuum of excitations at $\sim0.6$ mev was observed from a lowest measured temperature up to at least 2.5 k. at 10 k, a continuum has vanished and the broad quasielastic conventional paramagnetic scattering takes place at a observed energy range. finally, we show that a exchange parameters obtained within a framework of linear spin-wave theory do not accurately describe a observed zero field inelastic neutron scattering data."
"we study a following generalization of singularity categories. let x be the quasi-projective gorenstein scheme with isolated singularities and the the non-commutative resolution of singularities of x inside a sense of van den bergh. we introduce a relative singularity category as a verdier quotient of a bounded derived category of coherent sheaves on the modulo a category of perfect complexes on x. we view it as the measure considering a difference between x and a. a main results of this thesis are a following. (i) we prove an analogue of orlov's localization result inside our setup. if x has isolated singularities, then this reduces a study of a relative singularity categories to a affine case. (ii) we prove hom-finiteness and idempotent completeness of a relative singularity categories inside a complete local situation and determine its grothendieck group. (iii) we give the complete and explicit description of a relative singularity categories when x has only nodal singularities and a resolution was given by the sheaf of auslander algebras. (iv) we study relations between relative singularity categories and classical singularity categories. considering the simple hypersurface singularity and its auslander resolution, we show that these categories determine each other. (v) a developed technique leads to a following `purely commutative' application: the description of iyama & wemyss triangulated category considering rational surface singularities inside terms of a singularity category of a rational double point resolution. (vi) we give the description of singularity categories of gentle algebras."
"recent years have witnessed a rise of many successful e-commerce marketplace platforms like a amazon marketplace, airbnb, uber/lyft, and upwork, where the central platform mediates economic transactions between buyers and sellers. motivated by these platforms, we formulate the set of facility location problems that we term two-sided facility location. inside our model, agents arrive at nodes inside an underlying metric space, where a metric distance between any buyer and seller captures a quality of a corresponding match. a platform posts prices and wages at a nodes, and opens the set of facilities to route a agents to. a agents at any facility are assumed to be matched. a platform ensures high match quality by imposing the distance constraint between the node and a facilities it was routed to. it ensures high service availability by ensuring flow to a facility was at least the pre-specified lower bound. subject to these constraints, a goal of a platform was to maximize a social surplus (or gains from trade) subject to weak budget balance, i.e., profit being non-negative. we present an approximation algorithm considering this problem that yields the $(1 + \epsilon)$ approximation to surplus considering any constant $\epsilon > 0$, while relaxing a match quality (i.e., maximum distance of any match) by the constant factor. we use an lp rounding framework that easily extends to other objectives such as maximizing volume of trade or profit. we justify our models by considering the dynamic marketplace setting where agents arrive according to the stochastic process and have finite patience (or deadlines) considering being matched. we perform queueing analysis to show that considering policies that route agents to facilities and match them, ensuring the low abandonment probability of agents reduces to ensuring sufficient flow arrives at each facility."
"we report on theoretical investigations of carrier scattering asymmetry at ferromagnet-semiconductor junctions. by an analytical $2\times 2$ spin model, we show that, when dresselhaus interactions was included inside a conduction band of iii-v $t_d$ symmetry group semiconductors, a electrons may undergo the difference of transmission vs. a sign of their incident parallel wavevector normal to a in-plane magnetization. this asymmetry was universally scaled by the unique function independent of a spin-orbit strength. this particular feature was reproduced by the multiband $\mathbf{k}\cdot \mathbf{p}$ tunneling transport model. astonishingly, a asymmetry of transmission persists inside a valence band of semiconductors owing to a inner atomic spin-orbit strength and free of asymmetric potentials . we present multiband $14\times 14$ and $30\times 30$ $\mathbf{k}\cdot \mathbf{p}$ tunneling models together with tunneling transport perturbation calculations corroborating these results. those demonstrate that the tunnel spin-current normal to a interface should generate the surface transverse charge current, a so-called anomalous tunnel hall effect."
"a correlation between magnetic properties and microscopic structural aspects inside a diluted magnetic semiconductor ge$_{1-x}$mn$_{x}$te was investigated by x-ray diffraction and magnetization as the function of a mn concentration $x$. a occurrence of high ferromagnetic-transition temperatures inside a rhombohedrally distorted phase of slowly-cooled ge$_{1-x}$mn$_{x}$te was shown to be directly correlated with a formation and coexistence of strongly-distorted mn-poor and weakly-distorted mn-rich regions. it was demonstrated that a weakly-distorted phase fraction was responsible considering a occurrence of high-transition temperatures inside ge$_{1-x}$mn$_{x}$te. when a mn concentration becomes larger, a mn-rich regions start to switch into a undistorted cubic structure, and a transition temperature was suppressed concurrently. by identifying suitable annealing conditions, we successfully increased a transition temperature to above 200 k considering mn concentrations close to a cubic phase. structural data indicate that a weakly-distorted phase fraction should be restored at a expense of a cubic regions upon a enhancement of a transition temperature, clearly establishing a direct link between high-transition temperatures and a weakly-distorted mn-rich phase fraction."
"cooling system plays the critical role inside the modern data center (dc). developing an optimal control policy considering dc cooling system was the challenging task. a prevailing approaches often rely on approximating system models that are built upon a knowledge of mechanical cooling, electrical and thermal management, which was difficult to design and may lead to sub-optimal or unstable performances. inside this paper, we propose utilizing a large amount of monitoring data inside dc to optimize a control policy. to do so, we cast a cooling control policy design into an energy cost minimization problem with temperature constraints, and tap it into a emerging deep reinforcement learning (drl) framework. specifically, we propose an end-to-end cooling control algorithm (cca) that was based on a actor-critic framework and an off-policy offline version of a deep deterministic policy gradient (ddpg) algorithm. inside a proposed cca, an evaluation network was trained to predict an energy cost counter penalized by a cooling status of a dc room, and the policy network was trained to predict optimized control settings when gave a current load and weather information. a proposed algorithm was evaluated on a energyplus simulation platform and on the real data trace collected from a national super computing centre (nscc) of singapore. our results show that a proposed cca should achieve about 11% cooling cost saving on a simulation platform compared with the manually configured baseline control algorithm. inside a trace-based study, we propose the de-underestimation validation mechanism as we cannot directly test a algorithm on the real dc. even though with due a results are conservative, we should still achieve about 15% cooling energy saving on a nscc data trace if we set a inlet temperature threshold at 26.6 degree celsius."
"we propose the general framework considering interactively learning models, such as (binary or non-binary) classifiers, orderings/rankings of items, or clusterings of data points. our framework was based on the generalization of angluin's equivalence query model and littlestone's online learning model: inside each iteration, a algorithm proposes the model, and a user either accepts it or reveals the specific mistake inside a proposal. a feedback was correct only with probability $p > 1/2$ (and adversarially incorrect with probability $1 - p$), i.e., a algorithm must be able to learn inside a presence of arbitrary noise. a algorithm's goal was to learn a ground truth model with the help of few iterations. our general framework was based on the graph representation of a models and user feedback. to be able to learn efficiently, it was sufficient that there be the graph $g$ whose nodes are a models and (weighted) edges capture a user feedback, with a property that if $s, s^*$ are a proposed and target models, respectively, then any (correct) user feedback $s'$ must lie on the shortest $s$-$s^*$ path inside $g$. under this one assumption, there was the natural algorithm reminiscent of a multiplicative weights update algorithm, which will efficiently learn $s^*$ even inside a presence of noise inside a user's feedback. from this general result, we rederive with barely any extra effort classic results on learning of classifiers and the recent result on interactive clustering; inside addition, we easily obtain new interactive learning algorithms considering ordering/ranking."
"this paper computes a obstruction to a existence of equivariant extensions of basic gerbes over non-simply connected compact simple lie groups. by modifying the (finite dimensional) construction of gawȩdzki-reis [j. geom. phys. 50(1):28-55, 2004], we exhibit basic equivariant bundle gerbes over non-simply connected compact simple lie groups."
"we describe the fast approximation algorithm considering a $\delta$-separated sparsity projection problem. a $\delta$-separated sparsity model is introduced by hegde, duarte and cevher (2009) to capture a firing process of the single poisson neuron with absolute refractoriness. a running time of our projection algorithm was linear considering an arbitrary (but fixed) precision and it was both the head and the tail approximation. this solves the problem of hegde, indyk and schmidt (2015). we also describe how our algorithm fits into a approximate model iterative hard tresholding framework of hegde, indyk and schmidt (2014) that allows to recover $\delta$-separated sparse signals from noisy random linear measurements. a resulting recovery algorithm was substantially faster than a existing one, at least considering large data sets."
"inside this paper, we will show that (1) a results about a fuzzy reasoning algoritm obtained inside a paper ""computer sciences vol. 34, no.4, pp.145-148, 2007"" according to a paper ""ieee transactions on systems, man and cybernetics, 18, pp.1049-1056, 1988"" are correct; (2) example 2 inside a paper ""an algorithm of general fuzzy inference with a reductive property"" presented by he ying-si, quan hai-jin and deng hui-wen according to a paper ""an approximate analogical reasoning idea behind the method based on similarity measures"" presented by tursken i.b. and zhong zhao was incorrect; (3) a mistakes inside their paper are modified and then the calculation example of fmt was supplemented."
"robots have gained relevance inside society, increasingly performing critical tasks. nonetheless, robot security was being underestimated. robotics security was the complex landscape, which often requires the cross-disciplinar perspective to which classical security lags behind. to address this issue, we present a robot security framework (rsf), the methodology to perform systematic security assessments inside robots. we propose, adapt and develop specific terminology and provide guidelines to enable the holistic security assessment following four main layers (physical, network, firmware and application). we argue that modern robotics should regard as equally relevant internal and external communication security. finally, we advocate against ""security by obscurity"". we conclude that a field of security inside robotics deserves further research efforts."
"we report neutron diffraction and transport results on a newly discovered superconducting nitride thfeasn with $t_c=$ 30 k. no magnetic transition, but the weak structural distortion around 160 k, was observed cooling from 300 k to 6 k. analysis on a resistivity, hall transport and crystal structure suggests this material behaves as an electron optimally doped pnictide superconductors due to extra electrons from nitrogen deficiency or oxygen occupancy at a nitrogen site, which together with a low arsenic height may enhance a electron itinerancy and reduce a electron correlations, thus suppress a static magnetic order."
"we have grown single crystals of a iron-based ladder material bafe_2s_3, which was superconductive under high pressure, adopting different conditions. by comparing a behaviors of these samples, it was found that a inside situ annealing process should affect a crystal structure and a electrical transport, enhance a antiferromagnetic transition temperature, and reduce a extrinsic ferromagnetic component of a system. an in-depth analysis indicates that a crystal quality was improved by a inside situ annealing inside terms of reducing both a fe deficiency and a fe impurity inside a samples. a improvement of a sample quality will facilitate a investigations on a intrinsic properties of this material."
"many theories have emerged which investigate how in- variance was generated inside hierarchical networks through sim- ple schemes such as max and mean pooling. a restriction to max/mean pooling inside theoretical and empirical studies has diverted attention away from the more general way of generating invariance to nuisance transformations. we con- jecture that hierarchically building selective invariance (i.e. carefully choosing a range of a transformation to be in- variant to at each layer of the hierarchical network) was im- portant considering pattern recognition. we utilize the novel pooling layer called adaptive pooling to find linear pooling weights within networks. these networks with a learnt pooling weights have performances on object categorization tasks that are comparable to max/mean pooling networks. in- terestingly, adaptive pooling should converge to mean pooling (when initialized with random pooling weights), find more general linear pooling schemes or even decide not to pool at all. we illustrate a general notion of selective invari- ance through object categorization experiments on large- scale datasets such as svhn and ilsvrc 2012."
"we describe a design, implementation and performance of a new hybrid parallelization scheme inside our monte carlo radiative transfer code skirt, which has been used extensively considering modeling a continuum radiation of dusty astrophysical systems including late-type galaxies and dusty tori. a hybrid scheme combines distributed memory parallelization, with the help of a standard message passing interface (mpi) to communicate between processes, and shared memory parallelization, providing multiple execution threads within each process to avoid duplication of data structures. a synchronization between multiple threads was accomplished through atomic operations without high-level locking (also called lock-free programming). this improves a scaling behavior of a code and substantially simplifies a implementation of a hybrid scheme. a result was an extremely flexible solution that adjusts to a number of available nodes, processors and memory, and consequently performs well on the wide variety of computing architectures."
"we address a problem of maintaining high voltage power transmission networks inside security at all time, namely anticipating exceeding of thermal limit considering eventual single line disconnection (whatever its cause may be) by running slow, but accurate, physical grid simulators. new conceptual frameworks are calling considering the probabilistic risk-based security criterion. however, these approaches suffer from high requirements inside terms of tractability. here, we propose the new method to assess a risk. this method uses both machine learning techniques (artificial neural networks) and more standard simulators based on physical laws. more specifically we train neural networks to approximate a overall dangerousness of the grid state. the classical benchmark problem (manpower 118 buses test case) was used to show a strengths of a proposed method."
"we present an idea behind the method towards robust lane tracking considering assisted and autonomous driving, particularly under poor visibility. autonomous detection of lane markers improves road safety, and purely visual tracking was desirable considering widespread vehicle compatibility and reducing sensor intrusion, cost, and energy consumption. however, visual approaches are often ineffective because of the number of factors, including but not limited to occlusion, poor weather conditions, and paint wear-off. our method, named safedrive, attempts to improve visual lane detection approaches inside drastically degraded visual conditions without relying on additional active sensors. inside scenarios where visual lane detection algorithms are unable to detect lane markers, a proposed idea behind the method uses location information of a vehicle to locate and access alternate imagery of a road and attempts detection on this secondary image. subsequently, by with the help of the combination of feature-based and pixel-based alignment, an estimated location of a lane marker was found inside a current scene. we demonstrate a effectiveness of our system on actual driving data from locations inside a united states with google street view as a source of alternate imagery."
"we report the systematic study of a structure, electric and magnetic properties of ca$_3$co$_{2-x}$mn$_x$o$_6$ single crystals with $x =$ 0.72 and 0.26. a dc and ac magnetic susceptibilities display anomalies with characteristic of a spin freezing. a crystals show ferroelectric transition at 40 k and 35 k ($t_{fe}$) considering $x =$ 0.72 and 0.26, respectively, with the large value of 1400 $\mu$c/m$^2$ at 8 k considering electric polarization ($p_c$) along a spin-chain ($c$-axis) direction. interestingly, a electric polarization perpendicular to a chain direction ($p_{ab}$) should also be detected and has value of 450 and 500 $\mu$c/m$^2$ at 8 k considering a $x =$ 0.72 and 0.26 samples, respectively. a specific heat and magnetic susceptibility show no anomaly around $t_{fe}$, which means that a electric polarization of these samples has no direct relationship with a magnetism. a x-ray diffraction and a raman spectroscopy indicate that these samples may undergo jahn-teller distortions that could be a reason of electric polarization."
"learning to detect fraud inside large-scale accounting data was one of a long-standing challenges inside financial statement audits or fraud investigations. nowadays, a majority of applied techniques refer to handcrafted rules derived from known fraud scenarios. while fairly successful, these rules exhibit a drawback that they often fail to generalize beyond known fraud scenarios and fraudsters gradually find ways to circumvent them. to overcome this disadvantage and inspired by a recent success of deep learning we propose a application of deep autoencoder neural networks to detect anomalous journal entries. we demonstrate that a trained network's reconstruction error obtainable considering the journal entry and regularized by a entry's individual attribute probabilities should be interpreted as the highly adaptive anomaly assessment. experiments on two real-world datasets of journal entries, show a effectiveness of a idea behind the method resulting inside high f1-scores of 32.93 (dataset a) and 16.95 (dataset b) and less false positive alerts compared to state of a art baseline methods. initial feedback received by chartered accountants and fraud examiners underpinned a quality of a idea behind the method inside capturing highly relevant accounting anomalies."
"we consider a problem of finding a probability that the random triangle was obtuse, which is first raised by lewis caroll. our investigation leads us to the natural correspondence between plane polygons and a grassmann manifold of 2-planes inside real $n$-space proposed by allen knutson and jean-claude hausmann. this correspondence defines the natural probability measure on plane polygons. inside these terms, we answer caroll's question. we then explore a grassmannian geometry of planar quadrilaterals, providing an answer to sylvester's four-point problem, and describing explicitly a moduli space of unordered quadrilaterals. all of this provides the concrete introduction to the family of metrics used inside shape classification and computer vision."
"strong gravitational lensing provides the powerful test of cold dark matter (cdm) as it enables a detection and mass measurement of low mass haloes even if they do not contain baryons. compact lensed sources such as active galactic nuclei (agn) are particularly sensitive to perturbing subhalos, but their use as the test of cdm has been limited by a small number of systems which have significant radio emission which was extended enough avoid significant lensing by stars inside a plane of a lens galaxy, and red enough to be minimally affected by differential dust extinction. narrow-line emission was the promising alternative as it was also extended and, unlike radio, detectable inside virtually all optically selected agn lenses. we present first results from the wfc3 grism narrow-line survey of lensed quasars, considering a quadruply lensed agn he0435-1223. with the help of the forward modelling pipeline which enables us to robustly account considering spatial blending, we measure a [oiii] 5007 \aa~ flux ratios of a four images. we find that a [oiii] fluxes and positions are well fit by the simple smooth mass model considering a main lens. our data rule out the $m_{600}>10^{8} (10^{7.2}) m_\odot$ nfw perturber projected within $\sim$1\farcs0 (0\farcs1) arcseconds of each of a lensed images, where $m_{600}$ was a perturber mass within its central 600 pc. a non-detection was broadly consistent with a expectations of $\lambda$cdm considering the single system. a sensitivity achieved demonstrates that powerful limits on a nature of dark matter should be obtained with a analysis of $\sim20$ narrow-line lenses."
"modern datasets and models are notoriously difficult to explore and analyze due to their inherent high dimensionality and massive numbers of samples. existing visualization methods which employ dimensionality reduction to two or three dimensions are often inefficient and/or ineffective considering these datasets. this paper introduces t-sne-cuda, the gpu-accelerated implementation of t-distributed symmetric neighbor embedding (t-sne) considering visualizing datasets and models. t-sne-cuda significantly outperforms current implementations with 50-700x speedups on a cifar-10 and mnist datasets. these speedups enable, considering a first time, visualization of a neural network activations on a entire imagenet dataset - the feat that is previously computationally intractable. we also demonstrate visualization performance inside a nlp domain by visualizing a glove embedding vectors. from these visualizations, we should draw interesting conclusions about with the help of a l2 metric inside these embedding spaces. t-sne-cuda was publicly available atthis https url"
"inside a context of stochastic two-phase flow inside porous media, we introduce the novel and efficient method to approximate a probability distribution of a wetting saturation field under uncertain rock properties inside highly heterogeneous porous systems, where streamline patterns are dominated by permeability heterogeneity, and considering slow displacement processes (viscosity ratio close to unity). our method, referred to as a frozen streamline distribution method (frost), was based on the physical understanding of a stochastic problem. indeed, we identify key random fields that guide a wetting saturation variability, namely fluid particle times of flight and injection times. by comparing saturation statistics against full-physics monte carlo simulations, we illustrate how this simple, yet accurate frost method performs under a preliminary approximation of frozen streamlines. further, we inspect a performance of an accelerated frost variant that relies on the simplification about injection time statistics. finally, we introduce how quantiles of saturation should be efficiently computed within a frost framework, thus leading to robust uncertainty assessment."
"approximate message passing (amp) was an algorithmic framework considering solving linear inverse problems from noisy measurements, with exciting applications such as reconstructing images, audio, hyper spectral images, and various other signals, including those acquired inside compressive signal acquisiton systems. a growing prevalence of big data systems has increased interest inside large-scale problems, which may involve huge measurement matrices that are unsuitable considering conventional computing systems. to address a challenge of large-scale processing, multiprocessor (mp) versions of amp have been developed. we provide an overview of two such mp-amp variants. inside row-mp-amp, each computing node stores the subset of a rows of a matrix and processes corresponding measurements. inside column- mp-amp, each node stores the subset of columns, and was solely responsible considering reconstructing the portion of a signal. we will discuss pros and cons of both approaches, summarize recent research results considering each, and explain when each one may be the viable approach. aspects that are highlighted include some recent results on state evolution considering both mp-amp algorithms, and a use of data compression to reduce communication inside a mp network."
"the bayesian idea behind the method termed bayesian least squares optimization with nonnegative l1-norm constraint (balson) was proposed. a error distribution of data fitting was described by gaussian likelihood. a parameter distribution was assumed to be the dirichlet distribution. with a bayes rule, searching considering a optimal parameters was equivalent to finding a mode of a posterior distribution. inside order to explicitly characterize a nonnegative l1-norm constraint of a parameters, we further approximate a true posterior distribution by the dirichlet distribution. we approximate a statistics of a approximating dirichlet posterior distribution by sampling methods. four sampling methods have been introduced. with a estimated posterior distributions, a original parameters should be effectively reconstructed inside polynomial fitting problems, and a balson framework was found to perform better than conventional methods."
"we analyze a electronic properties of a recently discovered stoichiometric superconductor cakfe$_4$as$_4$ by combining an ab initio idea behind the method and the projection of a band structure to the lowenergy tight-binding hamiltonian, based on a maximally localized wannier orbitals of a 3d fe states. we identify a key symmetries as well as differences and similarities inside a electronic structure between cakfe$_4$as$_4$ and a parent systems cafe$_2$as$_2$ and kfe$_2$as$_2$. inside particular, we find cakfe4as4 to have the significantly more quasi-two-dimensional electronic structure than a latter systems. finally, we study a superconducting instabilities inside cakfe$_4$as$_4$ by employing a leading angular harmonics approximation (laha) and find two potential a$_{1g}$-symmetry representation of a superconducting gap to be a dominant instabilities inside this system."
"we present the general framework considering studying regularized estimators; i.e., approximation problems wherein ""plug-in"" type estimators are either ill-defined or ill-behaved. we derive primitive conditions that imply consistency and asymptotic linear representation considering regularized estimators, allowing considering slower than $\sqrt{n}$ estimators as well as infinite dimensional parameters. we also provide data-driven methods considering choosing tuning parameters that, under some conditions, achieve a aforementioned results. we illustrate a scope of our idea behind the method by studying the wide range of applications, revisiting known results and deriving new ones."
"attenuation correction was an essential requirement of positron emission tomography (pet) image reconstruction to allow considering accurate quantification. however, attenuation correction was particularly challenging considering pet-mri as neither pet nor magnetic resonance imaging (mri) should directly image tissue attenuation properties. mri-based computed tomography (ct) synthesis has been proposed as an alternative to physics based and segmentation-based approaches that assign the population-based tissue density value inside order to generate an attenuation map. we propose the novel deep fully convolutional neural network that generates synthetic cts inside the recursive manner by gradually reducing a residuals of a previous network, increasing a overall accuracy and generalisability, while keeping a number of trainable parameters within reasonable limits. a model was trained on the database of 20 pre-acquired mri/ct pairs and the four-fold random bootstrapped validation with the 80:20 split was performed. quantitative results show that a proposed framework outperforms the state-of-the-art atlas-based idea behind the method decreasing a mean absolute error (mae) from 131hu to 68hu considering a synthetic cts and reducing a pet reconstruction error from 14.3% to 7.2%."
"automatic image description systems are commonly trained and evaluated on large image description datasets. recently, researchers have started to collect such datasets considering languages other than english. an unexplored question was how different these datasets are from english and, if there are any differences, what causes them to differ. this paper provides the cross-linguistic comparison of dutch, english, and german image descriptions. we find that these descriptions are similar inside many respects, but a familiarity of crowd workers with a subjects of a images has the noticeable influence on description specificity."
"superconducting (sc) gap symmetry and magnetic response of cubic u0.97th0.03be13 are studied by means of high-precision heat-capacity and dc magnetization measurements with the help of the single crystal, inside order to address a long-standing question of its second phase transition at tc2 inside a sc state below tc1. a absence (presence) of an anomaly at tc2 inside a field-cooling (zero-field-cooling) magnetization indicates that this transition was between two different sc states. there was the qualitative difference inside a field variation of a transition temperatures; tc2(h) was isotropic whereas tc1(h) exhibits the weak anisotropy between [001] and [111] directions. inside a low temperature phase below tc2(h), a angle-resolved heat-capacity $c(t,h, \phi)$ reveals that a gap was fully opened over a fermi surface, narrowing down a possible gap symmetry."
"we study a phase transition between the trivial and the time-reversal-invariant topological superconductor inside the single-band system. by analyzing a interplay of symmetry, topology and energetics, we show that considering the generic normal state band structure, a phase transition occurs using extended intermediate phases inside which even- and odd-parity pairing components coexist. considering inversion-symmetric systems, a coexistence phase spontaneously breaks time-reversal symmetry. considering noncentrosymmetric superconductors, a low-temperature intermediate phase was time-reversal breaking, while a high-temperature phase preserves time-reversal symmetry and has topologically protected line nodes. furthermore, with approximate rotational invariance, a system has an emergent $u(1) \times u(1)$ symmetry, and novel topological defects, such as half vortex lines binding majorana fermions, should exist. we analytically solve considering a dispersion of a majorana fermion and show that it exhibit small and large velocities at low and high energies. relevance of our theory to superconducting pyrochlore oxide cd$_2$re$_2$o$_7$ and half-heusler materials was discussed."
"a repulsive fermi hubbard model on a square lattice has the rich phase diagram near half-filling (corresponding to a particle density per lattice site $n=1$): considering $n=1$ a ground state was an antiferromagnetic insulator, at $0.6 < n \lesssim 0.8$, it was the $d_{x^2-y^2}$-wave superfluid (at least considering moderately strong interactions $u \lesssim 4t$ inside terms of a hopping $t$), and a region $1-n \ll 1$ was most likely subject to phase separation. much of this physics was preempted at finite temperatures and to an extent driven by strong magnetic fluctuations, their quantitative characteristics and how they change with a doping level being much less understood. experiments on ultra-cold atoms have recently gained access to this interesting fluctuation regime, which was now under extensive investigation. inside this work we employ the self-consistent skeleton diagrammatic idea behind the method to quantify a characteristic temperature scale $t_{m}(n)$ considering a onset of magnetic fluctuations with the large correlation length and identify their nature. our results suggest that a strongest fluctuations---and thus highest $t_{m}$ and easiest experimental access to this regime---are observed at $u/t \approx 4-6$."
"inside traditional reinforcement learning, an agent maximizes a reward collected during its interaction with a environment by approximating a optimal policy through a approximation of value functions. typically, given the state s and action a, a corresponding value was a expected discounted sum of rewards. a optimal action was then chosen to be a action the with a largest value estimated by value function. however, recent developments have shown both theoretical and experimental evidence of superior performance when value function was replaced with value distribution inside context of deep q learning [1]. inside this paper, we develop the new algorithm that combines advantage actor-critic with value distribution estimated by quantile regression. we evaluated this new algorithm, termed distributional advantage actor-critic (da2c or qr-a2c) on the variety of tasks, and observed it to achieve at least as good as baseline algorithms, and outperforming baseline inside some tasks with smaller variance and increased stability."
"reasoning was the crucial part of natural language argumentation. to comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises. as arguments are highly contextualized, warrants are usually presupposed and left implicit. thus, a comprehension does not only require language understanding and logic skills, but also depends on common sense. inside this paper we develop the methodology considering reconstructing warrants systematically. we operationalize it inside the scalable crowdsourcing process, resulting inside the freely licensed dataset with warrants considering 2k authentic arguments from news comments. on this basis, we present the new challenging task, a argument reasoning comprehension task. given an argument with the claim and the premise, a goal was to choose a correct implicit warrant from two options. both warrants are plausible and lexically close, but lead to contradicting claims. the solution to this task will define the substantial step towards automatic warrant reconstruction. however, experiments with several neural attention and language models reveal that current approaches do not suffice."
"we study a crossover between a sudden quench limit and a adiabatic dynamics of superconducting states inside a attractive hubbard model. we focus on a dynamics induced by a change of a attractive interaction during the finite ramp time which was varied inside order to track a evolution of a dynamical phase diagram from a sudden quench to a equilibrium limit. two different dynamical regimes are realized considering quenches towards weak and strong coupling interactions. at weak coupling a dynamics depends only on a energy injected into a system, whereas the dynamics retaining memory of a initial state takes place at strong coupling. we show that this was related to the sharp transition between the weak and the strong coupling quench dynamical regime, which defines a boundaries beyond which the dynamics independent from a initial state was recovered. comparing a dynamics inside a superconducting and non-superconducting phases we argue that this was due to a lack of an adiabatic connection to a equilibrium ground state considering non-equilibrium superconducting states inside a strong coupling quench regime."
"superconducting spintronics inside hybrid superconductor/ferromagnet (s-f) heterostructures provides an exciting potential new class of device. a prototypical super-spintronic device was a superconducting spin-valve, where a critical temperature, $t_c$, of a s-layer should be controlled by a relative orientation of two (or more) f-layers. here, we show that such control was also possible inside the simple s/f bilayer. with the help of field history to set a remanent magnetic state of the thin er layer, we demonstrate considering the nb/er bilayer the high level of control of both $t_c$ and a shape of a resistive transition, r(t), to zero resistance. we are able to model a origin of a remanent magnetization, treating it as an increase inside a effective exchange field of a ferromagnet and link this, with the help of conventional s-f theory, to a suppression of $t_c$. we observe stepped features inside a r(t) which we argue was due to the fundamental interaction of superconductivity with inhomogeneous ferromagnetism, the phenomena currently lacking theoretical description."
"within the ginzburg-landau formalism we establish analytically a necessary and sufficient conditions to realize the doubly degenerate superconducting ground state with broken time-reversal symmetry (btrs) inside the multi-band superconductor. with the help of these results we analyze a ground state of the three-band superconductor inside a cylindrical geometry inside an external magnetic field. we show that depending on a interband coupling constants, the magnetic flux should induce current density jumps inside such superconducting geometries that are related to adiabatic or non-adiabatic transitions from btrs to time-reversal symmetric states and vice versa. this unusual current induced magnetic flux response should inside principle be used experimentally to detect superconducting btrs ground states as well as corresponding metastable excited states."
"the good measure of similarity between data points was crucial to many tasks inside machine learning. similarity and metric learning methods learn such measures automatically from data, but they do not scale well respect to a dimensionality of a data. inside this paper, we propose the method that should learn efficiently similarity measure from high-dimensional sparse data. a core idea was to parameterize a similarity measure as the convex combination of rank-one matrices with specific sparsity structures. a parameters are then optimized with an approximate frank-wolfe procedure to maximally satisfy relative similarity constraints on a training data. our algorithm greedily incorporates one pair of features at the time into a similarity measure, providing an efficient way to control a number of active features and thus reduce overfitting. it enjoys very appealing convergence guarantees and its time and memory complexity depends on a sparsity of a data instead of a dimension of a feature space. our experiments on real-world high-dimensional datasets demonstrate its potential considering classification, dimensionality reduction and data exploration."
"deep neural networks have been shown to be beneficial considering the variety of tasks, inside particular allowing considering end-to-end learning and reducing a requirement considering manual design decisions. however, still many parameters have to be chosen inside advance, also raising a need to optimize them. one important, but often ignored system parameter was a selection of the proper activation function. thus, inside this paper we target to demonstrate a importance of activation functions inside general and show that considering different tasks different activation functions might be meaningful. to avoid a manual design or selection of activation functions, we build on a idea of genetic algorithms to learn a best activation function considering the given task. inside addition, we introduce two new activation functions, elish and hardelish, which should easily be incorporated inside our framework. inside this way, we demonstrate considering three different image classification benchmarks that different activation functions are learned, also showing improved results compared to typically used baselines."
"bayesian additive regression trees (bart) was the regression technique developed by chipman et al. (2008). its usefulness inside standard regression settings has been clearly demonstrated, but it has not been applied to time series analysis as yet. we discuss a difficulties inside applying this technique to time series analysis and demonstrate its superior predictive capabilities inside a case of the well know time series: a southern oscillation index."
"a measurement of phrasal semantic relatedness was an important metric considering many natural language processing applications. inside this paper, we present three approaches considering measuring phrasal semantics, one based on the semantic network model, another on the distributional similarity model, and the hybrid between a two. our hybrid idea behind the method achieved an f-measure of 77.4% on a task of evaluating a semantic similarity of words and compositional phrases."
"artificial neural networks (anns) have received increasing attention inside recent years with applications that span the wide range of disciplines including vital domains such as medicine, network security and autonomous transportation. however, neural network architectures are becoming increasingly complex and with an increasing need to obtain real-time results from such models, it has become pivotal to use parallelization as the mechanism considering speeding up network training and deployment. inside this work we propose an implementation of network parallel training through cannon's algorithm considering matrix multiplication. we show that increasing a number of processes speeds up training until a point where process communication costs become prohibitive; this point varies by network complexity. we also show through empirical efficiency calculations that a speedup obtained was superlinear."
"we present the new clustering algorithm called k-means-u* which inside many cases was able to significantly improve a clusterings found by k-means++, a current de-facto standard considering clustering inside euclidean spaces. first we introduce a k-means-u algorithm which starts from the result of k-means++ and attempts to improve it with the sequence of non-local ""jumps"" alternated by runs of standard k-means. each jump transfers a ""least useful"" center towards a center with a largest local error, offset by the small random vector. this was continued as long as a error decreases and often leads to an improved solution. occasionally k-means-u terminates despite obvious remaining optimization possibilities. by allowing the limited number of retries considering a last jump it was frequently possible to reach better local minima. a resulting algorithm was called k-means-u* and dominates k-means++ wrt. solution quality which was demonstrated empirically with the help of various data sets. by construction a logarithmic quality bound established considering k-means++ holds considering k-means-u* as well."
"glaucoma was a second leading cause of blindness all over a world, with approximately 60 million cases reported worldwide inside 2010. if undiagnosed inside time, glaucoma causes irreversible damage to a optic nerve leading to blindness. a optic nerve head examination, which involves measurement of cup-to-disc ratio, was considered one of a most valuable methods of structural diagnosis of a disease. approximation of cup-to-disc ratio requires segmentation of optic disc and optic cup on eye fundus images and should be performed by modern computer vision algorithms. this work presents universal idea behind the method considering automatic optic disc and cup segmentation, which was based on deep learning, namely, modification of u-net convolutional neural network. our experiments include comparison with a best known methods on publicly available databases drions-db, rim-one v.3, drishti-gs. considering both optic disc and cup segmentation, our method achieves quality comparable to current state-of-the-art methods, outperforming them inside terms of a prediction time."
"we study pure coordination games where inside every outcome, all players have identical payoffs, 'win' or 'lose'. we identify and discuss the range of 'purely rational principles' guiding a reasoning of rational players inside such games and analyze which classes of coordination games should be solved by such players with no preplay communication or conventions. we observe that it was highly nontrivial to delineate the boundary between purely rational principles and other decision methods, such as conventions, considering solving such coordination games."
"this work introduces, considering a first time, non-orthogonal multiple access (noma) into short-packet communications to achieve low latency inside wireless networks. specifically, we address a optimization of transmission rates and power allocation to maximize a effective throughput of a user with the higher channel gain while guaranteeing a other user achieving the certain level of effective throughput. to demonstrate a benefits of noma, we analyze a performance of orthogonal multiple access (oma) as the benchmark. our examination shows that noma should significantly outperform oma by achieving the higher effective throughput with a same latency or incurring the lower latency to achieve a same effective throughput targets. surprisingly, we find that a performance gap between noma and oma becomes more prominent when a effective throughput targets at a two users become closer to each other. this demonstrates that noma should significantly reduce a latency inside a context of short-packet communications with practical constraints."
"a magnetism inside mn$_3$si$_2$te$_6$ has been investigated with the help of thermodynamic measurements, first principles calculations, neutron diffraction and diffuse neutron scattering on single crystals. these data confirm that mn$_3$si$_2$te$_6$ was the ferrimagnet below the curie temperature of $t_c$ approximately 78k. a magnetism was anisotropic, with magnetization and neutron diffraction demonstrating that a moments lie within a basal plane of a trigonal structure. a saturation magnetization of approximately 1.6$\mu_b$/mn at 5k originates from a different multiplicities of a two antiferromagnetically-aligned mn sites. first principles calculations reveal antiferromagnetic exchange considering a three nearest mn-mn pairs, which leads to the competition between a ferrimagnetic ground state and three other magnetic configurations. a ferrimagnetic state results from a energy associated with a third-nearest neighbor interaction, and thus long-range interactions are essential considering a observed behavior. diffuse magnetic scattering was observed around a 002 bragg reflection at 120k, which indicates a presence of strong spin correlations well above $t_c$. these are promoted by a competing ground states that result inside the relative suppression of $t_c$, and may be associated with the small ferromagnetic component that produces anisotropic magnetism below $\approx$330k."
"we study an exploration method considering model-free rl that generalizes a counter-based exploration bonus methods and takes into account long term exploratory value of actions rather than the single step look-ahead. we propose the model-free rl method that modifies delayed q-learning and utilizes a long-term exploration bonus with provable efficiency. we show that our proposed method finds the near-optimal policy inside polynomial time (pac-mdp), and also provide experimental evidence that our proposed algorithm was an efficient exploration method."
"understanding player behavior was fundamental inside game data science. video games evolve as players interact with a game, so being able to foresee player experience would aid to ensure the successful game development. inside particular, game developers need to evaluate beforehand a impact of in-game events. simulation optimization of these events was crucial to increase player engagement and maximize monetization. we present an experimental analysis of several methods to forecast game-related variables, with two main aims: to obtain accurate predictions of in-app purchases and playtime inside an operational production environment, and to perform simulations of in-game events inside order to maximize sales and playtime. our ultimate purpose was to take the step towards a data-driven development of games. a results suggest that, even though a performance of traditional approaches such as arima was still better, a outcomes of state-of-the-art techniques like deep learning are promising. deep learning comes up as the well-suited general model that could be used to forecast the variety of time series with different dynamic behaviors."
"we present an overview of a first data release (dr1) and first-look science from a green bank ammonia survey (gas). gas was the large program at a green bank telescope to map all gould belt star-forming regions with $a_v \gtrsim 7$ mag visible from a northern hemisphere inside emission from nh$_3$ and other key molecular tracers. this first release includes a data considering four regions inside gould belt clouds: b18 inside taurus, ngc 1333 inside perseus, l1688 inside ophiuchus, and orion the north inside orion. we compare a nh$_3$ emission to dust continuum emission from herschel, and find that a two tracers correspond closely. nh$_3$ was present inside over 60\% of lines-of-sight with $a_v \gtrsim 7$ mag inside three of a four dr1 regions, inside agreement with expectations from previous observations. a sole exception was b18, where nh$_3$ was detected toward ~ 40\% of lines-of-sight with $a_v \gtrsim 7$ mag. moreover, we find that a nh$_3$ emission was generally extended beyond a typical 0.1 pc length scales of dense cores. we produce maps of a gas kinematics, temperature, and nh$_3$ column densities through forward modeling of a hyperfine structure of a nh$_3$ (1,1) and (2,2) lines. we show that a nh$_3$ velocity dispersion, ${\sigma}_v$, and gas kinetic temperature, $t_k$, vary systematically between a regions included inside this release, with an increase inside both a mean value and spread of ${\sigma}_v$ and $t_k$ with increasing star formation activity. a data presented inside this paper are publicly available."
"emojis, as the new way of conveying nonverbal cues, are widely adopted inside computer-mediated communications. inside this paper, first from the message sender perspective, we focus on people's motives inside with the help of four types of emojis -- positive, neutral, negative, and non-facial. we compare a willingness levels of with the help of these emoji types considering seven typical intentions that people usually apply nonverbal cues considering inside communication. a results of extensive statistical hypothesis tests not only report a popularities of a intentions, but also uncover a subtle differences between emoji types inside terms of intended uses. second, from the perspective of message recipients, we further study a sentiment effects of emojis, as well as their duplications, on verbal messages. different from previous studies inside emoji sentiment, we study a sentiments of emojis and their contexts as the whole. a experiment results indicate that a powers of conveying sentiment are different between four emoji types, and a sentiment effects of emojis vary inside a contexts of different valences."
"let $l$ be a unique even self-dual lattice of signature $(25,1)$. a automorphism group $\operatorname{aut}(l)$ acts on a hyperbolic space $\mathcal{h}^{25}$. we study the poincaré series $e(z,s)$ defined considering $z$ inside $\mathcal{h}^{25}$, convergent considering $\operatorname{re}(s) > 25$, invariant under $\operatorname{aut}(l)$ and having singularities along a mirrors of a reflection group of $l$. we compute a fourier expansion of $e(z,s)$ at the ""leech cusp"" and prove that it should be meromorphically continued to $\operatorname{re}(s) > 25/2$. analytic continuation of kloosterman sum zeta functions imply that a individual fourier coefficients of $e(z,s)$ have meromorphic continuation to a whole $s$-plane."
"a sampling based motion planning algorithm known as rapidly-exploring random trees (rrt) has gained a attention of many researchers due to their computational efficiency and effectiveness. recently, the variant of rrt called rrt* has been proposed that ensures asymptotic optimality. subsequently its bidirectional version has also been introduced inside a literature known as bidirectional-rrt* (b-rrt*). we introduce the new variant called intelligent bidirectional-rrt* (ib-rrt*) which was an improved variant of a optimal rrt* and bidirectional version of rrt* (b-rrt*) algorithms and was specially designed considering complex cluttered environments. ib-rrt* utilizes a bidirectional trees idea behind the method and introduces intelligent sample insertion heuristic considering fast convergence to a optimal path solution with the help of uniform sampling heuristics. a proposed algorithm was evaluated theoretically and experimental results are presented that compares ib-rrt* with rrt* and b-rrt*. moreover, experimental results demonstrate a superior efficiency of ib-rrt* inside comparison with rrt* and b-rrt inside complex cluttered environments."
"the study of a group properties of galaxies inside our immediate neighborhood provides the singular opportunity to observationally constrain a halo mass function, the fundamental characterization of galaxy formation. detailed studies of individual groups have provided a coefficients of scaling relations between the proxy considering a virial radius, velocity dispersion, and mass that usefully allows groups to be defined over a range $10^{10} - 10^{15}$ $m_\odot$. at the second hierarchical level, associations are defined as regions around collapsed halos extending to a zero velocity surface at a decoupling from cosmic expansion. a most remarkable result of a study emerges from a construction of a halo mass function from a sample. at $\sim10^{12}$ $m_\odot$ there was the jog from a expectation sheth-tormen function, such that halo counts drop by the factor $\sim 3$ inside all lower mass bins."
"stochastic approximation (sa) was the classical idea behind the method considering stochastic convex optimization. previous studies have demonstrated that a convergence rate of sa should be improved by introducing either smoothness or strong convexity condition. inside this paper, we make use of smoothness and strong convexity simultaneously to boost a convergence rate. let $\lambda$ be a modulus of strong convexity, $\kappa$ be a condition number, $f_*$ be a minimal risk, and $\alpha>1$ be some small constant. first, we demonstrate that, inside expectation, an $o(1/[\lambda t^\alpha] + \kappa f_*/t)$ risk bound was attainable when $t = \omega(\kappa^\alpha)$. thus, when $f_*$ was small, a convergence rate could be faster than $o(1/[\lambda t])$ and approaches $o(1/[\lambda t^\alpha])$ inside a ideal case. second, to further benefit from small risk, we show that, inside expectation, an $o(1/2^{t/\kappa}+f_*)$ risk bound was achievable. thus, a excess risk reduces exponentially until reaching $o(f_*)$, and if $f_*=0$, we obtain the global linear convergence. finally, we emphasize that our proof was constructive and each risk bound was equipped with an efficient stochastic algorithm attaining that bound."
"this paper seeks to combine differential game theory with a actor-critic-identifier architecture to determine forward-in-time, approximate optimal controllers considering formation tracking inside multi-agent systems, where a agents have uncertain heterogeneous nonlinear dynamics. the continuous control strategy was proposed, with the help of communication feedback from extended neighbors on the communication topology that has the spanning tree. the model-based reinforcement learning technique was developed to cooperatively control the group of agents to track the trajectory inside the desired formation. simulation results are presented to demonstrate a performance of a developed technique."
"this expository article was an introduction to a adjoint orbits of complex semisimple groups, primarily inside a algebro-geometric and lie-theoretic contexts, and with the pronounced emphasis on a properties of semisimple and nilpotent orbits. it was intended to build the foundation considering more specialized settings inside which adjoint orbits feature prominently (ex. hyperkähler geometry, landau-ginzburg models, and a theory of symplectic singularities). also included are the few arguments and observations that, to a author's knowledge, have not yet appeared inside a research literature."
"honeycomb iridates such as $\gamma$-li$_2$iro$_3$ are argued to realize kitaev spin-anisotropic magnetic exchange, along with heisenberg and possibly other couplings. while systems with pure kitaev interactions are candidates to realize the quantum spin liquid ground state, inside $\gamma$-li$_2$iro$_3$ it has been shown that a balance of magnetic interactions leads to a incommensurate spiral spin order at ambient pressure below 38 k. we study a fragility of this state inside single crystals of $\gamma$-li$_2$iro$_3$ with the help of resonant x-ray scattering (rxs) under applied hydrostatic pressures of up to 3.0 gpa. rxs was the direct probe of a underlying electronic order, and we observe a abrupt disappearance of a $q$=(0.57, 0, 0) spiral order at the critical pressure $p_c = 1.5\ $gpa with no accompanying change inside a symmetry of a lattice. this dramatic disappearance was inside stark contrast with recent studies of $\beta$-li$_2$iro$_3$ that show continuous suppression of a spiral order inside magnetic field; under pressure, the new and possibly nonmagnetic ground state emerges."
"estimating a uncertainty of predictions was the crucial ability considering robots inside unstructured environments. most mobile robots considering indoor use rely on 2d laser scanners considering localization, mapping and navigation. these sensors, however, cannot detect transparent surfaces or measure a full occupancy of complex objects such as tables. deep neural networks have recently been proposed to overcome this limitation by learning to approximate object occupancy. these estimates are nevertheless subject to noise, making a evaluation of their confidence an important issue. inside this work we study uncertainty approximation inside deep models, proposing the novel solution based on the fully convolutional autoencoder. a proposed architecture was not restricted by a assumption that a uncertainty follows the gaussian model, as inside a case of many popular solutions considering deep model uncertainty estimation, e.g., mc dropout. we present results showing that uncertainty over obstacle distances was actually better modeled with the laplace distribution. as an example of application where uncertainty evaluation was crucial, we also present an algorithm to build the map that includes information over obstacle distance estimates while taking into account a level of uncertainty inside each estimate. we finally show how a constructed map should be used to increase global navigation safety by planning trajectories which avoid areas of high uncertainty, enabling higher autonomy considering mobile robots inside indoor settings."
"pbc j2333.9-2343 was the giant radio galaxy which shows different characteristics at different wavebands that are difficult to explain within a actual generic schemes of unification of active galactic nuclei (agn), thus being the good candidate to host different phases of nuclear activity. we aim at disentangling a nature of this agn by with the help of simultaneous multiwavelength data. we obtained data inside 2015 from a very long baseline array (vlba), a san pedro mártir telescope, and a xmm-newton}observatories. this allows a study of a nuclear parts of a galaxy through its morphology and spectra, as well as a analysis of a spectral energy distribution (sed). we also reanalysed optical data from a san pedro mártir telescope from 2009 previously presented inside parisi et al. (2012) considering the homogeneous comparison. at x-ray frequencies, a source was unabsorbed. a optical spectra are of the type 1.9 agn, both inside 2009 and 2015, although showing the broader component inside 2015. a vlba radio images show an inverted spectrum with self-absorbed, optically thick compact core and steep spectrum, optically thin jet. a sed resembles that of typical blazars and was best represented by an external compton (ec) model with the viewing angle of $\sim$ 3-6 degrees. a apparent size of a large scale structure of pbc j2333.9-2343 must correspond to an intrinsic deprojected value of $\sim 7$ mpc considering $\theta_v<10^\circ$, and to $> 13$ mpc considering $\theta_v<5^\circ$, the value much larger than a bigger giant radio galaxy known (4.5 mpc). a above arguments suggest that pbc j2333.9-2343 has undergone through the new episode of nuclear activity and that a direction of a new jet has changed, inside a plane of a sky and was now pointing towards us, making this source from being the radio galaxy to become the blazar, the very exceptional case of restarting activity."
"we provide lower bounds on a number of periodic finsler billiard trajectories in the quadratically convex smooth closed hypersurface m inside the d-dimensional finsler space with possibly irreversible finsler metric. an example of such the system was the billiard inside the sufficiently weak magnetic field. a r-periodic finsler billiard trajectories correspond to r-gons inscribed inside m and having extremal finsler length. a cyclic group z/rz acts on these extremal polygons, and one counts a z/rz-orbits. with the help of morse and lusternik-schnirelmann theories, we prove that if r was an odd prime, then a number of r-periodic finsler billiard trajectories was not less than (r-1)(d-2)+1. we also give stronger lower bounds when m was inside general position. a problem of estimating a number of periodic billiard trajectories from below goes back to birkhoff. our work extends to a finsler setting a results previously obtained considering euclidean billiards by babenko, farber, tabachnikov, and karasev."
"a principle of a common cause claims that if an improbable coincidence has occurred, there must exist the common cause. this was generally taken to mean that positive correlations between non-causally related events should disappear when conditioning on a action of some underlying common cause. a extended interpretation of a principle, by contrast, urges that common causes should be called considering inside order to explain positive deviations between a estimated correlation of two events and a expected value of their correlation. a aim of this paper was to provide a extended reading of a principle with the general probabilistic model, capturing a simultaneous action of the system of multiple common causes. to this end, two distinct models are elaborated, and a necessary and sufficient conditions considering their existence are determined."
"this paper presents an assume-guarantee reasoning idea behind the method to a computation of robust invariant sets considering network systems. parameterized signal temporal logic (pstl) was used to formally describe a behaviors of a subsystems, which we use as a template considering a contract. we show that set invariance should be proved with the valid assume-guarantee contract by reasoning about individual subsystems. if the valid assume-guarantee contract with monotonic pstl template was known, it should be further refined by value iteration. when such the contract was not known, an epigraph method was proposed to solve considering the contract that was valid, ---an idea behind the method that has linear complexity considering the sparse network. the microgrid example was used to demonstrate a proposed method. a simulation result shows that together with control barrier functions, a states of all a subsystems should be bounded in a individual robust invariant sets."
"deep learning on graph structures has shown exciting results inside various applications. however, few attentions have been paid to a robustness of such models, inside contrast to numerous research work considering image or text adversarial attack and defense. inside this paper, we focus on a adversarial attacks that fool a model by modifying a combinatorial structure of data. we first propose the reinforcement learning based attack method that learns a generalizable attack policy, while only requiring prediction labels from a target classifier. also, variants of genetic algorithms and gradient methods are presented inside a scenario where prediction confidence or gradients are available. we use both synthetic and real-world data to show that, the family of graph neural network models are vulnerable to these attacks, inside both graph-level and node-level classification tasks. we also show such attacks should be used to diagnose a learned classifiers."
"topological data analysis (tda) was the recent and fast growing eld providing the set of new topological and geometric tools to infer relevant features considering possibly complex data. this paper was the brief introduction, through the few selected topics, to basic fundamental and practical aspects of tda considering non experts. 1 introduction and motivation topological data analysis (tda) was the recent eld that emerged from various works inside applied (algebraic) topology and computational geometry during a rst decade of a century. although one should trace back geometric approaches considering data analysis quite far inside a past, tda really started as the eld with a pioneering works of edelsbrunner et al. (2002) and zomorodian and carlsson (2005) inside persistent homology and is popularized inside the landmark paper inside 2009 carlsson (2009). tda was mainly motivated by a idea that topology and geometry provide the powerful idea behind the method to infer robust qualitative, and sometimes quantitative, information about a structure of data-see, e.g. chazal (2017). tda aims at providing well-founded mathematical, statistical and algorithmic methods to infer, analyze and exploit a complex topological and geometric structures underlying data that are often represented as point clouds inside euclidean or more general metric spaces. during a last few years, the considerable eort has been made to provide robust and ecient data structures and algorithms considering tda that are now implemented and available and easy to use through standard libraries such as a gudhi library (c++ and python) maria et al. (2014) and its r software interface fasy et al. (2014a). although it was still rapidly evolving, tda now provides the set of mature and ecient tools that should be used inside combination or complementary to other data sciences tools. a tdapipeline. tda has recently known developments inside various directions and application elds. there now exist the large variety of methods inspired by topological and geometric approaches. providing the complete overview of all these existing approaches was beyond a scope of this introductory survey. however, most of them rely on a following basic and standard pipeline that will serve as a backbone of this paper: 1. a input was assumed to be the nite set of points coming with the notion of distance-or similarity between them. this distance should be induced by a metric inside a ambient space (e.g. a euclidean metric when a data are embedded inside r d) or come as an intrinsic metric dened by the pairwise distance matrix. a denition of a metric on a data was usually given as an input or guided by a application. it was however important to notice that a choice of a metric may be critical to reveal interesting topological and geometric features of a data."
"we propose the numerical methodology considering a numerical simulation of distinct, interacting physical processes described by the combination of compressible, inert and reactive forms of a euler equations, multiphase equations and elastoplastic equations. these systems of equations are usually solved by coupling finite element and cfd models. here we solve them simultaneously, by recasting all a equations inside a same, hyperbolic form and solving them on a same grid with a same finite-volume numerical schemes. a proposed compressible, multiphase, hydrodynamic formulation should employ the hierarchy of five reactive and non-reactive flow models, which allows simple to more involved applications to be directly described by a appropriate selection. a communication between a hydrodynamic and elastoplastic systems was facilitated by means of mixed-material riemann solvers at a boundaries of a systems, which represent physical material boundaries. to this end we derive approximate mixed riemann solvers considering each pair of a above models based on characteristic equations. a components considering reactive flow and elastoplastic solid modelling are validated separately before presenting validation considering a full, coupled systems. multi-dimensional use cases demonstrate a suitability of a reactive flow-solid interaction methodology inside a context of impact-driven initiation of reactive flow and structural response due to violent reaction inside automotive (e.g. car crash) or defence (e.g. explosive reactive armour) applications. several types of explosives (c4, deetasheet, nitromethane, gaseous fuel) inside gaseous, liquid and solid state are considered."
"how do we determine a mutational effects inside exome sequencing data with little or no statistical evidence? should protein structural information fill inside a gap of not having enough statistical evidence? inside this work, we answer a two questions with a goal towards determining pathogenic effects of rare variants inside rare disease. we take a idea behind the method of determining a importance of point mutation loci focusing on protein structure features. a proposed structure-based features contain information about geometric, physicochemical, and functional information of mutation loci and those of structural neighbors of a loci. a performance of a structure-based features trained on 80\% of humdiv and tested on 20\% of humdiv and on clinvar datasets showed high levels of discernibility inside a mutation's pathogenic or benign effects: f score of 0.71 and 0.68 respectively with the help of multi-layer perceptron. combining structure- and sequence-based feature further improve a accuracy: f score of 0.86 (humdiv) and 0.75 (clinvar). also, careful examination of a rare variants inside rare diseases cases showed that structure-based features are important inside discerning importance of variant loci."
"a performance of the distributed network state approximation problem depends strongly on collaborative signal processing, which often involves excessive communication and computation overheads on the resource-constrained sensor node. inside this work, we idea behind the method a distributed approximation problem from a viewpoint of sensor networks to design the more efficient algorithm with reduced overheads, while still achieving a required performance bounds on a results. we propose an event-trigger diffusion kalman filter, specifying when to communicate relative measurements between nodes based on the local signal indicative of a network error performance. this holistic idea behind the method leads to an energy-aware state approximation algorithm, which we then apply to a distributed simultaneous localization and time synchronization problem. we analytically prove that this algorithm leads to bounded error performance. our algorithm was then evaluated on the physical testbed of the mobile quadrotor node moving through the network of stationary custom ultra-wideband wireless devices. we observe a trade-off between communication cost and error performance. considering instance, we are able to save 86% of a communication overhead, while introducing 16% degradation inside a performance."
"let l be an abelian number field of degree n with galois group g. inside this paper we study how to compute efficiently the normal integral basis considering l, if there was at least one, assuming that a group g and an integral basis considering l are known."
"classification of social media data was an important idea behind the method inside understanding user behavior on a web. although information on social media should be of different modalities such as texts, images, audio or videos, traditional approaches inside classification usually leverage only one prominent modality. techniques that are able to leverage multiple modalities are often complex and susceptible to a absence of some modalities. inside this paper, we present simple models that combine information from different modalities to classify social media content and are able to handle a above problems with existing techniques. our models combine information from different modalities with the help of the pooling layer and an auxiliary learning task was used to learn the common feature space. we demonstrate a performance of our models and their robustness to a missing of some modalities inside a emotion classification domain. our approaches, although being simple, should not only achieve significantly higher accuracies than traditional fusion approaches but also have comparable results when only one modality was available."
"inside this paper, a problem of road friction prediction from the fleet of connected vehicles was investigated. the framework was proposed to predict a road friction level with the help of both historical friction data from a connected cars and data from weather stations, and comparative results from different methods are presented. a problem was formulated as the classification task where a available data was used to train three machine learning models including logistic regression, support vector machine, and neural networks to predict a friction class (slippery or non-slippery) inside a future considering specific road segments. inside addition to a friction values, which are measured by moving vehicles, additional parameters such as humidity, temperature, and rainfall are used to obtain the set of descriptive feature vectors as input to a classification methods. a proposed prediction models are evaluated considering different prediction horizons (0 to 120 minutes inside a future) where a evaluation shows that a neural networks method leads to more stable results inside different conditions."
"we describe the space-borne, multi-band, multi-beam polarimeter aiming at the precise and accurate measurement of a polarization of a cosmic microwave background. a instrument was optimized to be compatible with a strict budget requirements of the medium-size space mission within a cosmic vision programme of a european space agency. a instrument has no moving parts, and uses arrays of diffraction-limited kinetic inductance detectors to cover a frequency range from 60 ghz to 600 ghz inside 19 wide bands, inside a focal plane of the 1.2 m aperture telescope cooled at 40 k, allowing considering an accurate extraction of a cmb signal from polarized foreground emission. a projected cmb polarization survey sensitivity of this instrument, after foregrounds removal, was 1.7 {\mu}k$\cdot$arcmin. a design was robust enough to allow, if needed, the downscoped version of a instrument covering a 100 ghz to 600 ghz range with the 0.8 m aperture telescope cooled at 85 k, with the projected cmb polarization survey sensitivity of 3.2 {\mu}k$\cdot$arcmin."
"variational auto-encoder (vae) was the powerful unsupervised learning framework considering image generation. one drawback of vae was that it generates blurry images due to its gaussianity assumption and thus l2 loss. to allow a generation of high quality images by vae, we increase a capacity of decoder network by employing residual blocks and skip connections, which also enable efficient optimization. to overcome a limitation of l2 loss, we propose to generate images inside the multi-stage manner from coarse to fine. inside a simplest case, a proposed multi-stage vae divides a decoder into two components inside which a second component generates refined images based on a course images generated by a first component. since a second component was independent of a vae model, it should employ other loss functions beyond a l2 loss and different model architectures. a proposed framework should be easily generalized to contain more than two components. experiment results on a mnist and celeba datasets demonstrate that a proposed multi-stage vae should generate sharper images as compared to those from a original vae."
"this article discusses a description of wall-bounded turbulence as the deterministic high-dimensional dynamical system of interacting coherent structures, defined as eddies with enough internal dynamics to behave relatively autonomously from any remaining incoherent part of a flow. a guiding principle was that randomness was not the property, but the methodological choice of what to ignore inside a flow, and that the complete understanding of turbulence, including a possibility of control, requires that it be kept to the minimum. after briefly reviewing a underlying low-order statistics of flows at moderate reynolds numbers, a article examines what two-point statistics imply considering a decomposition of a flow into individual eddies. intense eddies are examined next, including their temporal evolution, and shown to satisfy many of a properties required considering coherence. inside particular, it was shown that coherent structures larger than a corrsin scale are the natural consequence of a shear. inside wall-bounded turbulence, they should be classified into coherent dispersive waves and transient bursts. a former are found inside a viscous layer near a wall and as very-large structures spanning a boundary layer thickness. although they are shear-driven, these waves have enough internal structure to maintain the uniform advection velocity. conversely, bursts exist at all scales, are characteristic of a logarithmic layer, and interact almost linearly with a shear. while a waves require the wall to determine their length scale, a bursts are essentially independent from it. a article concludes with the brief review of our present theoretical understanding of turbulent structures, and with the list of open problems and future perspectives."
"this paper fills the gap inside aspect-based sentiment analysis and aims to present the new method considering preparing and analysing texts concerning opinion and generating user-friendly descriptive reports inside natural language. we present the comprehensive set of techniques derived from rhetorical structure theory and sentiment analysis to extract aspects from textual opinions and then build an abstractive summary of the set of opinions. moreover, we propose aspect-aspect graphs to evaluate a importance of aspects and to filter out unimportant ones from a summary. additionally, a paper presents the prototype solution of data flow with interesting and valuable results. a proposed method's results proved a high accuracy of aspect detection when applied to a gold standard dataset."
"we consider a following semilinear wave equation with time-dependent damping. \begin{align} \tag{nldw} \left\{ \begin{array}{ll} \partial_t^2 u - \delta u + b(t)\partial_t u = |u|^{p}, & (t,x) \in [0,t) \times \mathbb{r}^n, \\ u(0,x)=\varepsilon u_0(x), u_t(0,x)=\varepsilon u_1(x), & x \in \mathbb{r}^n, \end{array} \right. \end{align} where $n \in \mathbb{n}$, $p>1$, $\varepsilon>0$, and $b(t)\thickapprox (t+1)^{-\beta}$ with $\beta \in [-1,1)$. it was known that small data blow-up occurs when $1<p< p_f$ and, on a other hand, small data global existence holds when $p>p_f$, where $p_f:=1+2/n$ was a fujita exponent. a sharp approximate of a lifespan is well studied when $1<p< p_f$. inside a critical case $p=p_f$, a lower approximate of a lifespan is also investigated. recently, lai and zhou obtained a sharp upper approximate of a lifespan when $p=p_f$ and $b(t)=1$. inside a present paper, we give a sharp upper approximate of a lifespan when $p=p_f$ and $b(t)\thickapprox (t+1)^{-\beta}$ with $\beta \in [-1,1)$ by a lai--zhou method."
"we consider spacetimes solving a einstein non-linear scalar field equations with t2-symmetry and show that they admit an areal time foliation inside a expanding direction. inside particular, we prove global existence and uniqueness of solutions to a corresponding system of evolution equations considering all future times. a only assumption we have to make was that a potential was the non-negative smooth function. inside a special case of the constant potential, the setting which was equivalent to the linear scalar field on the background with the positive cosmological constant, we achieve detailed asymptotic estimates considering a different components of a spacetime metric. this result holds considering all t3-gowdy symmetric metrics and extends to certain t2-symmetric ones satisfying an the priori decay property. building upon these asymptotic estimates, we show future causal geodesic completeness and prove a cosmic no-hair conjecture."
"obstacle avoidance was the key feature considering safe unmanned aerial vehicle (uav) navigation. while solutions have been proposed considering static obstacle avoidance, systems enabling avoidance of dynamic objects, such as drones, are hard to implement due to a detection range and field-of-view (fov) requirements, as well as a constraints considering integrating such systems on-board small uavs. inside this work, the dataset of 6k synthetic depth maps of drones has been generated and used to train the state-of-the-art deep learning-based drone detection model. while many sensing technologies should only provide relative altitude and azimuth of an obstacle, our depth map-based idea behind the method enables full 3d localization of a obstacle. this was extremely useful considering collision avoidance, as 3d localization of detected drones was key to perform efficient collision-free path planning. a proposed detection technique has been validated inside several real depth map sequences, with multiple types of drones flying at up to 2 m/s, achieving an average precision of 98.7%, an average recall of 74.7% and the record detection range of 9.5 meters."
"nbte2 crystal was quasi-2d layered semimetal with charge density wave ground state showing the distorted-1t structure at room temperature. here we report a anisotropic magneto-transport properties of nbte2. an anomalous linear magnetoresistance up to 30% at 3 k inside 9 t is observed, which should be well explained by quantum linear magnetoresistance model. our results reveal that the large quasi-2d fermi surface and small fermi pockets with linearly dispersive bands coexist inside nbte2. a comparison with a isostructural material tate2 provides more information about a electronic structure evolution with charge density wave transitions inside nbte2 and tate2."
"state approximation estimates a system condition inside real-time and provides the base case considering other energy management system (ems) applications including real-time contingency analysis and security-constrained economic dispatch. recent work inside a literature shows malicious cyber-attack should inject false measurements that bypass traditional bad data detection and cause actual overloads. thus, it was very important to detect such cyber-attacks. inside this paper, multiple metrics are proposed to monitor abnormal load deviations and suspicious branch flow changes. the systematic two-stage idea behind the method was proposed to detect false data injection (fdi) cyber-attack. a first stage determines whether a system was under attack while a second stage identifies a target branch. numerical simulations verify that fdi should cause severe system violations and demonstrate a effectiveness of a proposed two-stage fdi detection (fdid) method. it was concluded that a proposed fdid idea behind the method should efficiently detect fdi cyber-attacks and identify a target branch; furthermore, a associated false alarm rate and false dismissal rate are very low."
"we construct multisoliton solutions to a defocusing energy critical wave equation with potentials inside $\mathbb{r}^{3}$ based on regular and reversed strichartz estimates developed inside \cite{gc3} considering wave equations with charge transfer hamiltonians. we also show a asymptotic stability of multisoliton solutions. a multisoliton structures with both stable and unstable solitons are covered. since each soliton decays slowly with rate $\frac{1}{\left\langle x\right\rangle }$, a interactions among a solitons are strong. some reversed strichartz estimates and local decay estimates considering a charge transfer model are established to handle strong interactions."
the survey of real differential geometry and loop theory was given inside order to introduce a construction of an analytic loop associated to p-adic differential manifold.
"inside this paper we discuss a existence, uniqueness and regularity of solutions of a following system of coupled semilinear poisson equations on the smooth bounded domain $\omega$ inside $\mathbb{r}^n$: \[ \left\{{llll} \mathcal{a}^s u= v^p & {\rm in} \ \ \omega \mathcal{a}^s v = f(u) & {\rm in} \ \ \omega u= v=0 & {\rm on} \ \ \partial\omega \right. \] where $s\in (0, 1)$ and $\mathcal{a}^s$ denote spectral fractional laplace operators. we assume that $1< p<\frac{2s}{n-2s}$, and a function $f$ was superlinear and with no growth restriction (for example $f(r)=re^r$); thus a system has the nontrivial solution. another important example was given by $f(r)=r^q$. inside this case, we prove that such the system admits at least one positive solution considering the certain set of a couple $(p,q)$ below a critical hyperbola \[ \frac{1}{p + 1} + \frac{1}{q + 1} = \frac{n - 2s}{n} \] whenever $n > 2s$. considering such weak solutions, we prove an $l^\infty$ approximate of brezis-kato type and derive a regularity property of a weak solutions."
"electronic structure has been studied inside lightly electron doped correlated spin-orbit insulator sr$_2$iro$_4$ by angle-resolved photoelectron spectroscopy. we have observed coexistence of a lower hubbard band and a in-gap band, a momentum dependence of a latter traces that of a band calculations without on-site coulomb repulsion. a in-gap state remained anisotropically gapped inside all observed momentum area, forming the remnant fermi surface state, evolving towards a fermi energy by carrier doping. these experimental results show the striking similarity with those observed inside deeply underdoped cuprates, suggesting a common nature of a nodal liquid states observed inside both compounds."
"we report a spin to charge current conversation inside an intrinsic topological insulator (ti) $(bi_{0.22}sb_{0.78})_2te_3$ film at room temperature. a spin currents are generated inside the thin layer of permalloy (py) by two different processes, spin pumping (spe) and spin seebeck effects (sse). inside a first we use microwave-driven ferromagnetic resonance of a py film to generate the spe spin current that was injected into a ti $(bi_{0.22}sb_{0.78})_2te_3$ layer inside direct contact with py. inside a second we use a sse inside a longitudinal configuration inside py without contamination by a nernst effect made possible with the thin nio layer between a py and $(bi_{0.22}sb_{0.78})_2te_3$ layers. a spin-to-charge current conversion was attributed to a inverse edelstein effect (iee) made possible by a spin-momentum locking inside a electron fermi contours due to a rashba field. a measurements by a two techniques yield very similar values considering a iee parameter, which are larger than a reported values inside a previous studies on topological insulators."
"we use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are the governing factor inside determining how different patients will react to different interventions. we compare a performance of autoencoders that take fixed length sequences of concatenated timesteps as input with the recurrent sequence-to-sequence autoencoder. we evaluate our methods on around 35,500 patients from a latest mimic iii dataset from beth israel deaconess hospital."
"this article presents a prediction difference analysis method considering visualizing a response of the deep neural network to the specific input. when classifying images, a method highlights areas inside the given input image that provide evidence considering or against the certain class. it overcomes several shortcoming of previous methods and provides great additional insight into a decision making process of classifiers. making neural network decisions interpretable through visualization was important both to improve models and to accelerate a adoption of black-box classifiers inside application areas such as medicine. we illustrate a method inside experiments on natural images (imagenet data), as well as medical images (mri brain scans)."
"state-space models are commonly used to describe different forms of ecological data. we consider a case of count data with observation errors. considering such data a system process was typically multi-dimensional consisting of coupled markov processes, where each component corresponds to the different characterisation of a population, such as age group, gender or breeding status. a associated system process equations describe a biological mechanisms under which a system evolves over time. however, there was often limited information inside a count data alone to sensibly approximate demographic parameters of interest, so these are often combined with additional ecological observations leading to an integrated data analysis. unfortunately, fitting these models to a data should be challenging, especially if a state-space model considering a count data was non-linear or non-gaussian. we propose an efficient particle markov chain monte carlo algorithm to approximate a demographic parameters without a need considering resorting to linear or gaussian approximations. inside particular, we exploit a integrated model structure to enhance a efficiency of a algorithm. we then incorporate a algorithm into the sequential monte carlo sampler inside order to perform model comparison with regards to a dependence structure of a demographic parameters. finally, we demonstrate a applicability and computational efficiency of our algorithms on two real datasets."
"by with the help of cosmological hydrodynamical simulations we study a effect of supernova (sn) and active galactic nuclei (agn) feedback on a mass transport of gas on to galactic nuclei and a black hole (bh) growth down to redshift z~6. we study a bh growth inside relation with a mass transport processes associated with gravity and pressure torques, and how they are modified by feedback. cosmological gas funelled through cold flows reaches a galactic outer region close to free-fall. then torques associated to pressure triggered by gas turbulent motions produced inside a circum-galactic medium by shocks and explosions from sne are a main source of mass transport beyond a central ~ 100 pc. due to high concentrations of mass inside a central galactic region, gravitational torques tend to be more important at high redshift. a combined effect of almost free-falling material and both gravity and pressure torques produces the mass accretion rate of order ~ 1 m_sun/yr at ~ pc scales. inside a absence of sn feedback, agn feedback alone does not affect significantly either star formation or bh growth until a bh reaches the sufficiently high mass of $\sim 10^6$ m_sun to self-regulate. sn feedback alone, instead, decreases both stellar and bh growth. finally, sn and agn feedback inside tandem efficiently quench a bh growth, while star formation remains at a levels set by sn feedback alone due to a small final bh mass, ~ few 10^5 m_sun. sne create the more rarefied and hot environment where energy injection from a central agn should accelerate a gas further."
"a xmm cluster archive super survey (x-class) was the serendipitously-detected x-ray-selected sample of 845 galaxy clusters based on 2774 xmm archival observations and covering approximately 90 deg$^2$ spread across a high-galactic latitude ($|b|>20$ deg) sky. a primary goal of this survey was to produce the well-selected sample of galaxy clusters on which cosmological analyses should be performed. this article presents a photometric redshift followup of the high signal-to-noise subset of 266 of these clusters with declination $\delta<+20$ deg with grond, the seven channel ($grizjhk$) simultaneous imager on a mpg 2.2m telescope at a eso la silla observatory. we use the newly developed technique based on a red sequence colour-redshift relation, enhanced with information coming from a x-ray detection to provide photometric redshifts considering this sample. we determine photometric redshifts considering 236 clusters, finding the median redshift of $z=0.39$ with an accuracy of $\delta z = 0.02 (1+z)$ when compared to the sample of 76 spectroscopically confirmed clusters. we also compute x-ray luminosities considering a entire sample and find the median bolometric luminosity of $7.2\times10^{43} \mathrm{erg\ s^{-1}}$ and the median temperature 2.9 kev. we compare our results to a xmm-xcs and xmm-xxl surveys, finding good agreement inside both samples. a x-class catalogue was available online at this http url."
"we investigate a extensions of a hecke algebras of finite (complex) reflection groups by lattices of reflection subgroups that we introduced, considering some of them, inside our previous work on a yokonuma-hecke algebras and their connections with artin groups. when a hecke algebra was attached to a symmetric group, and a lattice contains all reflection subgroups, then these algebras are a diagram algebras of braids and ties of aicardi and juyumaya. we prove the stucture theorem considering these algebras, generalizing the result of espinoza and ryom-hansen from a case of a symmetric group to a general case. we prove that these algebras are symmetric algebras at least when $w$ was the coxeter group, and inside general under a trace conjecture of broué, malle and michel."
"this paper proposes the distributed strategy regulated on the subset of individual buses inside the power network described by a swing equations to achieve transient frequency control while preserving asymptotic stability. transient frequency control refers to a ability to maintain a transient frequency of each bus of interest inside the given safe region, provided it was initially inside it, and ii) if it was initially not, then drive a frequency to converge to this region within the finite time, with the guaranteed convergence rate. building on lyapunov stability and set invariance theory, we formulate a stability and a transient frequency requirements as two separate constraints considering a control input. our design synthesizes the controller that satisfies both constraints simultaneously. a controller was distributed and lipschitz, guaranteeing a existence and uniqueness of a trajectories of a closed-loop system. we further bound its magnitude and demonstrate its robustness against measurement inaccuracies. simulations on a ieee 39-bus power network illustrate our results."
"inside this paper we propose the scalable version of the state-of-the-art deterministic time-invariant feature extraction idea behind the method based on consecutive changes of basis and nonlinearities, namely, a scattering network. a first focus of a paper was to extend a scattering network to allow a use of higher order nonlinearities as well as extracting nonlinear and fourier based statistics leading to a required invariants of any inherently structured input. inside order to reach fast convolutions and to leverage a intrinsic structure of wavelets, we derive our complete model inside a fourier domain. inside addition of providing fast computations, we are now able to exploit sparse matrices due to extremely high sparsity well localized inside a fourier domain. as the result, we are able to reach the true linear time complexity with inputs inside a fourier domain allowing fast and energy efficient solutions to machine learning tasks. validation of a features and computational results will be presented through a use of these invariant coefficients to perform classification on audio recordings of bird songs captured inside multiple different soundscapes. inside a end, a applicability of a presented solutions to deep artificial neural networks was discussed."
"inside this paper, we study a robust consensus problem considering the set of discrete-time linear agents to coordinate over an uncertain communication network, which was to achieve consensus against a transmission errors and noises resulted from a information exchange between a agents. we model a network by means of communication links subject to multiplicative stochastic uncertainties, which are susceptible to describing packet dropout, random delay, and fading phenomena. different communication topologies, such as undirected graphs and leader-follower graphs, are considered. we derive sufficient conditions considering robust consensus inside a mean square sense. this results unveil intrinsic constraints on consensus attainment imposed by a network synchronizability, a unstable agent dynamics, and a channel uncertainty variances. consensus protocols are designed based on a state information transmitted over a uncertain channels, by solving the modified algebraic riccati equation."
"we detect 20 $z=7.0$ ly$\alpha$ emitter (lae) candidates to $l({\rm ly}\alpha) \geq 2 \times 10^{42}$ erg s$^{-1}$ or $0.3$ $l^*_{z=7}$ and inside $6.1\times 10^5$ mpc$^3$ volume inside a subaru deep field and a subaru/xmm-newton deep survey field by 82 and 37 hours of subaru suprime-cam narrowband nb973 and reddest optical $y$-band imaging. we compare their ly$\alpha$ and uv luminosity functions (lfs) and densities and ly$\alpha$ equivalent widths (ews) to those of $z=5.7$, 6.6 and 7.3 laes from previous suprime-cam surveys. a ly$\alpha$ lf (density) rapidly declines by the factor of $\times$1.5 (1.9) inside $l({\rm ly}\alpha)$ at $z=5.7-6.6$ (160 myr), $\times$1.5 (1.6) at $z=6.6-7.0$ (60 myr) at a faint end and $\times$2.0 (3.8) at $z=7.0-7.3$ (40 myr). also, inside addition to a systematic decrease inside ew at $z=5.7-6.6$ previously found, 2/3 of a $z=7.0$ laes detected inside a uv continuum exhibit lower ews than a $z=6.6$ ones. moreover, while a uv lf and density do not evolve at $z=5.7-6.6$, they modestly decline at $z=6.6-7.0$, implying galaxy evolution contributing to a decline of a ly$\alpha$ lf. comparison of a $z=7.0$ ly$\alpha$ lf to a one predicted by an lae evolution model further reveals that galaxy evolution alone cannot explain all a decline of ly$\alpha$ lf. if we attribute a discrepancy to ly$\alpha$ attenuation by neutral hydrogen, a intergalactic medium transmission of ly$\alpha$ photons at $z=7.0$ would be $t_{{\rm ly}\alpha}^{\rm igm} \leq 0.6-0.7$. it was lower (higher) than $t_{{\rm ly}\alpha}^{\rm igm}$ at $z=6.6$ (7.3) derived by previous studies, suggesting rapid increase inside neutral fraction at $z > 6$."
"superconductivity often emerges inside proximity of other symmetry-breaking ground states, such as antiferromagnetism or charge-density-wave (cdw) order. however, a subtle inter-relation of these phases remains poorly understood, and inside some cases even a existence of short-range correlations considering superconducting compositions was uncertain. inside such circumstances, ultrafast experiments should provide new insights, by tracking a relaxation kinetics following excitation at frequencies related to a broken symmetry state. here, we investigate a transient terahertz conductivity of bapb1-xbixo3 - the material considering which superconductivity was adjacent to the competing cdw phase - after optical excitation tuned to a cdw absorption band. inside insulating babio3 we observed an increase inside conductivity and the subsequent relaxation, which are consistent with quasiparticles injection across the rigid semiconducting gap. inside a doped compound bapb0.72bi0.28o3 (superconducting below tc=7k), the similar response is also found immediately above tc. this observation evidences a presence of the robust gap up to t=40 k, which was presumably associated with short-range cdw correlations. the qualitatively different behaviour is observed inside a same material fo t>40 k. here, a photo-conductivity is dominated by an enhancement inside carrier mobility at constant density, suggestive of melting of a cdw correlations rather than excitation across an optical gap. a relaxation displayed the temperature dependent, arrhenius-like kinetics, suggestive of a crossing of the free-energy barrier between two phases. these results support a existence of short-range cdw correlations above tc inside underdoped bapb1-xbixo3, and provide new information on a dynamical interplay between superconductivity and charge order."
"word embeddings improve a performance of nlp systems by revealing a hidden structural relationships between words. despite their success inside many applications, word embeddings have seen very little use inside computational social science nlp tasks, presumably due to their reliance on big data, and to the lack of interpretability. i propose the probabilistic model-based word embedding method which should recover interpretable embeddings, without big data. a key insight was to leverage mixed membership modeling, inside which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. i show how to train a model with the help of the combination of state-of-the-art training techniques considering word embeddings and topic models. a experimental results show an improvement inside predictive language modeling of up to 63% inside mrr over a skip-gram, and demonstrate that a representations are beneficial considering supervised learning. i illustrate a interpretability of a models with computational social science case studies on state of a union addresses and nips articles."
"inside this paper, we define a curvature dimension inequalities cd(m, k) on finite directed graphs modifying a case of undirected graphs. as the main result, we evaluate m and k on finite directed graphs."
"we give the direct proof of a fact that a $l^{p)$-norms of global solutions of a boussinesq system inside $r^{3}$ grow large as $ t \rightarrow + \infty $ considering $ 1 < p < 3 $ and decay to zero considering $ 3 < p \leq \infty $, providing exact estimates from below and above with the help of the suitable decomposition of a space-time space $ r^{+} \times r^{3} $. inside particular, a kinetic energy blows up as $ \| u(t) \|_{2}^{2} \sim c t^{1/2} $ considering large time. this contrasts with a case of a navier-stokes equations."
"poisoning attack was identified as the severe security threat to machine learning algorithms. inside many applications, considering example, deep neural network (dnn) models collect public data as a inputs to perform re-training, where a input data should be poisoned. although poisoning attack against support vector machines (svm) has been extensively studied before, there was still very limited knowledge about how such attack should be implemented on neural networks (nn), especially dnns. inside this work, we first examine a possibility of applying traditional gradient-based method (named as a direct gradient method) to generate poisoned data against nns by leveraging a gradient of a target model w.r.t. a normal data. we then propose the generative method to accelerate a generation rate of a poisoned data: an auto-encoder (generator) used to generate poisoned data was updated by the reward function of a loss, and a target nn model (discriminator) receives a poisoned data to calculate a loss w.r.t. a normal data. our experiment results show that a generative method should speed up a poisoned data generation rate by up to 239.38x compared with a direct gradient method, with slightly lower model accuracy degradation. the countermeasure was also designed to detect such poisoning attack methods by checking a loss of a target model."
"considering a iv-vi semiconductor family we derive an exact relation between a microscopic gap edge wave functions of a bulk insulator and a dirac-weyl topological surface state wave function, thus obtaining the fully microscopic surface state. we find that a balance of spin-orbit interaction and crystal field inside a bulk, and a band bending at a surface, should profoundly influence a surface state spin-momentum locking. as the manifestation of this we predict that a spin texture of a $m$-point dirac cones of snte should be tuned through an unexpectedly rich sequence of spin textures -- warped helical with winding number $\pm 1$, $k_x$ linear, hyperbolic, and $k_y$ linear -- e.g. by tuning a band bending at a surface."
"a low energy optical conductivity of conventional superconductors was usually well described by mattis-bardeen (mb) theory which predicts a onset of absorption above an energy corresponding to twice a superconducing (sc) gap parameter delta. recent experiments on strongly disordered superconductors have challenged a application of a mb formulas due to a occurrence of additional spectral weight at low energies below 2delta. here we identify three crucial items which have to be included inside a analysis of optical-conductivity data considering these systems: (a) a correct identification of a optical threshold inside a mattis-bardeen theory, and its relation with a gap value extracted from a measured density of states, (b) a gauge-invariant evaluation of a current-current response function, needed to account considering a optical absorption by sc collective modes, and (c) a inclusion into a mb formula of a energy dependence of a density of states present already above tc. by computing a optical conductvity inside a disordered attractive hubbard model we analyze a relevance of all these items, and we provide the compelling scheme considering a analysis and interpretation of a optical data inside real materials."
"models considering accurately predicting species distributions have become essential tools considering many ecological and conservation problems. considering many species, presence-background (presence-only) data was a most commonly available type of spatial data. the number of important methods have been proposed to model presence-background (pb) data, and there have been debates on a connection between these seemingly disparate methods. a paper begins by studying a close relationship between a li (lancaster & imbens, 1996) and lk (lele & keim, 2006) models, which were among a first developed methods considering analysing pb data. a second part of a paper identifies close connections between a lk and point process models, as well as a equivalence between a scaled binomial (sb), expectation-maximization (em), partial likelihood based lele (2009) and li methods, many of which have not been noted inside a literature. we clarify that all these methods are a same inside their ability to approximate a relative probability (or intensity) of presence from pb data; and a absolute probability of presence, when extra information of a species' prevalence was known. the new unified constrained lk (clk) method was also proposed as the generalisation of a better known existing approaches, with less theory involved and greater ease of implementation."
systems of disordered interacting bosons with particle-hole symmetry should undergo the quantum phase transition between a superfluid phase and a mott glass phase which was the gapless incompressible insulator. we employ large-scale monte carlo simulations of the two-dimensional site-diluted quantum rotor model to investigate a properties of a superfluid density and a compressibility at this transition. we find that both quantities feature power-law critical behavior with exponents governed by generalized josephson relations.
"designing an auction that maximizes expected revenue was an intricate task. indeed, as of today--despite major efforts and impressive progress over a past few years--only a single-item case was fully understood. inside this work, we initiate a exploration of a use of tools from deep learning on this topic. a design objective was revenue optimal, dominant-strategy incentive compatible auctions. we show that multi-layer neural networks should learn almost-optimal auctions considering settings considering which there are analytical solutions, such as myerson's auction considering the single item, manelli and vincent's mechanism considering the single bidder with additive preferences over two items, or yao's auction considering two additive bidders with binary support distributions and multiple items, even if no prior knowledge about a form of optimal auctions was encoded inside a network and a only feedback during training was revenue and regret. we further show how characterization results, even rather implicit ones such as rochet's characterization through induced utilities and their gradients, should be leveraged to obtain more precise fits to a optimal design. we conclude by demonstrating a potential of deep learning considering deriving optimal auctions with high revenue considering poorly understood problems."
"we study a following nonlocal diffusion equation inside a heisenberg group $\mathbb{h}_n$, \[ u_t(z,s,t)=j\ast u(z,s,t)-u(z,s,t), \] where $\ast$ denote convolution product and $j$ satisfies appropriated hypothesis. considering a cauchy problem we obtain that a asymptotic behavior of a solutions was a same form that a one considering a heat equation inside a heisenberg group. to obtain this result we use a spherical transform related to a pair $(u(n),\mathbb{h}_n)$. finally we prove that solutions of properly rescaled nonlocal dirichlet problem converge uniformly to a solution of a corresponding dirichlet problem considering a classical heat equation inside a heisenberg group."
"a precise radial velocity technique was the cornerstone of exoplanetary astronomy. astronomers measure doppler shifts inside a star's spectral features, which track a line-of/sight gravitational accelerations of the star caused by a planets orbiting it. a method has its roots inside binary star astronomy, and exoplanet detection represents a low-companion-mass limit of that application. this limit requires control of several effects of much greater magnitude than a signal sought: a motion of a telescope must be subtracted, a instrument must be calibrated, and spurious doppler shifts ""jitter"" must be mitigated or corrected. two primary forms of instrumental calibration are a stable spectrograph and absorption cell methods, a former being a path taken considering a next generation of spectrographs. spurious, apparent doppler shifts due to non-center-of-mass motion (jitter) should be a result of stellar magnetic activity or photospheric motions and granulation. several avoidance, mitigation, and correction strategies exist, including careful analysis of line shapes and radial velocity wavelength dependence."
"recognition of social signals, from human facial expressions or prosody of speech, was the popular research topic inside human-robot interaction studies. there was also the long line of research inside a spoken dialogue community that investigates user satisfaction inside relation to dialogue characteristics. however, very little research relates the combination of multimodal social signals and language features detected during spoken face-to-face human-robot interaction to a resulting user perception of the robot. inside this paper we show how different emotional facial expressions of human users, inside combination with prosodic characteristics of human speech and features of human-robot dialogue, correlate with users' impressions of a robot after the conversation. we find that happiness inside a user's recognised facial expression strongly correlates with likeability of the robot, while dialogue-related features (such as number of human turns or number of sentences per robot utterance) correlate with perceiving the robot as intelligent. inside addition, we show that facial expression, emotional features, and prosody are better predictors of human ratings related to perceived robot likeability and anthropomorphism, while linguistic and non-linguistic features more often predict perceived robot intelligence and interpretability. as such, these characteristics may inside future be used as an online reward signal considering in-situ reinforcement learning based adaptive human-robot dialogue systems."
"recently, deep learning approaches with various network architectures have achieved significant performance improvement over existing iterative reconstruction methods inside various imaging problems. however, it was still unclear why these deep learning architectures work considering specific inverse problems. to address these issues, here we show that a long-searched-for missing link was a convolution framelets considering representing the signal by convolving local and non-local bases. a convolution framelets is originally developed to generalize a theory of low-rank hankel matrix approaches considering inverse problems, and this paper further extends a idea so that we should obtain the deep neural network with the help of multilayer convolution framelets with perfect reconstruction (pr) under rectilinear linear unit nonlinearity (relu). our analysis also shows that a popular deep network components such as residual block, redundant filter channels, and concatenated relu (crelu) do indeed aid to achieve a pr, while a pooling and unpooling layers should be augmented with high-pass branches to meet a pr condition. moreover, by changing a number of filter channels and bias, we should control a shrinkage behaviors of a neural network. this discovery leads us to propose the novel theory considering deep convolutional framelets neural network. with the help of numerical experiments with various inverse problems, we demonstrated that our deep convolution framelets network shows consistent improvement over existing deep architectures.this discovery suggests that a success of deep learning was not from the magical power of the black-box, but rather comes from a power of the novel signal representation with the help of non-local basis combined with data-driven local basis, which was indeed the natural extension of classical signal processing theory."
"we propose the new idea behind the method considering estimating causal effects when a exposure was measured with error and confounding adjustment was performed using the generalized propensity score (gps). with the help of validation data, we propose the regression calibration (rc)-based adjustment considering the continuous error-prone exposure combined with gps to adjust considering confounding (rc-gps). a outcome analysis was conducted after transforming a corrected continuous exposure into the categorical exposure. we consider confounding adjustment inside a context of gps subclassification, inverse probability treatment weighting (iptw) and matching. inside simulations with varying degrees of exposure error and confounding bias, rc-gps eliminates bias from exposure error and confounding compared to standard approaches that rely on a error-prone exposure. we applied rc-gps to the rich data platform to approximate a causal effect of long-term exposure to fine particles ($pm_{2.5}$) on mortality inside new england considering a period from 2000 to 2012. a main study consists of $2,202$ zip codes covered by $217,660$ 1km $\times$ 1km grid cells with yearly mortality rates, yearly $pm_{2.5}$ averages estimated from the spatio-temporal model (error-prone exposure) and several potential confounders. a internal validation study includes the subset of 83 1km $\times$ 1km grid cells within 75 zip codes from a main study with error-free yearly $pm_{2.5}$ exposures obtained from monitor stations. under assumptions of non-interference and weak unconfoundedness, with the help of matching we found that exposure to moderate levels of $pm_{2.5}$ ($8 <$ $pm_{2.5}$ $\leq 10\ {\rm \mu g/m^3}$) causes the $2.8\%$ ($95\%$ ci: $0.6\%, 3.6\%$) increase inside all-cause mortality compared to low exposure ($pm_{2.5}$ $\leq 8\ {\rm \mu g/m^3}$)."
"codes over galois rings have been studied extensively during a last three decades. negacyclic codes over $gr(2^a,m)$ of length $2^s$ have been characterized: a ring $\mathcal{r}_2(a,m,-1)= \frac{gr(2^a,m)[x]}{\langle x^{2^s}+1\rangle}$ was the chain ring. furthermore, these results have been generalized to $\lambda$-constacyclic codes considering any unit $\lambda$ of a form $4z-1$, $z\in gr(2^a, m)$. inside this paper, we study more general cases and investigate all cases where $\mathcal{r}_p(a,m,\gamma)= \frac{gr(p^a,m)[x]}{\langle x^{p^s}-\gamma \rangle}$ was the chain ring. inside particular, necessary and sufficient conditions considering a ring $\mathcal{r}_p(a,m,\gamma)$ to be the chain ring are obtained. inside addition, by with the help of this structure we investigate all $\gamma$-constacyclic codes over $gr(p^a,m)$ when $\mathcal{r}_p(a,m,\gamma)$ was the chain ring. necessary and sufficient conditions considering a existence of self-orthogonal and self-dual $\gamma$-constacyclic codes are also provided. among others, considering any prime $p$, a structure of $\mathcal{r}_p(a,m,\gamma)=\frac{gr(p^a,m)[x]}{\langle x^{p^s}-\gamma\rangle}$ was used to establish a hamming and homogeneous distances of $\gamma$-constacyclic codes."
"a esa gaia mission provides the unique time-domain survey considering more than one billion sources brighter than g=20.7 mag. gaia offers a unprecedented opportunity to study variability phenomena inside a universe thanks to multi-epoch g-magnitude photometry inside addition to astrometry, blue and red spectro-photometry, and spectroscopy. within a gaia consortium, coordination unit 7 has a responsibility to detect variable objects, classify them, derive characteristic parameters considering specific variability classes, and provide global descriptions of variable phenomena. we describe a variability processing and analysis that we plan to apply to a successive data releases, and we present its application to a g-band photometry results of a first 14 months of gaia operations that comprises 28 days of ecliptic pole scanning law and 13 months of nominal scanning law. out of a 694 million, all-sky, sources that have calibrated g-band photometry inside this first stage of a mission, about 2.3 million sources that have at least 20 observations are located within 38 degrees from a south ecliptic pole. we detect about 14% of them as variable candidates, among which a automated classification identified 9347 cepheid and rr lyrae candidates. additional visual inspections and selection criteria led to a publication of 3194 cepheid and rr lyrae stars, described inside clementini et al. (2016). under a restrictive conditions considering dr1, a completenesses of cepheids and rr lyrae stars are estimated at 67% and 58%, respectively, numbers that will significantly increase with subsequent gaia data releases. data processing within a gaia consortium was iterative, a quality of a data and a results being improved at each iteration. a results presented inside this article show the glimpse of a exceptional harvest that was to be expected from a gaia mission considering variability phenomena. [abridged]"
"this paper introduces the probabilistic framework considering k-shot image classification. a goal was to generalise from an initial large-scale classification task to the separate task comprising new classes and small numbers of examples. a new idea behind the method not only leverages a feature-based representation learned by the neural network from a initial task (representational transfer), but also information about a classes (concept transfer). a concept information was encapsulated inside the probabilistic model considering a final layer weights of a neural network which acts as the prior considering probabilistic k-shot learning. we show that even the simple probabilistic model achieves state-of-the-art on the standard k-shot learning dataset by the large margin. moreover, it was able to accurately model uncertainty, leading to well calibrated classifiers, and was easily extensible and flexible, unlike many recent approaches to k-shot learning."
"online social networks contain the constantly increasing amount of images - most of them focusing on people. due to cultural and climate factors, fashion trends and physical appearance of individuals differ from city to city. inside this paper we investigate to what extent such cues should be exploited inside order to infer a geographic location, i.e. a city, where the picture is taken. we conduct the user study, as well as an evaluation of automatic methods based on convolutional neural networks. experiments on a fashion 144k and the pinterest-based dataset show that a automatic methods succeed at this task to the reasonable extent. as the matter of fact, our empirical results suggest that automatic methods should surpass human performance by the large margin. further inspection of a trained models shows that human-centered characteristics, like clothing style, physical features, and accessories, are informative considering a task at hand. moreover, it reveals that also contextual features, e.g. wall type, natural environment, etc., are taken into account by a automatic methods."
"one-class classification (occ) has been prime concern considering researchers and effectively employed inside various disciplines. but, traditional methods based one-class classifiers are very time consuming due to its iterative process and various parameters tuning. inside this paper, we present six occ methods based on extreme learning machine (elm) and online sequential elm (oselm). our proposed classifiers mainly lie inside two categories: reconstruction based and boundary based, which supports both types of learning viz., online and offline learning. out of various proposed methods, four are offline and remaining two are online methods. out of four offline methods, two methods perform random feature mapping and two methods perform kernel feature mapping. kernel feature mapping based approaches have been tested with rbf kernel and online version of one-class classifiers are tested with both types of nodes viz., additive and rbf. it was well known fact that threshold decision was the crucial factor inside case of occ, so, three different threshold deciding criteria have been employed so far and analyses a effectiveness of one threshold deciding criteria over another. further, these methods are tested on two artificial datasets to check there boundary construction capability and on eight benchmark datasets from different discipline to evaluate a performance of a classifiers. our proposed classifiers exhibit better performance compared to ten traditional one-class classifiers and elm based two one-class classifiers. through proposed one-class classifiers, we intend to expand a functionality of a most used toolbox considering occ i.e. dd toolbox. all of our methods are totally compatible with all a present features of a toolbox."
"inside this paper, we investigate a fundamental limitations of feedback mechanism inside dealing with uncertainties considering network systems. a study of maximum capability of feedback control is pioneered inside xie and guo (2000) considering scalar systems with nonparametric nonlinear uncertainty. inside the network setting, nodes with unknown and nonlinear dynamics are interconnected through the directed interaction graph. nodes should design feedback controls based on all available information, where a objective was to stabilize a network state. with the help of information structure and decision pattern as criteria, we specify three categories of network feedback laws, namely a global-knowledge/global-decision, network-flow/local-decision, and local-flow/local-decision feedback. we establish the series of network capacity characterizations considering these three fundamental types of network control laws. first of all, we prove that considering global-knowledge/global-decision and network-flow/local-decision control where nodes know a information flow across a entire network, there exists the critical number $\big(3/2+\sqrt{2}\big)/\|a_{\mathrm{g}}\|_\infty$, where $3/2+\sqrt{2}$ was as known as a xie-guo constant and $a_{\mathrm{g}}$ was a network adjacency matrix, defining exactly how much uncertainty inside a node dynamics should be overcome by feedback. interestingly enough, a same feedback capacity should be achieved under max-consensus enhanced local flows where nodes only observe information flows from neighbors as well as extreme (max and min) states inside a network. next, considering local-flow/local-decision control, we prove that there exists the structure-determined value being the lower bound of a network feedback capacity. these results reveal a important connection between network structure and fundamental capabilities of in-network feedback control."
"we study the scenario inside which a baryon asymmetry of a universe arises from the cosmological phase transition where lepton-number was spontaneously broken. if a phase transition was first order, the lepton-number asymmetry should arise at a bubble wall, through dynamics similar to electroweak baryogenesis, but involving right-handed neutrinos. inside addition to a usual neutrinoless double beta decay inside nuclear experiments, a model may be probed through the variety of ""baryogenesis by-products,"" which include the stochastic background of gravitational waves created by a colliding bubbles. depending on a model, other aspects may include the network of topological defects that produce their own gravitational waves, additional contribution to dark radiation, and the light pseudo-goldstone boson (majoron) as dark matter candidate."
"abriged: quantifying a number, type and distribution of w-r stars was the key component inside a context of galaxy evolution, since they put constraints on a age of a star formation bursts. nearby galaxies (d<5 mpc) are particularly relevant inside this context since they fill a gap between studies inside a local group, where individual stars should be resolved, and galaxies inside a local volume and beyond. we intend to characterize a w-r star population inside ngc625, the low-metallicity dwarf galaxy suffering the currently declining burst of star formation. optical ifs data have been obtained with a vimos-ifu covering a starburst region. we approximate a number of w-r stars with the help of the linear combination of 3 w-r templates: 1 early-type nitrogen (wn) star, 1 late-type wn star and 1 carbon-type (wc) star (or oxygen-type (wo) star). fits with the help of several ensembles of templates were tested. results were confronted with: i) high spatial resolution hst photometry; ii) numbers of w-r stars inside nearby galaxies; iii) model predictions. a w-r star population was spread over a main body of a galaxy, not necessarily coincident with a overall stellar distribution. our best approximation considering a number of w-r stars yields the total of 28 w-r stars inside a galaxy, out of which 17 are early- type wn, 6 are late-type wn and 5 are wc stars. a width of a stellar features nicely correlates with a dominant w-r type found inside each aperture. a distribution of a different types of wr inside a galaxy was roughly compatible with a way star formation has propagated inside a galaxy, according to previous findings with the help of hst images. fits with the help of templates at a metallicity of a lmc yield more reasonable number of w-r than those with the help of templates at a metallicity of a smc. given a metallicity of ngc 625, this suggests the non-linear relation between a metallicity and a luminosity of a w-r spectral features."
"we study effect of cavity collapse inside non-ideal explosives as the means of controlling their sensitivity. a main aim was to understand a origin of localised temperature peaks (hot spots) that play the leading order role at early ignition stages. thus, we perform 2d and 3d numerical simulations of shock induced single gas-cavity collapse inside nitromethane. ignition was a result of the complex interplay between fluid dynamics and exothermic chemical reaction. inside part i of this work we focused on a hydrodynamic effects inside a collapse process by switching off a reaction terms inside a mathematical model. here, we reinstate a reactive terms and study a collapse of a cavity inside a presence of chemical reactions. we use the multi-phase formulation which overcomes current challenges of cavity collapse modelling inside reactive media to obtain oscillation-free temperature fields across material interfaces to allow a use of the temperature-based reaction rate law. a mathematical and physical models are validated against experimental and analytic data. we identify which of a previously-determined (in part i of this work) high-temperature regions lead to ignition and comment on their reactive strength and reaction growth rate. we quantify a sensitisation of nitromethane by a collapse of a cavity by comparing ignition times of neat and single-cavity material; a ignition occurs inside less than half a ignition time of a neat material. we compare 2d and 3d simulations to examine a change inside topology, temperature and reactive strength of a hot spots by a third dimension. it was apparent that belated ignition times should be avoided by a use of 3d simulations. a effect of a chemical reactions on a topology and strength of a hot spots inside a timescales considered was studied by comparing inert and reactive simulations and examine maximum temperature fields and their growth rates."
"this paper focuses on the passivity-based distributed reference governor (rg) applied to the pre-stabilized mobile robotic network. a novelty of this paper lies inside a method used to solve a rg problem, where the passivity-based distributed optimization scheme was proposed. inside particular, a gradient descent method minimizes a global objective function while a dual ascent method maximizes a hamiltonian. to make a agents converge to a agreed optimal solution, the proportional-integral consensus estimator was used. this paper proves a convergence of a state estimates of a rg to a optimal solution through passivity arguments, considering a physical system static. then, a effectiveness of a scheme considering a dynamics of a physical system was demonstrated through simulations and experiments."
"link prediction inside networks was typically accomplished by estimating or ranking a probabilities of edges considering all pairs of nodes. inside practice, especially considering social networks, a data are often collected by egocentric sampling, which means selecting the subset of nodes and recording all of their edges. this sampling mechanism requires different prediction tools than a typical assumption of links missing at random. we propose the new computationally efficient link prediction algorithm considering egocentrically sampled networks, which estimates a underlying probability matrix by estimating its row space. considering networks created by sampling rows, our method outperforms many popular link prediction and graphon approximation techniques."
"weak attractive interactions inside the spin-imbalanced fermi gas induce the multi-particle instability, binding multiple fermions together. a maximum binding energy per particle was achieved when a ratio of a number of up- and down-spin particles inside a instability was equal to a ratio of a up- and down-spin densities of states inside momentum at a fermi surfaces, to utilize a variational freedom of all available momentum states. we derive this result with the help of an analytical approach, and verify it with the help of exact diagonalization. a multi-particle instability extends a cooper pairing instability of balanced fermi gases to a imbalanced case, and could form a basis of the many-body state, analogously to a construction of a bardeen-cooper-schrieffer theory of superconductivity out of cooper pairs."
"excluding irrelevant features inside the pattern recognition task plays an important role inside maintaining the simpler machine learning model and optimizing a computational efficiency. nowadays with a rise of large scale datasets, feature selection was inside great demand as it becomes the central issue when facing high-dimensional datasets. a present study provides the new measure of saliency considering features by employing the sensitivity analysis (sa) technique called a extended fourier amplitude sensitivity test, and the well-trained feedforward neural network (fnn) model, which ultimately leads to a selection of the promising optimal feature subset. ideas of a paper are mainly demonstrated based on adopting fnn model considering feature selection inside classification problems. but inside a end, the generalization framework was discussed inside order to give insights into a usage inside regression problems as well as expressing how other function approximate models should be deployed. effectiveness of a proposed method was verified by result analysis and data visualization considering the series of experiments over several well-known datasets drawn from uci machine learning repository."
"we study --both inside theory and practice-- a use of momentum motions inside classic iterative hard thresholding (iht) methods. by simply modifying plain iht, we investigate its convergence behavior on convex optimization criteria with non-convex constraints, under standard assumptions. inside diverse scenaria, we observe that acceleration inside iht leads to significant improvements, compared to state of a art projected gradient descent and frank-wolfe variants. as the byproduct of our inspection, we study a impact of selecting a momentum parameter: similar to convex settings, two modes of behavior are observed --""rippling"" and linear-- depending on a level of momentum."
"a sequential analysis of series often requires nonparametric procedures, where a most powerful ones frequently use rank transformations. re-ranking a data sequence after each new observation should become too intensive computationally. this led to a idea of sequential ranks, where only a most recent observation was ranked. however, difficulties finding, or approximating, a null distribution of a statistics may have contributed to a lack of popularity of these methods. inside this paper, we propose transforming a sequential ranks into sequential normal scores which are independent, and asymptotically standard normal random variables. thus original methods based on a normality assumption may be used. the novel idea behind the method permits a inclusion of the priori information inside a form of quantiles. it was developed as the strategy to increase a sensitivity of a scoring statistic. a result was the powerful convenient method to analyze non-normal data sequences. also, four variations of sequential normal scores are presented with the help of examples from a literature. researchers and practitioners might find this idea behind the method useful to develop nonparametric procedures to address new problems extending a use of parametric procedures when distributional assumptions are not met. these methods are especially useful with large data streams where efficient computational methods are required."
"machine learning was often used inside competitive scenarios: participants learn and fit static models, and those models compete inside the shared platform. a common assumption was that inside order to win the competition one has to have a best predictive model, i.e., a model with a smallest out-sample error. was that necessarily true? does a best theoretical predictive model considering the target always yield a best reward inside the competition? if not, should one take a best model and purposefully change it into the theoretically inferior model which inside practice results inside the higher competitive edge? how does that modification look like? and finally, if all participants modify their prediction models towards a best practical performance, who benefits a most? players with inferior models, or those with theoretical superiority? a main theme of this paper was to raise these important questions and propose the theoretical model to answer them. we consider the study case where two linear predictive models compete over the shared target. a model with a closest approximate gets a whole reward, which was equal to a absolute value of a target. we characterize a reward function of each model, and with the help of the basic game theoretic approach, demonstrate that a inferior competitor should significantly improve his performance by choosing optimal model coefficients that are different from a best theoretical prediction. this was the preliminary study that emphasizes a fact that inside many applications where predictive machine learning was at a service of competition, much should be gained from practical (back-testing) optimization of a model compared to static prediction improvement."
we consider the two-phase elliptic-parabolic moving boundary problem modelling an evaporation front inside the porous medium. our main result was the proof of short-time existence and uniqueness of strong solutions to a corresponding nonlinear evolution problem inside an $l_{p}$-setting. it relies critically on nonstandard optimal regularity results considering the linear elliptic-parabolic system with dynamic boundary condition.
inside a present note we consider a problem of constructing honest and adaptive confidence sets considering a matrix completion problem. considering a bernoulli model with known variance of a noise we provide the realizable method considering constructing confidence sets that adapt to a unknown rank of a true matrix.
"the pivotal step toward understanding unconventional superconductors would be to decipher how superconductivity emerges from a unusual normal state upon cooling. inside a cuprates, traces of superconducting pairing appear above a macroscopic transition temperature $t_c$, yet extensive investigation has led to disparate conclusions. a main difficulty has been a separation of superconducting contributions from complex normal state behaviour. here we avoid this problem by measuring a nonlinear conductivity, an observable that was zero inside a normal state. we uncover considering several representative cuprates that a nonlinear conductivity vanishes exponentially above $t_c$, both with temperature and magnetic field, and exhibits temperature-scaling characterized by the nearly universal scale $t_0$. attempts to model a response with a frequently evoked ginzburg-landau theory are unsuccessful. instead, our findings are captured by the simple percolation model that should also explain other properties of a cuprates. we thus resolve the long-standing conundrum by showing that a emergence of superconductivity inside a cuprates was dominated by their inherent inhomogeneity."
"these letters, written inside 1998-2000, contain various basic results about courant algebroids (cas), such as classification of exact and transitive cas, reduction of cas, description inside terms of symplectic dg manifolds, the canonical generating dirac operator, and the relation with poisson-lie t-duality."
"cluster analysis was used to explore structure inside unlabeled data sets inside the wide range of applications. an important part of cluster analysis was validating a quality of computationally obtained clusters. the large number of different internal indices have been developed considering validation inside a offline setting. however, this concept has not been extended to a online setting. the key challenge was to find an efficient incremental formulation of an index that should capture both cohesion and separation of a clusters over potentially infinite data streams. inside this paper, we develop two online versions (with and without forgetting factors) of a xie-beni and davies-bouldin internal validity indices, and analyze their characteristics, with the help of two streaming clustering algorithms (sk-means and online ellipsoidal clustering), and illustrate their use inside monitoring evolving clusters inside streaming data. we also show that incremental cluster validity indices are capable of sending the distress signal to online monitors when evolving clusters go awry. our numerical examples indicate that a incremental xie-beni index with forgetting factor was superior to a other three indices tested."
"this paper studies a sobolev-lorentz capacity and its regularity inside a euclidean setting considering $n \ge 1$ integer. we extend here our previous results on a sobolev-lorentz capacity obtained considering $n \ge 2.$ moreover, considering $n \ge 2$ integer we obtain the few new results concerning a $n,1$ relative and global capacities. we obtain sharp estimates considering a $n,1$ relative capacity of a concentric condensers $(\overline{b}(0,r), b(0,1))$ considering all $r$ inside $[0,1).$ as the consequence we obtain a exact value of a $n,1$ capacity of the point relative to all its bounded open neighborhoods from ${\mathbf{r}}^n$ when $n \ge 2.$ we also show that this aforementioned constant was a value of a $n,1$ global capacity of any point from ${\mathbf{r}}^n,$ where $n \ge 2$ was integer. this allows us to give the new proof of a embedding $h_{0}^{1,(n,1)}(\omega) \hookrightarrow c(\overline{\omega}) \cap l^{\infty}(\omega),$ where $\omega \subset {\mathbf{r}}^n$ was open and $n \ge 2$ was an integer. inside a penultimate section of our paper we prove the new weak convergence result considering bounded sequences inside a non-reflexive spaces $h^{1,(p,1)}(\omega)$ and $h_{0}^{1,(p,1)}(\omega).$ a weak convergence result concerning a spaces $h^{1,(p,1)}(\omega)$ was valid whenever $1<p<\infty,$ while a weak convergence result concerning a spaces $h_{0}^{1,(p,1)}(\omega)$ was valid whenever $1 \le n<p<\infty$ or $1<n=p<\infty.$ as the consequence of a weak convergence result concerning a spaces $h_{0}^{1,(p,1)}(\omega),$ inside a last section of our paper we show that a relative and a global $(p,1)$ and $p,1$ capacities are choquet whenever $1 \le n<p<\infty$ or $1<n=p<\infty.$"
"weyl points with monopole charge $\pm 1$ have been extensively studied, however, real materials of multi-weyl points, whose monopole charges are higher than $1$, have yet to be found. inside this rapid communication, we show that nodal-line semimetals with nontrivial line connectivity provide natural platforms considering realizing floquet multi-weyl points. inside particular, we show that driving crossing nodal lines by circularly polarized light generates double-weyl points. furthermore, we show that monopole combination and annihilation should be observed inside crossing-nodal-line semimetals and nodal-chain semimetals. these proposals should be experimentally verified inside pump-probe angle-resolved photoemission spectroscopy."
"a short ranged magnetic correlations and dynamics of hole doped $pr_{1-x}ca_{x}mno_{3}$ (0.33 < x < 0.5) of different crystallite sizes have been investigated with the help of electron spin resonance spectroscopy (esr). a major contribution to a temperature dependence of paramagnetic line-width was attributed to a spin-lattice relaxation dominated by thermally activated hopping of small polarons with typical activation energy of 20-50 mev. irrespective of a crystallite size and dopant concentration, a transverse spin relaxation time ($t_{2}$) follows the universal scaling behaviour of a type $t_{2}$~($t/t_{0})^n$ inside a paramagnetic regime, where $t_0$ and n are scaling parameters. with the help of a temperature dependence of $t_{2}$, we construct the phase diagram which shows that near half-doping, a magnetic correlations associated with charge ordering not just survives even down to a crystallite size of 22 nm, but was actually enhanced. we conclude that a eventual suppression of charge ordering with reduction inside particle size was possibly more to do with greater influence of chemical disorder than any intrinsic effect."
"we introduce atomicrex, an open-source code considering constructing interatomic potentials as well as more general types of atomic-scale models. such effective models are required to simulate extended materials structures comprising many thousands of atoms or more, because electronic structure methods become computationally too expensive at this scale. atomicrex covers the wide range of interatomic potential types and fulfills many needs inside atomistic model development. as inputs, it supports experimental property values as well as \textit{ab initio} energies and forces, to which models should be fitted with the help of various optimization algorithms. a open architecture of atomicrex allows it to be used inside custom model development scenarios beyond classical interatomic potentials while thanks to its python interface it should be readily integrated e.g., with electronic structure calculations or machine learning algorithms."
"inside this paper, we provide the complete characterization on a robust isolated calmness of a karush-kuhn-tucker (kkt) solution mapping considering convex constrained optimization problems regularized by a nuclear norm function. this study was motivated by a recent work inside [8], where a authors show that under a robinson constraint qualification at the local optimal solution, a kkt solution mapping considering the wide class of conic programming problem was robustly isolated calm if and only if both a second order sufficient condition (sosc) and a strict robinson constraint qualification (srcq) are satisfied. based on a variational properties of a nuclear norm function and its conjugate, we establish a equivalence between a primal/dual sosc and a dual/primal srcq. a derived results lead to several equivalent characterizations of a robust isolated calmness of a kkt solution mapping and add insights to a existing literature on a stability of a nuclear norm regularized convex optimization problems."
"inside this paper, an idea behind the method was proposed to fuse lidar and hyperspectral data, which considers both spectral and spatial information inside the single framework. here, an extended self-dual attribute profile (esdap) was investigated to extract spatial information from the hyperspectral data set. to extract spectral information, the few well-known classifiers have been used such as support vector machines (svms), random forests (rfs), and artificial neural networks (anns). a proposed method accurately classify a relatively volumetric data set inside the few cpu processing time inside the real ill-posed situation where there was no balance between a number of training samples and a number of features. a classification part of a proposed idea behind the method was fully-automatic."
"we study risk-sensitive imitation learning where a agent's goal was to perform at least as well as a expert inside terms of the risk profile. we first formulate our risk-sensitive imitation learning setting. we consider a generative adversarial idea behind the method to imitation learning (gail) and derive an optimization problem considering our formulation, which we call it risk-sensitive gail (rs-gail). we then derive two different versions of our rs-gail optimization problem that aim at matching a risk profiles of a agent and a expert w.r.t. jensen-shannon (js) divergence and wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. we evaluate a performance of our algorithms and compare them with gail and a risk-averse imitation learning (rail) algorithms inside two mujoco and two openai classical control tasks."
"exploration of extreme environments, including caves, canyons and cliffs on low-gravity surfaces such as a moon, mars and asteroids should provide insight into a geological history of a solar system, origins of water, life and prospect considering future habitation and resource exploitation. current methods of exploration utilize large rovers that are unsuitable considering exploring these extreme environments. inside this work, we analyze a feasibility of small, low-cost, reconfigurable multirobot systems to climb steep cliffs and canyon walls. each robot was the 30-cm sphere covered inside microspines considering gripping onto rugged surfaces and attaches to several robots with the help of the spring-tether. even if one robot were to slip and fall, a system would be held up with multiple attachment points much like the professional alpine climber. we analyzed and performed detailed simulations of a design configuration space to identify an optimal system design that trades off climbing performance with risk of falling. our results identify the system of 4 robots was best suited when enabling single-robot climbs, while the system of 6 robots are suited when two robots climb simultaneously. a results show the pathway towards demonstration of a system on real robots."
"understanding spin-wave dynamics inside chiral magnets was the key step considering a development of high-speed, spin-wave based spintronic devices that take advantage of chiral and topological spin textures considering their operation. here we present an experimental and theoretical study of spin-wave dynamics inside the cubic b20 fege single crystal. with the help of a combination of waveguide microwave absorption spectroscopy (mas), micromagnetic simulations, and analytical theory, we identify a resonance dynamics inside all magnetic phases (field polarized, conical, helical, and skyrmion phases). because a resonance frequencies of specific chiral spin textures are unique, quantitative agreement between our theoretical predictions and experimental findings considering all resonance frequencies and spin wave modes enables us to unambiguously identify chiral magnetic phases and to demonstrate that mas was the powerful tool to efficiently extract the magnetic phase diagram. these results provide the new tool to accelerate a integration of chiral magnetic materials into spintronic devices."
"inside this paper, we present the number of robust methodologies considering an underwater robot to visually detect, follow, and interact with the diver considering collaborative task execution. we design and develop two autonomous diver-following algorithms, a first of which utilizes both spatial- and frequency-domain features pertaining to human swimming patterns inside order to visually track the diver. a second algorithm uses the convolutional neural network-based model considering robust tracking-by-detection. inside addition, we propose the hand gesture-based human-robot communication framework that was syntactically simpler and computationally more efficient than a existing grammar-based frameworks. inside a proposed interaction framework, deep visual detectors are used to provide accurate hand gesture recognition; subsequently, the finite-state machine performs robust and efficient gesture-to-instruction mapping. a distinguishing feature of this framework was that it should be easily adopted by divers considering communicating with underwater robots without with the help of artificial markers or requiring memorization of complex language rules. furthermore, we validate a performance and effectiveness of a proposed methodologies through extensive field experiments inside closed- and open-water environments. finally, we perform the user interaction study to demonstrate a usability benefits of our proposed interaction framework compared to existing methods."
"a topology of any complex system was key to understanding its structure and function. fundamentally, algebraic topology guarantees that any system represented by the network should be understood through its closed paths. a length of each path provides the notion of scale, which was vitally important inside characterizing dominant modes of system behavior. here, by combining topology with scale, we prove a existence of universal features which reveal a dominant scales of any network. we use these features to compare several canonical network types inside a context of the social media discussion which evolves through a sharing of rumors, leaks and other news. our analysis enables considering a first time the universal understanding of a balance between loops and tree-like structure across network scales, and an assessment of how this balance interacts with a spreading of information online. crucially, our results allow networks to be quantified and compared inside the purely model-free way that was theoretically sound, fully automated, and inherently scalable."
"a paper proposes a task of universal semantic tagging---tagging word tokens with language-neutral, semantically informative tags. we argue that a task, with its independent nature, contributes to better semantic analysis considering wide-coverage multilingual text. we present a initial version of a semantic tagset and show that (a) a tags provide semantically fine-grained information, and (b) they are suitable considering cross-lingual semantic parsing. an application of a semantic tagging inside a parallel meaning bank supports both of these points as a tags contribute to formal lexical semantics and their cross-lingual projection. as the part of a application, we annotate the small corpus with a semantic tags and present new baseline result considering universal semantic tagging."
"execution monitor of high-level robot actions should be effectively improved by visual monitoring a state of a world inside terms of preconditions and postconditions that hold before and after a execution of an action. furthermore the policy considering searching where to look at, either considering verifying a relations that specify a pre and postconditions or to refocus inside case of the failure, should tremendously improve a robot execution inside an uncharted environment. it was now possible to strongly rely on visual perception inside order to make a assumption that a environment was observable, by a amazing results of deep learning. inside this work we present visual execution monitoring considering the robot executing tasks inside an uncharted lab environment. a execution monitor interacts with a environment using the visual stream that uses two dcnn considering recognizing a objects a robot has to deal with and manipulate, and the non-parametric bayes approximation to discover a relations out of a dcnn features. to recover from lack of focus and failures due to missed objects we resort to visual search policies using deep reinforcement learning."
"there was the paradox inside a model of social dynamics determined by voting inside the stochastic environment (the vise model) called ""pit of losses."" it consists inside a fact that the series of democratic decisions may systematically lead a society to a states unacceptable considering all a voters. a paper examines how this paradox should be neutralized by a presence inside society of the group that votes considering its benefit and should regulate a threshold of its claims. we obtain and analyze analytical results characterizing a welfare of a whole society, a group, and a other participants as functions of a said claims threshold."
"properties of planetary atmospheres, ionospheres, and magnetospheres are difficult to measure from earth. radio occultations are the common method considering measuring these properties, but they traditionally rely on radio transmissions from the spacecraft near a planet. here we explore whether occultations of radio emissions from the distant astrophysical radio source should be used to measure magnetic field strength, plasma density, and neutral density around planets. inside the theoretical case study of jupiter, we find that significant changes inside polarization angle due to faraday rotation occur considering radio signals that pass within 10 jupiter radii of a planet and that significant changes inside frequency and power occur from radio signals that pass through a neutral atmosphere. there are sufficient candidate radio sources, such as pulsars, active galactic nuclei, and masers, that occultations are likely to occur at least once per year. considering pulsars, time delays inside a arrival of their emitted pulses should be used to measure plasma density. exoplanets, whose physical properties are very challenging to observe, may also occult distant astrophysical radio sources, such as their parent stars."
"a purpose of this paper was to study a bimeromorphic invariants of compact complex manifolds inside terms of bott-chern cohomology. we prove the blow-up formula considering bott-chern cohomology. as an application, we show that considering compact complex threefolds a non-kählerness degrees, introduced by angella-tomassini [invent. math. 192, (2013), 71-81], are bimeromorphic invariants. consequently, a $\partial\bar{\partial}$-lemma on threefolds admits a bimeromorphic invariance."
"this paper investigates a classical statistical signal processing problem of detecting the signal inside a presence of colored noise with an unknown covariance matrix. inside particular, we consider the scenario where m-dimensional p possible signal-plus-noise samples and m-dimensional n noise-only samples are available at a detector. then a presence of the signal should be detected with the help of a largest generalized eigenvalue (l.g.e.) of a so called whitened sample covariance matrix. this amounts to statistically characterizing a maximum eigenvalue of a deformed jacobi unitary ensemble (jue). to this end, we employ a powerful orthogonal polynomial idea behind the method to determine the new finite dimensional expression considering a cumulative distribution function (c.d.f.) of a l.g.e. of a deformed jue. this new c.d.f. expression facilitates a further analysis of a receiver operating characteristics (roc) of a detector. it turns out that, considering m=n, when m and p increase such that m/p attains the fixed value, there exists an optimal roc profile corresponding to each fixed signal-to-noise ratio (snr). inside this respect, we have established the tight approximation considering a corresponding optimal roc profile."
"causal mediation analysis should improve understanding of a mechanisms underlying epidemiologic associations. however, a utility of natural direct and indirect effect approximation has been limited by a assumption of no confounder of a mediator-outcome relationship that was affected by prior exposure---an assumption frequently violated inside practice. we build on recent work that identified alternative estimands that do not require this assumption and propose the flexible and double robust semiparametric targeted minimum loss-based estimator considering data-dependent stochastic direct and indirect effects. a proposed method treats a intermediate confounder affected by prior exposure as the time-varying confounder and intervenes stochastically on a mediator with the help of the distribution which conditions on baseline covariates and marginalizes over a intermediate confounder. inside addition, we assume a stochastic intervention was given, conditional on observed data, which results inside the simpler estimator and weaker identification assumptions. we demonstrate a estimator's finite sample and robustness properties inside the simple simulation study. we apply a method to an example from a moving to opportunity experiment. inside this application, randomization to receive the housing voucher was a treatment/instrument that influenced moving to the low-poverty neighborhood, which was a intermediate confounder. we approximate a data-dependent stochastic direct effect of randomization to a voucher group on adolescent marijuana use not mediated by change inside school district and a stochastic indirect effect mediated by change inside school district. we find no evidence of mediation. our estimator was easy to implement inside standard statistical software, and we provide annotated r code to further lower implementation barriers."
"third order nonlinear evolution equations, that was a korteweg-devries (kdv), modified korteweg-devries (mkdv) equation and other ones are considered: they all are connected using baecklund transformations. these links should be depicted inside the wide baecklund chart} which further extends a previous one constructed inside [22]. inside particular, a baecklund transformation which links a mkdv equation to a kdv singularity manifold equation was reconsidered and a nonlinear equation considering a kdv eigenfunction was shown to be linked to all a equations inside a previously constructed baecklund chart. that is, such the baecklund chart was expanded to encompass a nonlinear equation considering a kdv eigenfunctions [30], which finds its origin inside a early days of a study of inverse scattering transform method, when a lax pair considering a kdv equation is constructed. a nonlinear equation considering a kdv eigenfunctions was proved to enjoy the nontrivial invariance property. furthermore, a hereditary recursion operator it admits [30 was recovered using the different method. then, a results are extended to a whole hierarchy of nonlinear evolution equations it generates. notably, a established links allow to show that also a nonlinear equation considering a kdv eigenfunction was connected to a dym equation since both such equations appear inside a same baecklund chart."
"this paper presents the multi-pose face recognition idea behind the method with the help of hybrid face features descriptors (hffd). a hffd was the face descriptor containing of rich discriminant information that was created by fusing some frequency-based features extracted with the help of both wavelet and dct analysis of several different poses of 2d face images. a main aim of this method was to represent a multi-pose face images with the help of the dominant frequency component with still having reasonable achievement compared to a recent multi-pose face recognition methods. a hffd based face recognition tends to achieve better performance than that of a recent 2d-based face recognition method. inside addition, a hffd-based face recognition also was sufficiently to handle large face variability due to face pose variations ."
"many statistical applications require a quantification of joint dependence among more than two random vectors. inside this work, we generalize a notion of distance covariance to quantify joint dependence among d >= 2 random vectors. we introduce a high order distance covariance to measure a so-called lancaster interaction dependence. a joint distance covariance was then defined as the linear combination of pairwise distance covariances and their higher order counterparts which together completely characterize mutual independence. we further introduce some related concepts including a distance cumulant, distance characteristic function, and rank-based distance covariance. empirical estimators are constructed based on certain euclidean distances between sample elements. we study a large sample properties of a estimators and propose the bootstrap procedure to approximate their sampling distributions. a asymptotic validity of a bootstrap procedure was justified under both a null and alternative hypotheses. a new metrics are employed to perform model selection inside causal inference, which was based on a joint independence testing of a residuals from a fitted structural equation models. a effectiveness of a method was illustrated using both simulated and real datasets."
"we present results of the detailed theoretical study of a electronic, magnetic, and structural properties of a chalcogenide parent system fese with the help of the fully charge self-consistent implementation of a density functional theory plus dynamical mean-field theory (dft+dmft) method. inside particular, we predict the remarkable change of a electronic structure of fese which was accompanied by the complete reconstruction of a fermi surface topology (lifshitz transition) upon the moderate expansion of a lattice volume. a phase transition results inside the change of a in-plane magnetic nesting wave vector from $(\pi,\pi)$ to $(\pi,0)$ and was associated with the transition from itinerant to orbital-selective localized magnetic moments. we attribute this behavior to the correlation-induced shift of a van hove singularity of a fe $t_{2}$ bands at a m-point across a fermi level. our results reveal the strong orbital-selective renormalization of a effective mass $m^*/m$ of a fe $3d$ electrons upon expansion. a largest effect occurs inside a fe $xy$ orbital, which gives rise to the non-fermi-liquid-like behavior above a transition. a behavior of a momentum-resolved magnetic susceptibility $\chi({\bf q})$ demonstrates that magnetic correlations are also characterized by the pronounced orbital selectivity, suggesting the spin-fluctuation origin of a nematic phase of paramagnetic fese. we conjecture that a anomalous behavior of fese upon expansion was associated with a proximity of a fe $t_{2}$ van hove singularity to a fermi level and a sensitive dependence of its position on external conditions."
"we study a strichartz estimates considering schrödinger equation on the metric cone $x$, where a metric cone $x=c(y)=(0,\infty)_r\times y$ and a cross section $y$ was the $(n-1)$-dimensional closed riemannian manifold $(y,h)$. a equipped metric on $x$ was given by $g=dr^2+r^2h$, and let $\delta_g$ be a friedrich extension positive laplacian on $x$ and $v=v_0 r^{-2}$ where $v_0\in\cc^\infty(y)$ was the real function such that a operator $\delta_h+v_0+(n-2)^2/4$ was the strictly positive operator on $l^2(y)$. we establish a full range of a global-in-time strichartz approximate without loss considering a schödinger equation associated with a operator $\ll_v=\delta_g+v_0 r^{-2}$ including a endpoint approximate both inside homogeneous and inhomogeneous cases. as an application, we study a well-posed theory and scattering theory considering a schödinger equation with cubic nonlinearity on this setting."
"detecting attacks inside control systems was an important aspect of designing secure and resilient control systems. recently, the dynamic watermarking idea behind the method is proposed considering detecting malicious sensor attacks considering siso lti systems with partial state observations and mimo lti systems with the full rank input matrix and full state observations; however, these previous approaches cannot be applied to general lti systems that are mimo and have partial state observations. this paper designs the dynamic watermarking idea behind the method considering detecting malicious sensor attacks considering general lti systems, and we provide the new set of asymptotic and statistical tests. we prove these tests should detect attacks that follow the specified attack model (more general than replay attacks), and we also show that these tests simplify to existing tests when a system was siso or has full rank input matrix and full state observations. a benefit of our idea behind the method was demonstrated with the simulation analysis of detecting sensor attacks inside autonomous vehicles. our idea behind the method should distinguish between sensor attacks and wind disturbance (through an internal model principle framework), whereas improperly designed tests cannot distinguish between sensor attacks and wind disturbance."
"background: we propose the method considering estimating a timing of in-bed intervals with the help of objective data inside the large representative u.s. sample, and quantify a association between these intervals and age, sex, and day of a week. methods: a study included 11,951 participants six years and older from a national health and nutrition examination survey (nhanes) 2003-2006, who wore accelerometers to measure physical activity considering seven consecutive days. participants were instructed to remove a device just before a nighttime sleep period and put it back on immediately after. this nighttime period of non-wear is defined inside this paper as a objective bedtime (obt), an objectively estimated record of a in-bed-interval. considering each night of a week, we estimated two measures: a duration of a obt (obt-d) and, as the measure of a chronotype, a midpoint of a obt (obt-m). we estimated day-of-the-week-specific obt-d and obt-m with the help of gender-specific population percentile curves. differences inside obt-m (chronotype) and obt-d (the amount of time spent inside bed) by age and sex were estimated with the help of regression models. results: a estimates of obt-m and their differences among age groups were consistent with a estimates of chronotype obtained using self-report inside european populations. a average obt-m varied significantly by age, while obt-d is less variable with age. a most pronounced differences were observed between obt-m of weekday and weekend nights. conclusions: a proposed measures, obt-d and obt-m, provide useful information of time inside bed and chronotype inside nhanes 2003-2006. they identify within-week patterns of bedtime and should be used to study associations between a bedtime and a large number of health outcomes collected inside nhanes 2003-2006."
"topological crystalline insulators (tcis) have been of great interest inside a area of condensed matter physics. we investigated a effect of indium substitution on a crystal structure and transport properties inside a tci system (pb$_{1-x}$sn$_{x}$)$_{1-y}$in$_{y}$te. considering samples with the tin concentration $x\le50\%$, a low-temperature resisitivities show the dramatic variation as the function of indium concentration: with up to ~2% indium doping a samples show weak-metallic behavior, similar to their parent compounds; with ~6% indium doping, samples have true bulk-insulating resistivity and present evidence considering nontrivial topological surface states; with higher indium doping levels, superconductivity is observed, with the transition temperature, tc, positively correlated to a indium concentration and reaching as high as 4.7 k. we address this issue from a view of bulk electronic structure modified by a indium-induced impurity level that pins a fermi level. a current work summarizes a indium substitution effect on (pb,sn)te, and discusses a topological and superconducting aspects, which should be provide guidance considering future studies on this and related systems."
"over two decades of exoplanetology have yielded thousands of discoveries, yet some types of systems are yet to be observed. circumstellar planets around one star inside the binary have been found, but not considering tight binaries (< 5 au). additionally, extra-solar moons are yet to be found. this paper motivates finding both types of three-body system by calculating analytic and numerical probabilities considering all transit configurations, accounting considering any mutual inclination and orbital precession. a precession and relative three-body motion should increase a transit probability to as high as tens of per cent, and make it inherently time-dependent over the precession period as short as 5-10 yr. circumstellar planets inside such tight binaries present the tempting observational challenge: enhanced transit probabilities but with the quasi-periodic signature that may be difficult to identify. this may aid explain their present non-detection, or maybe they simply do not exist. whilst this paper considers binaries of all orientations, it was demonstrated how eclipsing binaries favourably bias a transit probabilities, sometimes to a point of being guaranteed. transits of exomoons exhibit the similar behaviour under precession, but unfortunately only have one star to transit rather than two."
"a study of social networks --- where people are located, geographically, and how they might be connected to one another --- was the current hot topic of interest, because of its immediate relevance to important applications, from devising efficient immunization techniques considering a arrest of epidemics, to a design of better transportation and city planning paradigms, to a understanding of how rumors and opinions spread and take shape over time. we develop the spatial social complex network (sscn) model that captures not only essential connectivity features of real-life social networks, including the heavy-tailed degree distribution and high clustering, but also a spatial location of individuals, reproducing zipf's law considering a distribution of city populations as well as other observed hallmarks. we then simulate milgram's small-world experiment on our sscn model, obtaining good qualitative agreement with a known results and shedding light on a role played by various network attributes and a strategies used by a players inside a game. this demonstrates a potential of a sscn model considering a simulation and study of a many social processes mentioned above, where both connectivity and geography play the role inside a dynamics."
"empirical evidence suggests that heavy-tailed degree distributions occurring inside many real networks are well-approximated by power laws with exponents $\eta$ that may take values either less than and greater than two. models based on various forms of exchangeability are able to capture power laws with $\eta < 2$, and admit tractable inference algorithms; we draw on previous results to show that $\eta > 2$ cannot be generated by a forms of exchangeability used inside existing random graph models. preferential attachment models generate power law exponents greater than two, but have been of limited use as statistical models due to a inherent difficulty of performing inference inside non-exchangeable models. motivated by this gap, we design and implement inference algorithms considering the recently proposed class of models that generates $\eta$ of all possible values. we show that although they are not exchangeable, these models have probabilistic structure amenable to inference. our methods make the large class of previously intractable models useful considering statistical inference."
"we present the new distributed representation inside deep neural nets wherein a information was represented inside native form as the matrix. this differs from current neural architectures that rely on vector representations. we consider matrices as central to a architecture and they compose a input, hidden and output layers. a model representation was more compact and elegant -- a number of parameters grows only with a largest dimension of a incoming layer rather than a number of hidden units. we derive several new deep networks: (i) feed-forward nets that map an input matrix into an output matrix, (ii) recurrent nets which map the sequence of input matrices into the sequence of output matrices. we also reinterpret existing models considering (iii) memory-augmented networks and (iv) graphs with the help of matrix notations. considering graphs we demonstrate how a new notations lead to simple but effective extensions with multiple attentions. extensive experiments on handwritten digits recognition, face reconstruction, sequence to sequence learning, eeg classification, and graph-based node classification demonstrate a efficacy and compactness of a matrix architectures."
"we present atacama large millimeter array (alma) observations at an angular resolution of 0.1-0.2"" of a disk surrounding a young herbig ae star mwc 758. a data consist of images of a dust continuum emission recorded at 0.88 millimeter, as well as images of a 13co and c18o j = 3-2 emission lines. a dust continuum emission was characterized by the large cavity of roughly 40 au inside radius which might contain the mildly inner warped disk. a outer disk features two bright emission clumps at radii of about 47 and 82 au that present azimuthal extensions and form the double-ring structure. a comparison with radiative transfer models indicates that these two maxima of emission correspond to local increases inside a dust surface density of about the factor 2.5 and 6.5 considering a south and north clumps, respectively. a optically thick 13co peak emission, which traces a temperature, and a dust continuum emission, which probes a disk midplane, additionally reveal two spirals previously detected inside near-ir at a disk surface. a spirals seen inside a dust continuum emission present, however, the slight shift of the few au towards larger radii and one of a spirals crosses a south dust clump. finally, we present different scenarios inside order to explain a complex structure of a disk."
"we propose an algorithm to compute a dynamics of articulated rigid-bodies with different sensor distributions. prior to a on-line computations, a proposed algorithm performs an off-line optimisation step to simplify a computational complexity of a underlying solution. this optimisation step consists inside formulating a dynamic computations as the system of linear equations. a computational complexity of computing a associated solution was reduced by performing the permuted lu-factorisation with off-line optimised permutations. we apply our algorithm to solve classical dynamic problems: inverse and forward dynamics. a computational complexity of a proposed solution was compared to `gold standard' algorithms: recursive newton-euler and articulated body algorithm. it was shown that our algorithm reduces a number of floating point operations with respect to previous approaches. we also evaluate a numerical complexity of our algorithm by performing tests on dynamic computations considering which no gold standard was available."
"a messenger mission sought to discover what physical processes determined mercury's high metal to silicate ratio. instead, a mission has discovered multiple anomalous characteristics about our innermost planet. a lack of feo and a reduced oxidation state of mercury's crust and mantle are more extreme than nearly all other known materials inside a solar system. inside contrast, moderately volatile elements are present inside abundances comparable to a other terrestrial planets. no single process during mercury's formation was able to explain all of these observations. here, we review a current ideas considering a origin of mercury's unique features. gaps inside understanding a innermost regions of a solar nebula limit testing different hypotheses. even so, all proposed models are incomplete and need further development inside order to unravel mercury's remaining secrets."
"machine learning models have proved extremely successful considering the wide variety of supervised learning problems, but a predictions of many of these models are difficult to interpret. the recent literature interprets a predictions of more general ""black-box"" machine learning models by approximating these models inside terms of simpler models such as piecewise linear or piecewise constant models. existing literature constructs these approximations inside an ad-hoc manner. we provide the tractable dynamic programming algorithm that partitions a feature space into clusters inside the principled way and then uses this partition to provide both piecewise constant and piecewise linear interpretations of an arbitrary ""black-box"" model. when loss was measured inside terms of mean squared error, our approximation was optimal (under certain conditions); considering more general loss functions, our interpretation was probably approximately optimal (in a sense of pac learning). experiments with real and synthetic data show that it continues to provide significant improvements (in terms of mean squared error) over competing approaches."
"social media platforms provide continuous access to user generated content that enables real-time monitoring of user behavior and of events. a geographical dimension of such user behavior and events has recently caught the lot of attention inside several domains: mobility, humanitarian, or infrastructural. while resolving a location of the user should be straightforward, depending on a affordances of their device and/or of a application they are using, inside most cases, locating the user demands the larger effort, such as exploiting textual features. on twitter considering instance, only 2% of all tweets are geo-referenced. inside this paper, we present the system considering zoomed-in grounding (below city level) considering short messages (e.g., tweets). a system combines different natural language processing and machine learning techniques to increase a number of geo-grounded tweets, which was essential to many applications such as disaster response and real-time traffic monitoring."
"let $l/k$ be the tame and galois extension of number fields with group $g$. it was well-known that any ambiguous ideal inside $l$ was locally free over $\mathcal{o}_kg$ (of rank one), and so it defines the class inside a locally free class group of $\mathcal{o}_kg$, where $\mathcal{o}_k$ denotes a ring of integers of $k$. inside this paper, we shall study a relationship among a classes arising from a ring of integers $\mathcal{o}_l$ of $l$, a inverse different $\mathfrak{d}_{l/k}^{-1}$ of $l/k$, and a square root of a inverse different $a_{l/k}$ of $l/k$ (if it exists), inside a case that $g$ was abelian. they are naturally related because $a_{l/k}^2 = \mathfrak{d}_{l/k}^{-1} = \mathcal{o}_l^*$, and $a_{l/k}$ was special because $a_{l/k} = a_{l/k}^*$, where $*$ denotes dual with respect to a trace of $l/k$."
"many tasks inside artificial intelligence require a collaboration of multiple agents. we exam deep reinforcement learning considering multi-agent domains. recent research efforts often take a form of two seemingly conflicting perspectives, a decentralized perspective, where each agent was supposed to have its own controller; and a centralized perspective, where one assumes there was the larger model controlling all agents. inside this regard, we revisit a idea of a master-slave architecture by incorporating both perspectives within one framework. such the hierarchical structure naturally leverages advantages from one another. a idea of combining both perspectives was intuitive and should be well motivated from many real world systems, however, out of the variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. with network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both inside synthetic experiments and when applied to challenging starcraft micromanagement tasks."
"inside this paper we establish the close connection between three notions at- tached to the modular subgroup. namely a set of weight two meromorphic modular forms, a set of equivariant functions on a upper half-plane commuting with a action of a modular subgroup and a set of elliptic zeta functions generalizing a weierstrass zeta functions. inside particular, we show that a equivariant functions should be parameterized by modular objects as well as by elliptic objects."
"inside this work, we consider solutions of a maxwell equations on a schwarzschild-de sitter family of black hole spacetimes. we prove that, inside a static region bounded by black hole and cosmological horizons, solutions of a maxwell equations decay to stationary coulomb solutions at the super-polynomial rate, with decay measured according to ingoing and outgoing null coordinates. our method employs the differential transformation of maxwell tensor components to obtain higher-order quantities satisfying the fackerell-ipser equation, inside a style of chandrasekhar and a more recent work of pasqualotto. a analysis of a fackerell-ipser equation was accomplished by means of a vector field method, with decay estimates considering a higher-order quantities leading to decay estimates considering components of a maxwell tensor."
"modeling of longitudinal data often requires diffusion models that incorporate overall time-dependent, nonlinear dynamics of multiple components and provide sufficient flexibility considering subject-specific modeling. this complexity challenges parameter inference and approximations are inevitable. we propose the method considering approximate maximum-likelihood parameter approximation inside multivariate time-inhomogeneous diffusions, where subject-specific flexibility was accounted considering by incorporation of multidimensional mixed effects and covariates. we consider $n$ multidimensional independent diffusions $x^i = (x^i_t)_{0\leq t\leq t^i}, 1\leq i\leq n$, with common overall model structure and unknown fixed-effects parameter $\mu$. their dynamics differ by a subject-specific random effect $\phi^i$ inside a drift and possibly by (known) covariate information, different initial conditions and observation times and duration. a distribution of $\phi^i$ was parametrized by an unknown $\vartheta$ and $\theta = (\mu, \vartheta)$ was a target of statistical inference. its maximum likelihood estimator was derived from a continuous-time likelihood. we prove consistency and asymptotic normality of $\hat{\theta}_n$ when a number $n$ of subjects goes to infinity with the help of standard techniques and consider a more general concept of local asymptotic normality considering less regular models. a bias induced by time-discretization of sufficient statistics was investigated. we discuss verification of conditions and investigate parameter approximation and hypothesis testing inside simulations."
"inside this paper, we prove a existence of the global entropy weak solution $u\in h^1(\mathbb{r})$ and $\partial_{x}u\in l^1(\mathbb{r})\cap bv(\mathbb{r})$ considering a cauchy problem of the generalized camassa-holm equation by a viscous approximation method."
"a well known duality between a sobolev inequality and a hardy-littlewood-sobolev inequality suggests that a nash inequality could also have an interesting dual form, even though a nash inequality relates three norms instead of two. we provide such the dual form here with sharp constants. this dual inequality relates a $l^2$ norm to a infimal convolution of a $l^\infty $ and $h^{-1}$ norms. a computation of this infimal convolution was the minimization problem, which we solve explicitly, thus providing the new proof of a sharp nash inequality itself. this proof, using duality, also yields a sharp form of some new, weighted generalizations of a nash inequality as well as a dual of these weighted variants."
"inside this work we study a two-orbital hubbard model on the square lattice inside a presence of hybridization between nearest-neighbor orbitals and the crystal-field splitting. we use the highly reliable numerical technique based on a density matrix renormalization group to solve a dynamical mean field theory self-consistent impurity problem. we find that a orbital mixing always leads to the finite local density states at a fermi energy inside both orbitals when at least one band was metallic. when one band was doped, and a chemical potential lies between a hubbard bands inside a other band, a coherent quasiparticle peak inside this orbital has an exponential behavior with a hubbard interaction $u$."
"we study an industrial computer code related to nuclear safety. the major topic of interest was to assess a uncertainties tainting a results of the computer simulation. inside this work we gain robustness on a quantification of the risk measurement by accounting considering all sources of uncertainties tainting a inputs of the computer code. to that extent, we evaluate a maximum quantile over the class of distributions defined only by constraints on their moments. two options are available when dealing with such complex optimization problems: one should either optimize under constraints; or preferably, one should reformulate a objective function. we identify the well suited parameterization to compute a optimal quantile based on a theory of canonical moments. it allows an effective, free of constraints, optimization."
"computation has changed a world more than any previous expressions of knowledge. inside its particular algorithmic embodiment, it offers the perspective, within which a digital computer (one of many possible) exercises the role reminiscent of theology. since it was closed to meaning, algorithmic digital computation should at most mimic a creative aspects of life. ai, inside a perspective of time, proved to be less an acronym considering artificial intelligence and more of automating tasks associated with intelligence. a entire development led to a hypostatized role of a machine: outputting nothing else but reality, including that of a humanity that made a machine happen. a convergence machine called deep learning was only a latest form through which a deterministic theology of a machine claims more than what extremely effective data processing actually is. the new understanding of complexity, as well as a need to distinguish between a reactive nature of a artificial and a anticipatory nature of a living are suggested as practical responses to a challenges posed by machine theology."
"when the 2d superconductor was subjected to the strong in-plane magnetic field, zeeman polarization of a fermi surface should give rise to inhomogeneous fflo order with the spatially modulated gap. further increase of a magnetic field eventually drives a system into the normal metal state. here, we perform the renormalization group analysis of this quantum phase transition, starting from an appropriate low-energy theory recently introduced by piazza et al. (ref.1). we compute one-loop flow equations within a controlled dimensional regularization scheme with fixed dimension of fermi surface, expanding inside $\epsilon = 5/2 - d$. we find the new stable non-fermi liquid fixed point and discuss its critical properties. one of a most interesting aspects of a fflo non-fermi liquid scenario was that a quantum critical point was potentially naked, with a scaling regime observable down to arbitrary low temperatures. inside order to study this possibility, we perform the general analysis of competing instabilities, which suggests that only charge density wave order was enhanced inside a vicinity of a quantum critical point."
"inside this paper we study multi-agent discrete-event systems where a agents should be divided into several groups, and within each group a agents have similar or identical state transition structures. we employ the relabeling map to generate the ""template structure"" considering each group, and synthesize the scalable supervisor whose state size and computational process are independent of a number of agents. this scalability allows a supervisor to remain invariant (no recomputation or reconfiguration needed) if and when there are agents removed due to failure or added considering increasing productivity. a constant computational effort considering synthesizing a scalable supervisor also makes our method promising considering handling large-scale multi-agent systems. moreover, based on a scalable supervisor we design scalable local controllers, one considering each component agent, to establish the purely distributed control architecture. three examples are provided to illustrate our proposed scalable supervisory synthesis and a resulting scalable supervisors as well as local controllers."
"ordinary least square (ols) approximation of the linear regression model was well-known to be highly sensitive to outliers. it was common practice to first identify and remove outliers by looking at a data then to fit ols and form confidence intervals and p-values on a remaining data as if this were a original data collected. we show inside this paper that this ""detect-and-forget"" idea behind the method should lead to invalid inference, and we propose the framework that properly accounts considering outlier detection and removal to provide valid confidence intervals and hypothesis tests. our inferential procedures apply to any outlier removal procedure that should be characterized by the set of quadratic constraints on a response vector, and we show that several of a most commonly used outlier detection procedures are of this form. our methodology was built upon recent advances inside selective inference (taylor & tibshirani 2015), which are focused on inference corrected considering variable selection. we conduct simulations to corroborate a theoretical results, and we apply our method to two classic data sets considered inside a outlier detection literature to illustrate how our inferential results should differ from a traditional detect-and-forget strategy. the companion r package, outference, implements these new procedures with an interface that matches a functions commonly used considering inference with lm inside r."
"learning-based hashing methods are widely used considering nearest neighbor retrieval, and recently, online hashing methods have demonstrated good performance-complexity trade-offs by learning hash functions from streaming data. inside this paper, we first address the key challenge considering online hashing: a binary codes considering indexed data must be recomputed to keep pace with updates to a hash functions. we propose an efficient quality measure considering hash functions, based on an information-theoretic quantity, mutual information, and use it successfully as the criterion to eliminate unnecessary hash table updates. next, we also show how to optimize a mutual information objective with the help of stochastic gradient descent. we thus develop the novel hashing method, mihash, that should be used inside both online and batch settings. experiments on image retrieval benchmarks (including the 2.5m image dataset) confirm a effectiveness of our formulation, both inside reducing hash table recomputations and inside learning high-quality hash functions."
"we consider a energy-critical half-wave maps equation $$\partial_t \mathbf{u} + \mathbf{u} \wedge |\nabla| \mathbf{u} = 0$$ considering $\mathbf{u} : [0,t) \times \mathbb{r} \to \mathbb{s}^2$. we give the complete classification of all traveling solitary waves with finite energy. a proof was based on the geometric characterization of these solutions as minimal surfaces with (not necessarily free) boundary on $\mathbb{s}^2$. inside particular, we discover an explicit lorentz boost symmetry, which was implemented by a conformal möbius group on a target $\mathbb{s}^2$ applied to half-harmonic maps from $\mathbb{r}$ to $\mathbb{s}^2$. complementing our classification result, we carry out the detailed analysis of a linearized operator $l$ around half-harmonic maps $\mathbf{q}$ with arbitrary degree $m \geq 1$. here we explicitly determine a nullspace including a zero-energy resonances; inside particular, we prove a nondegeneracy of $\mathbf{q}$. moreover, we give the full description of a spectrum of $l$ by finding all its $l^2$-eigenvalues and proving their simplicity. furthermore, we prove the coercivity approximate considering $l$ and we rule out embedded eigenvalues in a essential spectrum. our spectral analysis was based on the reformulation inside terms of certain jacobi operators (tridiagonal infinite matrices) obtained from the conformal transformation of a spectral problem posed on $\mathbb{r}$ to a unit circle $\mathbb{s}$. finally, we construct the unitary map which should be seen as the gauge transform tailored considering the future stability and blowup analysis close to half-harmonic maps. our spectral results also have potential applications to a half-harmonic map heat flow, which was a parabolic counterpart of a half-wave maps equation."
"recent advances inside bioinformatics have made high-throughput microbiome data widely available, and new statistical tools are required to maximize a information gained from these data. considering example, analysis of high-dimensional microbiome data from designed experiments remains an open area inside microbiome research. contemporary analyses work on metrics that summarize collective properties of a microbiome, but such reductions preclude inference on a fine-scale effects of environmental stimuli on individual microbial taxa. other approaches model a proportions or counts of individual taxa as response variables inside mixed models, but these methods fail to account considering complex correlation patterns among microbial communities. inside this paper, we propose the novel bayesian mixed-effects model that exploits cross-taxa correlations within a microbiome, the model we call mimix (microbiome mixed model). mimix offers global tests considering treatment effects, local tests and approximation of treatment effects on individual taxa, quantification of a relative contribution from heterogeneous sources to microbiome variability, and identification of latent ecological subcommunities inside a microbiome. mimix was tailored to large microbiome experiments with the help of the combination of bayesian factor analysis to efficiently represent dependence between taxa and bayesian variable selection methods to achieve sparsity. we demonstrate a model with the help of the simulation experiment and on the 2x2 factorial experiment of a effects of nutrient supplement and herbivore exclusion on a foliar fungal microbiome of $\textit{andropogon gerardii}$, the perennial bunchgrass, as part of a global nutrient network research initiative."
"we present the new model drnet that learns disentangled image representations from video. our idea behind the method leverages a temporal coherence of video and the novel adversarial loss to learn the representation that factorizes each frame into the stationary part and the temporally varying component. a disentangled representation should be used considering the range of tasks. considering example, applying the standard lstm to a time-vary components enables prediction of future frames. we evaluate our idea behind the method on the range of synthetic and real videos, demonstrating a ability to coherently generate hundreds of steps into a future."
"social network analysis provides meaningful information about behavior of network members that should be used considering diverse applications such as classification, link prediction. however, network analysis was computationally expensive because of feature learning considering different applications. inside recent years, many researches have focused on feature learning methods inside social networks. network embedding represents a network inside the lower dimensional representation space with a same properties which presents the compressed representation of a network. inside this paper, we introduce the novel algorithm named ""care"" considering network embedding that should be used considering different types of networks including weighted, directed and complex. current methods try to preserve local neighborhood information of nodes, whereas a proposed method utilizes local neighborhood and community information of network nodes to cover both local and global structure of social networks. care builds customized paths, which are consisted of local and global structure of network nodes, as the basis considering network embedding and uses a skip-gram model to learn representation vector of nodes. subsequently, stochastic gradient descent was applied to optimize our objective function and learn a final representation of nodes. our method should be scalable when new nodes are appended to network without information loss. parallelize generation of customized random walks was also used considering speeding up care. we evaluate a performance of care on multi label classification and link prediction tasks. experimental results on various networks indicate that a proposed method outperforms others inside both micro and macro-f1 measures considering different size of training data."
"we associate to the 2-vector bundle over an essentially finite groupoid the 2-vector space of parallel sections, or, inside representation theoretic terms, of higher invariants, which should be described as homotopy fixed points. our main result was a extension of this assignment to the symmetric monoidal 2-functor $\operatorname{par} : \mathbf{2vecbungrpd} \to \mathbf{2vect}$. it was defined on a symmetric monoidal bicategory $\mathbf{2vecbungrpd}$ whose morphisms arise from spans of groupoids inside such the way that a functor $\operatorname{par}$ provides pull-push maps between 2-vector spaces of parallel sections of 2-vector bundles. a direct motivation considering our construction comes from a orbifoldization of extended equivariant topological field theories."
"a proposed algorithmic idea behind the method deals with finding a sense of the word inside an electronic data. now the day,in different communication mediums like internet, mobile services etc. people use few words, which are slang inside nature. this idea behind the method detects those abusive words with the help of supervised learning procedure. but inside a real life scenario, a slang words are not used inside complete word forms always. most of a times, those words are used inside different abbreviated forms like sounds alike forms, taboo morphemes etc. this proposed idea behind the method should detect those abbreviated forms also with the help of semi supervised learning procedure. with the help of a synset and concept analysis of a text, a probability of the suspicious word to be the slang word was also evaluated."
"many different classification tasks need to manage structured data, which are usually modeled as graphs. moreover, these graphs should be dynamic, meaning that a vertices/edges of each graph may change during time. our goal was to jointly exploit structured data and temporal information through a use of the neural network model. to a best of our knowledge, this task has not been addressed with the help of these kind of architectures. considering this reason, we propose two novel approaches, which combine long short-term memory networks and graph convolutional networks to learn long short-term dependencies together with graph structure. a quality of our methods was confirmed by a promising results achieved."
"many real-world systems are profitably described as complex networks that grow over time. preferential attachment and node fitness are two simple growth mechanisms that not only explain certain structural properties commonly observed inside real-world systems, but are also tied to the number of applications inside modeling and inference. while there are statistical packages considering estimating various parametric forms of a preferential attachment function, there was no such package implementing non-parametric approximation procedures. a non-parametric idea behind the method to a approximation of a preferential attachment function allows considering comparatively finer-grained investigations of a `rich-get-richer' phenomenon that could lead to novel insights inside a search to explain certain nonstandard structural properties observed inside real-world networks. this paper introduces a r package pafit, which implements non-parametric procedures considering estimating a preferential attachment function and node fitnesses inside the growing network, as well as the number of functions considering generating complex networks from these two mechanisms. a main computational part of a package was implemented inside c++ with openmp to ensure scalability to large-scale networks. we first introduce a main functionalities of pafit through simulated examples, and then use a package to analyze the collaboration network between scientists inside a field of complex networks. a results indicate a joint presence of `rich-get-richer' and `fit-get-richer' phenomena inside a collaboration network. a estimated attachment function was observed to be near-linear, which we interpret as meaning that a chance an author gets the new collaborator was proportional to their current number of collaborators. furthermore, a estimated author fitnesses reveal the host of familiar faces from a complex networks community among a field's topmost fittest network scientists."
"inside this paper, an explicit expression was obtained considering a conformally invariant higher spin laplace operator $\mathcal{d}_{\lambda}$, which acts on functions taking values inside an arbitrary (finite-dimensional) irreducible representation considering a orthogonal group with integer valued highest weight. once an explicit expression was obtained, the special kind of (polynomial) solutions of this operator was determined."
"intrinsically motivated goal exploration processes enable agents to autonomously sample goals to explore efficiently complex environments with high-dimensional continuous actions. they have been applied successfully to real world robots to discover repertoires of policies producing the wide diversity of effects. often these algorithms relied on engineered goal spaces but it is recently shown that one should use deep representation learning algorithms to learn an adequate goal space inside simple environments. however, inside a case of more complex environments containing multiple objects or distractors, an efficient exploration requires that a structure of a goal space reflects a one of a environment. inside this paper we show that with the help of the disentangled goal space leads to better exploration performances than an entangled goal space. we further show that when a representation was disentangled, one should leverage it by sampling goals that maximize learning progress inside the modular manner. finally, we show that a measure of learning progress, used to drive curiosity-driven exploration, should be used simultaneously to discover abstract independently controllable features of a environment."
"we present a first limits on a epoch of reionization (eor) 21-cm hi power spectra, inside a redshift range $z=7.9-10.6$, with the help of a low-frequency array (lofar) high-band antenna (hba). inside total 13\,h of data were used from observations centred on a north celestial pole (ncp). after subtraction of a sky model and a noise bias, we detect the non-zero $\delta^2_{\rm i} = (56 \pm 13 {\rm mk})^2$ (1-$\sigma$) excess variance and the best 2-$\sigma$ upper limit of $\delta^2_{\rm 21} < (79.6 {\rm mk})^2$ at $k=0.053$$h$cmpc$^{-1}$ inside a range $z=$9.6-10.6. a excess variance decreases when optimizing a smoothness of a direction- and frequency-dependent gain calibration, and with increasing a completeness of a sky model. it was likely caused by (i) residual side-lobe noise on calibration baselines, (ii) leverage due to non-linear effects, (iii) noise and ionosphere-induced gain errors, or the combination thereof. further analyses of a excess variance will be discussed inside forthcoming publications."
"gaussian process (gp) regression was the powerful interpolation technique due to its flexibility inside capturing non-linearity. inside this paper, we provide the general framework considering understanding a frequentist coverage of point-wise and simultaneous bayesian credible sets inside gp regression. as an intermediate result, we develop the bernstein von-mises type result under supremum norm inside random design gp regression. identifying both a mean and covariance function of a posterior distribution of a gaussian process as regularized $m$-estimators, we show that a sampling distribution of a posterior mean function and a centered posterior distribution should be respectively approximated by two population level gps. by developing the comparison inequality between two gps, we provide exact characterization of frequentist coverage probabilities of bayesian point-wise credible intervals and simultaneous credible bands of a regression function. our results show that inference based on gp regression tends to be conservative; when a prior was under-smoothed, a resulting credible intervals and bands have minimax-optimal sizes, with their frequentist coverage converging to the non-degenerate value between their nominal level and one. as the byproduct of our theory, we show that a gp regression also yields minimax-optimal posterior contraction rate relative to a supremum norm, which provides the positive evidence to a long standing problem on optimal supremum norm contraction rate inside gp regression."
"linear perturbations of a wave dark matter, or $\psi$ dark matter ($\psi$dm), of particle mass $\sim 10^{-22}$ev inside a radiation-dominant era are analyzed, and a matter power spectrum at a photon-matter equality was obtained. we identify four phases of evolution considering $\psi$dm perturbations, where a dynamics should be vastly different from a counterparts of cold dark matter (cdm). while inside late stages after mass oscillation long-wave $\psi$dm perturbations are almost identical to cdm perturbations, some subtle differences remain, let alone intermediate-to-short waves that bear no resemblance with those of cdm throughout a whole evolutionary history. a dissimilarity was due to quantum mechanical effects which lead to severe mode suppression. we also discuss a axion model with the cosine field potential. a power spectrum of axion models are generally almost identical to those of $\psi$dm, but inside a extreme case when a initial axion angle was near a field potential top, this axion model predict the power excess over the range of wave number and the higher spectral cutoff than $\psi$dm as if $\psi$dm had the higher particle mass."
"quora was one of a most popular community q&a sites of recent times. however, many question posts on this q&a site often do not get answered. inside this paper, we quantify various linguistic activities that discriminates an answered question from an unanswered one. our central finding was that a way users use language while writing a question text should be the very effective means to characterize answerability. this characterization helps us to predict early if the question remaining unanswered considering the specific time period t will eventually be answered or not and achieve an accuracy of 76.26% (t = 1 month) and 68.33% (t = 3 months). notably, features representing a language use patterns of a users are most discriminative and alone account considering an accuracy of 74.18%. we also compare our method with some of a similar works (dror et al., yang et al.) achieving the maximum improvement of ~39% inside terms of accuracy."
"inside extensions of a standard model with extra scalars, a electroweak phase transition should be very strong, and a bubble walls should be highly relativistic. we revisit our previous argument that electroweak bubble walls should ""run away,"" that is, achieve extreme ultrarelativistic velocities $\gamma \sim 10^{14}$. we show that, when particles cross a bubble wall, they should emit transition radiation. wall-frame soft processes, though suppressed by the power of a coupling $\alpha$, have the significance enhanced by a $\gamma$-factor of a wall, limiting wall velocities to $\gamma \sim 1/\alpha$. though a bubble walls should move at almost a speed of light, they carry an infinitesimal share of a plasma's energy."
"this paper demonstrates end-to-end neural network architectures considering vietnamese named entity recognition. our best model was the combination of bidirectional long short-term memory (bi-lstm), convolutional neural network (cnn), conditional random field (crf), with the help of pre-trained word embeddings as input, which achieves an f1 score of 88.59% on the standard test set. our system was able to achieve the comparable performance to a first-rank system of a vlsp campaign without with the help of any syntactic or hand-crafted features. we also give an extensive empirical study on with the help of common deep learning models considering vietnamese ner, at both word and character level."
"robots such as autonomous underwater vehicles (auvs) and autonomous surface vehicles (asvs) have been used considering sensing and monitoring aquatic environments such as oceans and lakes. environmental sampling was the challenging task because a environmental attributes to be observed should vary both spatially and temporally, and a target environment was usually the large and continuous domain whereas a sampling data was typically sparse and limited. a challenges require that a sampling method must be informative and efficient enough to catch up with a environmental dynamics. inside this paper we present the planning and learning method that enables the sampling robot to perform persistent monitoring tasks by learning and refining the dynamic ""data map"" that models the spatiotemporal environment attribute such as ocean salinity content. our environmental sampling framework consists of two components: to maximize a information collected, we propose an informative planning component that efficiently generates sampling waypoints that contain a maximal information; to alleviate a computational bottleneck caused by large-scale data accumulated, we develop the component based on the sparse gaussian process whose hyperparameters are learned online by taking advantage of only the subset of data that provides a greatest contribution. we validate our method with both simulations running on real ocean data and field trials with an asv inside the lake environment. our experiments show that a proposed framework was both accurate inside learning a environmental data map and efficient inside catching up with a dynamic environmental changes."
"agile localization of anomalous events plays the pivotal role inside enhancing a overall reliability of a grid and avoiding cascading failures. this was especially of paramount significance inside a large-scale grids due to their geographical expansions and a large volume of data generated. this paper proposes the stochastic graphical framework, by leveraging which it aims to localize a anomalies with a minimum amount of data. this framework capitalizes on a strong correlation structures observed among a measurements collected from different buses. a proposed approach, at its core, collects a measurements sequentially and progressively updates its decision about a location of a anomaly. a process resumes until a location of a anomaly should be identified with desired reliability. we provide the general theory considering a quickest anomaly localization and also investigate its application considering quickest line outage localization. simulations inside a ieee 118-bus model are provided to establish a gains of a proposed approach."
"we construct the dynamical system considering the reaction diffusion system due to murray, which relies on a use of a thomas system nonlinearities and describes a formation of animal coat patterns. first, we prove existence and uniqueness of global positive strong solutions to a system by with the help of semigroup methods. second, we show that a solutions are continuously dependent on initial values. third, we show that a dynamical system enjoys exponential attractors whose fractal dimensions should be estimated. finally, we give the numerical example."
"recognition of handwritten mathematical expressions (hmes) was the challenging problem because of a ambiguity and complexity of two-dimensional handwriting. moreover, a lack of large training data was the serious issue, especially considering academic recognition systems. inside this paper, we propose pattern generation strategies that generate shape and structural variations to improve a performance of recognition systems based on the small training set. considering data generation, we employ a public databases: crohme 2014 and 2016 of online hmes. a first strategy employs local and global distortions to generate shape variations. a second strategy decomposes an online hme into sub-online hmes to get more structural variations. a hybrid strategy combines both these strategies to maximize shape and structural variations. a generated online hmes are converted to images considering offline hme recognition. we tested our strategies inside an end-to-end recognition system constructed from the recent deep learning model: convolutional neural network and attention-based encoder-decoder. a results of experiments on a crohme 2014 and 2016 databases demonstrate a superiority and effectiveness of our strategies: our hybrid strategy achieved classification rates of 48.78% and 45.60%, respectively, on these databases. these results are competitive compared to others reported inside recent literature. our generated datasets are openly available considering research community and constitute the useful resource considering a hme recognition research inside future."
"predictive modeling was invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). such methods are usually described by the large number of parameters or hyper parameters - the price that one needs to pay considering elasticity. a very number of parameters makes models hard to understand. this paper describes the consistent collection of explainers considering predictive models, a.k.a. black boxes. each explainer was the technique considering exploration of the black box model. presented approaches are model-agnostic, what means that they extract useful information from any predictive method despite its internal structure. each explainer was linked with the specific aspect of the model. some are useful inside decomposing predictions, some serve better inside understanding performance, while others are useful inside understanding importance and conditional responses of the particular variable. every explainer presented inside this paper works considering the single model or considering the collection of models. inside a latter case, models should be compared against each other. such comparison helps to find strengths and weaknesses of different approaches and gives additional possibilities considering model validation. presented explainers are implemented inside a dalex package considering r. they are based on the uniform standardized grammar of model exploration which may be easily extended. a current implementation supports a most popular frameworks considering classification and regression."
"let $(u_n)_{n \geq 0}$ be the nondegenerate linear recurrence of integers, and let $\mathcal{a}$ be a set of positive integers $n$ such that $u_n$ and $n$ are relatively prime. we prove that $\mathcal{a}$ has an asymptotic density, and that this density was positive unless $(u_n / n)_{n \geq 1}$ was the linear recurrence."
"this work concerns sampling of smooth signals on arbitrary graphs. we first study the structured sampling strategy considering such smooth graph signals that consists of the random selection of few pre-defined groups of nodes. a number of groups to sample to stably embed a set of $k$-bandlimited signals was driven by the quantity called a \emph{group} graph cumulative coherence. considering some optimised sampling distributions, we show that sampling $o(k\log(k))$ groups was always sufficient to stably embed a set of $k$-bandlimited signals but that this number should be smaller -- down to $o(\log(k))$ -- depending on a structure of a groups of nodes. fast methods to approximate these sampling distributions are detailed. second, we consider $k$-bandlimited signals that are nearly piecewise constant over pre-defined groups of nodes. we show that it was possible to speed up a reconstruction of such signals by reducing drastically a dimension of a vectors to reconstruct. when combined with a proposed structured sampling procedure, we prove that a method provides stable and accurate reconstruction of a original signal. finally, we present numerical experiments that illustrate our theoretical results and, as an example, show how to combine these methods considering interactive object segmentation inside an image with the help of superpixels."
"we present a very first robust bayesian online changepoint detection algorithm through general bayesian inference (gbi) with $\beta$-divergences. a resulting inference procedure was doubly robust considering both a parameter and a changepoint (cp) posterior, with linear time and constant space complexity. we provide the construction considering exponential models and demonstrate it on a bayesian linear regression model. inside so doing, we make two additional contributions: firstly, we make gbi scalable with the help of structural variational approximations that are exact as $\beta \to 0$. secondly, we give the principled way of choosing a divergence parameter $\beta$ by minimizing expected predictive loss on-line. reducing false discovery rates of cps from more than 90% to 0% on real world data, this offers a state of a art."
"recent years have witnessed the widespread increase of interest inside network representation learning (nrl). by far most research efforts have focused on nrl considering homogeneous networks like social networks where vertices are of a same type, or heterogeneous networks like knowledge graphs where vertices (and/or edges) are of different types. there has been relatively little research dedicated to nrl considering bipartite networks. arguably, generic network embedding methods like node2vec and line should also be applied to learn vertex embeddings considering bipartite networks by ignoring a vertex type information. however, these methods are suboptimal inside doing so, since real-world bipartite networks concern a relationship between two types of entities, which usually exhibit different properties and patterns from other types of network data. considering example, e-commerce recommender systems need to capture a collaborative filtering patterns between customers and products, and search engines need to consider a matching signals between queries and webpages. this work addresses a research gap of learning vertex representations considering bipartite networks. we present the new solution bine, short considering bipartite network embedding}, which accounts considering two special properties of bipartite networks: long-tail distribution of vertex degrees and implicit connectivity relations between vertices of a same type. technically speaking, we make three contributions: (1) we design the biased random walk generator to generate vertex sequences that preserve a long-tail distribution of vertices; (2) we propose the new optimization framework by simultaneously modeling a explicit relations (i.e., observed links) and implicit relations (i.e., unobserved but transitive links); (3) we explore a theoretical foundations of bine to shed light on how it works, proving that bine should be interpreted as factorizing multiple matrices."
"inside this work we establish a first linear convergence result considering a stochastic heavy ball method. a method performs sgd steps with the fixed stepsize, amended by the heavy ball momentum term. inside a analysis, we focus on minimizing a expected loss and not on finite-sum minimization, which was typically the much harder problem. while inside a analysis we constrain ourselves to quadratic loss, a overall objective was not necessarily strongly convex."
"inside a iron-based superconductors, understanding a relation between superconductivity and electronic structure upon doping was crucial considering exploring a pairing mechanism. recently it is found that inside iron selenide (fese), enhanced superconductivity (tc over 40k) should be achieved using electron doping, with a fermi surface only comprising m-centered electron pockets. here by utilizing surface potassium dosing, scanning tunneling microscopy/spectroscopy (stm/sts) and angle-resolved photoemission spectroscopy (arpes), we studied a electronic structure and superconductivity of (li0.8fe0.2oh)fese inside a deep electron-doped regime. we find that the {\gamma}-centered electron band, which originally lies above a fermi level (ef), should be continuously tuned to cross ef and contribute the new electron pocket at {\gamma}. when this lifshitz transition occurs, a superconductivity inside a m-centered electron pocket was slightly suppressed; while the possible superconducting gap with small size (up to ~5 mev) and the dome-like doping dependence was observed on a new {\gamma} electron pocket. upon further k dosing, a system eventually evolves into an insulating state. our findings provide new clues to understand superconductivity versus fermi surface topology and a correlation effect inside fese-based superconductors."
"motivated by experimental observations of time-symmetry breaking behavior inside the periodically driven (floquet) system, we study the one-dimensional spin model to explore a stability of such floquet discrete time crystals (dtcs) under a interplay between interaction and a microwave driving. considering intermediate interactions and high drivings, from a time evolution of both stroboscopic spin polarization and mutual information between two ends, we show that floquet dtcs should exist inside the prethermal time regime without a tuning of strong disorder. considering much weak interactions a system was the symmetry-unbroken phase, while considering strong interactions it gives its way to the thermal phase. through analyzing a entanglement dynamics, we show that large driving fields protect a prethermal dtcs from many-body localization and thermalization. our results suggest that by increasing a spin interaction, one should drive a experimental system into optimal regime considering observing the robust prethermal dtc phase."
"several material families show competition between superconductivity and other orders. when such competition was driven by doping, it invariably involves spatial inhomogeneities which should seed competing orders. we study impurity-induced charge order inside a attractive hubbard model, the prototypical model considering competition between superconductivity and charge density wave order. we show that the single impurity induces the charge-ordered texture over the length scale set by a energy cost of a competing phase. our results are consistent with the strong-coupling field theory proposed earlier inside which superconducting and charge order parameters form components of an $so(3)$ vector field. to discuss a effects of multiple impurities, we focus on two cases: correlated and random distributions. inside a correlated case, a cdw puddles around each impurity overlap coherently leading to the `supersolid' phase with coexisting pairing and charge order. inside contrast, the random distribution of impurities does not lead to coherent cdw formation. we argue that a energy lowering from coherent ordering should have the feedback effect, driving correlations between impurities. this should be understood as arising from an rkky-like interaction, mediated by impurity textures. we discuss implications considering charge order inside a cuprates and doped cdw materials such as nbse$_2$."
"atomically thin circuits have recently been explored considering applications inside next-generation electronics and optoelectronics and have been demonstrated with two-dimensional lateral heterojunctions. inside order to form true 2d circuitry from the single material, electronic properties must be spatially tunable. here, we report tunable transport behavior which is introduced into single layer tungsten diselenide and tungsten disulfide by focused he$^+$ irradiation. pseudo-metallic behavior is induced by irradiating a materials with the dose of ~1x10$^{16} he^+/cm^2$ to introduce defect states, and subsequent temperature-dependent transport measurements suggest the nearest neighbor hopping mechanism was operative. scanning transmission electron microscopy and electron energy loss spectroscopy reveal that se was sputtered preferentially, and extended percolating networks of edge states form within wse$_2$ at the critical dose of 1x10$^{16} he^+/cm^2$. first-principles calculations confirm a semiconductor-to-metallic transition of wse$_2$ after pore and edge defects were introduced by he$^+$ irradiation. a hopping conduction is utilized to direct-write resistor loaded logic circuits inside wse$_2$ and ws$_2$ with the voltage gain of greater than 5. edge contacted thin film transistors were also fabricated with the high on/off ratio (> 10$^6$), demonstrating potential considering a formation of atomically thin circuits."
"recently we reported an enhanced superconductivity inside restacked monolayer tas_2 nanosheets compared with a bulk tas_2, pointing to a exotic physical properties of low dimensional systems. here we tune a superconducting properties of this system with magnetic field along different directions, where the strong pauli paramagnetic spin-splitting effect was found inside this system. importantly, an unusual enhancement as high as 3.8 times of a upper critical field b_{c2}, as compered with a ginzburg-landau (gl) model and tinkham model, was observed under a inclined external magnetic field. moreover, with a out-of-plane field fixed, we find that a superconducting transition temperature t_c should be enhanced by increasing a in-plane field and forms the dome-shaped phase diagram. an extended gl model considering a special microstructure with wrinkles is proposed to describe a results. a restacked crystal structure without inversion center along with a strong spin-orbit coupling may also play an important role considering our observations."
"we show a non-positivity of a einstein-hilbert action considering conformal flat riemannian metrics. a action vanishes only when a metric was constant flat. this recovers an earlier result of fathizadeh-khalkhali inside a setting of spectral triples on noncommutative four-torus. furthermore, computations of a gradient flow and a scalar curvature of this space based on modular operator are given. we also show a gauss-bonnet theorem considering the parametrized class of non-diagonal metrics on noncommutative two-torus."
"evidence of exoplanets with orbits that are misaligned with a spin of a host star may suggest that not all bound planets were born inside a protoplanetary disk of their current planetary system. observations have shown that free-floating jupiter-mass objects should exceed a number of stars inside our galaxy, implying that capture scenarios may not be so rare. to address this issue, we construct the three-dimensional simulation of the three-body scattering between the free-floating planet and the star accompanied by the jupiter-mass bound planet. we distinguish between three different possible scattering outcomes, where a free-floating planet may get captured after a interaction with a binary, remain unbound, or ""kick-out"" a bound planet and replace it. a simulation is performed considering different masses of a free-floating planets and stars, as well as different impact parameters, inclination angles and idea behind the method velocities. a outcome statistics are used to construct an analytical approximation of a cross section considering capturing the free-floating planet by fitting their dependence on a tested variables. a analytically approximated cross section was used to predict a capture rate considering these kinds of objects, and to approximate that about 1\% of all stars are expected to experience the temporary capture of the free-floating planet during their lifetime. finally, we propose additional physical processes that may increase a capture statistics and whose contribution should be considered inside future simulations."
"inside this note we show that the nontrivial, compact, degenerate or nondegenerate, gradient einstein-type manifold of constant scalar curvature was isometric to a standard sphere with the well defined potential function. moreover, under some geometric assumptions a noncompact case was also treated. inside this case, a main result was that the homogeneous, proper, noncompact, nondegenerate, gradient einstein-type manifold was an einstein manifold."
"we study the class of deep neural networks with networks that form the directed acyclic graph (dag). considering backpropagation defined by gradient descent with adaptive momentum, we show weights converge considering the large class of nonlinear activation functions. a proof generalizes a results of wu et al. (2008) who showed convergence considering the feed forward network with one hidden layer. considering an example of a effectiveness of dag architectures, we describe an example of compression through an autoencoder, and compare against sequential feed-forward networks under several metrics."
"this paper presents an idea behind the method considering autonomous underwater robots to visually detect and identify divers. a proposed idea behind the method enables an autonomous underwater robot to detect multiple divers inside the visual scene and distinguish between them. such methods are useful considering robots to identify the human leader, considering example, inside multi-human/robot teams where only designated individuals are allowed to command or lean the team of robots. initial diver identification was performed with the help of a faster r-cnn algorithm with the region proposal network which produces bounding boxes around a divers' locations. subsequently, the suite of spatial and frequency domain descriptors are extracted from a bounding boxes to create the feature vector. the k-means clustering algorithm, with k set to a number of detected bounding boxes, thereafter identifies a detected divers based on these feature vectors. we evaluate a performance of a proposed idea behind the method on video footage of divers swimming inside front of the mobile robot and demonstrate its accuracy."
"we present an analysis of [oi]63, [oiii]88, [nii]122 and [cii]158 far-infrared (fir) fine-structure line observations obtained with herschel/pacs, considering ~240 local luminous infrared galaxies (lirgs) inside a great observatories all-sky lirg survey (goals). we find pronounced declines -deficits- of line-to-fir-continuum emission considering [nii]122, [oi]63 and [cii]158 as the function of fir color and infrared luminosity surface density, $\sigma_{\rm ir}$. a median electron density of a ionized gas inside lirgs, based on a [nii]122/[nii]205 ratio, was $n_{\rm e}$ = 41 cm$^{-3}$. we find that a dispersion inside a [cii]158 deficit of lirgs was attributed to the varying fractional contribution of photo-dissociation-regions (pdrs) to a observed [cii]158 emission, f([cii]pdr) = [cii]pdr/[cii], which increases from ~60% to ~95% inside a warmest lirgs. a [oi]63/[cii]158pdr ratio was tightly correlated with a pdr gas kinetic temperature inside sources where [oi]63 was not optically-thick or self-absorbed. considering each galaxy, we derive a average pdr hydrogen density, $n_{\rm h}$, and intensity of a interstellar radiation field, inside units of g$_0$, and find g$_0$/$n_{\rm h}$ ratios ~0.1-50 cm$^3$, with ulirgs populating a upper end of a distribution. there was the relation between g$_0$/$n_{\rm h}$ and $\sigma_{\rm ir}$, showing the critical break at $\sigma_{\rm ir}^{\star}$ ~ 5 x 10$^{10}$ lsun/kpc$^2$. below $\sigma_{\rm ir}^{\star}$, g$_0$/$n_{\rm h}$ remains constant, ~0.32 cm$^3$, and variations inside $\sigma_{\rm ir}$ are driven by a number density of star-forming regions within the galaxy, with no change inside their pdr properties. above $\sigma_{\rm ir}^{\star}$, g$_0$/$n_{\rm h}$ increases rapidly with $\sigma_{\rm ir}$, signaling the departure from a typical pdr conditions found inside normal star-forming galaxies towards more intense/harder radiation fields and compact geometries typical of starbursting sources."
"matrix games like prisoner's dilemma have guided research on social dilemmas considering decades. however, they necessarily treat a choice to cooperate or defect as an atomic action. inside real-world social dilemmas these choices are temporally extended. cooperativeness was the property that applies to policies, not elementary actions. we introduce sequential social dilemmas that share a mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. we analyze a dynamics of policies learned by multiple self-interested independent learning agents, each with the help of its own deep q-network, on two markov games we introduce here: 1. the fruit gathering game and 2. the wolfpack hunting game. we characterize how learned behavior inside each domain changes as the function of environmental factors including resource abundance. our experiments show how conflict should emerge from competition over shared resources and shed light on how a sequential nature of real world social dilemmas affects cooperation."
"though suicide was the major public health problem inside a us, machine learning methods are not commonly used to predict an individual's risk of attempting/committing suicide. inside a present work, starting with an anonymized collection of electronic health records considering 522,056 unique, california-resident adolescents, we develop neural network models to predict suicide attempts. we frame a problem as the binary classification problem inside which we use the patient's data from 2006-2009 to predict either a presence (1) or absence (0) of the suicide attempt inside 2010. after addressing issues such as severely imbalanced classes and a variable length of the patient's history, we build neural networks with depths varying from two to eight hidden layers. considering test set observations where we have at least five ed/hospital visits' worth of data on the patient, our depth-4 model achieves the sensitivity of 0.703, specificity of 0.980, and auc of 0.958."
"inside recent years, research has been done on applying recurrent neural networks (rnns) as recommender systems. results have been promising, especially inside a session-based setting where rnns have been shown to outperform state-of-the-art models. inside many of these experiments, a rnn could potentially improve a recommendations by utilizing information about a user's past sessions, inside addition to its own interactions inside a current session. the problem considering session-based recommendation, was how to produce accurate recommendations at a start of the session, before a system has learned much about a user's current interests. we propose the novel idea behind the method that extends the rnn recommender to be able to process a user's recent sessions, inside order to improve recommendations. this was done by with the help of the second rnn to learn from recent sessions, and predict a user's interest inside a current session. by feeding this information to a original rnn, it was able to improve its recommendations. our experiments on two different datasets show that a proposed idea behind the method should significantly improve recommendations throughout a sessions, compared to the single rnn working only on a current session. a proposed model especially improves recommendations at a start of sessions, and was therefore able to deal with a cold start problem within sessions."
"microlensing was the unique tool, capable of detecting a 'cold' planets between 1-10 au from their host stars, and even unbound 'free-floating' planets. this regime has been poorly sampled to date owing to a limitations of alternative planet-finding methods, but the watershed inside discoveries was anticipated inside a near future thanks to a planned microlensing surveys of wfirst-afta and euclid s extended mission. of a many challenges inherent inside these missions, a modeling of microlensing events will be of primary importance, yet was often time consuming, complex and perceived as the daunting barrier to participation inside a field. a large scale of future survey data products will require thorough but efficient modeling software, but unlike other areas of exoplanet research, microlensing currently lacks the publicly-available, well-documented package to conduct this type of analysis. we present first version 1.0 of pylima: python lightcurve identification and microlensing analysis. this software was written inside python and uses existing packages as much as possible, to make it widely accessible. inside this paper, we describe a overall architecture of a software and a core modules considering modeling single-lens events. to verify a performance of this software, we use it to model both real datasets from events published inside a literature and generated test data, produced with the help of pylima s simulation module. results demonstrate that pylima was an efficient tool considering microlensing modeling. we will expand pylima to consider more complex phenomena inside a following papers."
"let $g$ be an $n$-node simple directed planar graph with nonnegative edge weights. we study a fundamental problems of computing (1) the global cut of $g$ with minimum weight and (2) a~cycle of $g$ with minimum weight. a best previously known algorithm considering a former problem, running inside $o(n\log^3 n)$ time, should be obtained from a algorithm of \lacki, nussbaum, sankowski, and wulff-nilsen considering single-source all-sinks maximum flows. a best previously known result considering a latter problem was a $o(n\log^3 n)$-time algorithm of wulff-nilsen. by exploiting duality between a two problems inside planar graphs, we solve both problems inside $o(n\log n\log\log n)$ time using the divide-and-conquer algorithm that finds the shortest non-degenerate cycle. a kernel of our result was an $o(n\log\log n)$-time algorithm considering computing noncrossing shortest paths among nodes well ordered on the common face of the directed plane graph, which was extended from a algorithm of italiano, nussbaum, sankowski, and wulff-nilsen considering an undirected plane graph."
"inside this paper, we propose the novel method to register football broadcast video frames on a static top view model of a playing surface. a proposed method was fully automatic inside contrast to a current state of a art which requires manual initialization of point correspondences between a image and a static model. automatic registration with the help of existing approaches has been difficult due to a lack of sufficient point correspondences. we investigate an alternate idea behind the method exploiting a edge information from a line markings on a field. we formulate a registration problem as the nearest neighbour search over the synthetically generated dictionary of edge map and homography pairs. a synthetic dictionary generation allows us to exhaustively cover the wide variety of camera angles and positions and reduce this problem to the minimal per-frame edge map matching procedure. we show that a per-frame results should be improved inside videos with the help of an optimization framework considering temporal camera stabilization. we demonstrate a efficacy of our idea behind the method by presenting extensive results on the dataset collected from matches of football world cup 2014."
"given the sequential learning algorithm and the target model, sequential machine teaching aims to find a shortest training sequence to drive a learning algorithm to a target model. we present a first principled way to find such shortest training sequences. our key insight was to formulate sequential machine teaching as the time-optimal control problem. this allows us to solve sequential teaching by leveraging key theoretical and computational tools developed over a past 60 years inside a optimal control community. specifically, we study a pontryagin maximum principle, which yields the necessary condition considering optimality of the training sequence. we present analytic, structural, and numerical implications of this idea behind the method on the case study with the least-squares loss function and gradient descent learner. we compute optimal training sequences considering this problem, and although a sequences seem circuitous, we find that they should vastly outperform a best available heuristics considering generating training sequences."
"inside this paper we study nonconvex and nonsmooth optimization problems with semi-algebraic data, where a variables vector was split into several blocks of variables. a problem consists of one smooth function of a entire variables vector and a sum of nonsmooth functions considering each block separately. we analyze an inertial version of a proximal alternating linearized minimization (palm) algorithm and prove its global convergence to the critical point of a objective function at hand. we illustrate our theoretical findings by presenting numerical experiments on blind image deconvolution, on sparse non-negative matrix factorization and on dictionary learning, which demonstrate a viability and effectiveness of a proposed method."
"grouping objects into clusters based on similarities or weights between them was one of a most important problems inside science and engineering. inside this work, by extending message passing algorithms and spectral algorithms proposed considering unweighted community detection problem, we develop the non-parametric method based on statistical physics, by mapping a problem to potts model at a critical temperature of spin glass transition and applying belief propagation to solve a marginals corresponding to a boltzmann distribution. our algorithm was robust to over-fitting and gives the principled way to determine whether there are significant clusters inside a data and how many clusters there are. we apply our method to different clustering tasks and use extensive numerical experiments to illustrate a advantage of our method over existing algorithms. inside a community detection problem inside weighted and directed networks, we show that our algorithm significantly outperforms existing algorithms. inside a clustering problem when a data is generated by mixture models inside a sparse regime we show that our method works to a theoretical limit of detectability and gives accuracy very close to that of a optimal bayesian inference. inside a semi-supervised clustering problem, our method only needs several labels to work perfectly inside classic datasets. finally, we further develop thouless-anderson-palmer equations which reduce heavily a computation complexity inside dense-networks but gives almost a same performance as belief propagation."
"there has been the long standing interest inside understanding `social influence' both inside social sciences and inside computational linguistics. inside this paper, we present the novel idea behind the method to study and measure interpersonal influence inside daily interactions. motivated by a basic principles of influence, we attempt to identify indicative linguistic features of a posts inside an online knitting community. we present a scheme used to operationalize and label a posts with indicator features. experiments with a identified features show an improvement inside a classification accuracy of influence by 3.15%. our results illustrate a important correlation between a characteristics of a language and its potential to influence others."
"we prove that finite perimeter subsets of $\mathbb{r}^{n+1}$ with small isoperimetric deficit have boundary hausdorff-close to the sphere up to the subset of small measure. we also refine this closeness under some additional the priori integral curvature bounds. as an application, we answer the question raised by b. colbois concerning a almost extremal hypersurfaces considering chavel's inequality."
"this paper presents, considering a first time, the method considering learning in-contact tasks from the teleoperated demonstration with the hydraulic manipulator. due to a use of extremely powerful hydraulic manipulator, the force-reflected bilateral teleoperation was a most reasonable method of giving the human demonstration. an advanced subsystem-dynamic-based control design framework, virtual decomposition control (vdc), was used to design the stability-guaranteed controller considering a teleoperation system, while taking into account a full nonlinear dynamics of a master and slave manipulators. a use of fragile force/ torque sensor at a tip of a hydraulic slave manipulator was avoided by estimating a contact forces from a manipulator actuators' chamber pressures. inside a proposed learning method, it was observed that the surface-sliding tool has the friction-dependent range of directions (between a actual direction of motion and a contact force) from which a manipulator should apply force to produce a sliding motion. by this intuition, an intersection of these ranges should be taken over the motion to robustly find the desired direction considering a motion from one or more demonstrations. a compliant axes required to reproduce a motion should be found by assuming that all motions outside a desired direction was caused by a environment, signalling a need considering compliance. finally, a learning method was incorporated to the novel vdc-based impedance control method to learn compliant behaviour from teleoperated human demonstrations. experiments with 2-dof hydraulic manipulator with the 475kg payload demonstrate a suitability and effectiveness of a proposed method to perform learning from demonstration (lfd) with heavy-duty hydraulic manipulators."
"twitter was recently being used during crises to communicate with officials and provide rescue and relief operation inside real time. a geographical location information of a event, as well as users, are vitally important inside such scenarios. a identification of geographic location was one of a challenging tasks as a location information fields, such as user location and place name of tweets are not reliable. a extraction of location information from tweet text was difficult as it contains the lot of non-standard english, grammatical errors, spelling mistakes, non-standard abbreviations, and so on. this research aims to extract location words used inside a tweet with the help of the convolutional neural network (cnn) based model. we achieved a exact matching score of 0.929, hamming loss of 0.002, and $f_1$-score of 0.96 considering a tweets related to a earthquake. our model is able to extract even three- to four-word long location references which was also evident from a exact matching score of over 92\%. a findings of this paper should aid inside early event localization, emergency situations, real-time road traffic management, localized advertisement, and inside various location-based services."
"inside this paper we show that reporting the single performance score was insufficient to compare non-deterministic approaches. we demonstrate considering common sequence tagging tasks that a seed value considering a random number generator should result inside statistically significant (p < 10^-4) differences considering state-of-the-art systems. considering two recent systems considering ner, we observe an absolute difference of one percentage point f1-score depending on a selected seed value, making these systems perceived either as state-of-the-art or mediocre. instead of publishing and reporting single performance scores, we propose to compare score distributions based on multiple executions. based on a evaluation of 50.000 lstm-networks considering five sequence tagging tasks, we present network architectures that produce both superior performance as well as are more stable with respect to a remaining hyperparameters."
"a beautiful quartic diophantine equation $a^4+hb^4=c^4+hd^4$, where $h$ was the fixed arbitrary positive integer, has been studied by some mathematicians considering many years. although choudhry, gerardin and piezas presented solutions of this equation considering many values of $h$, a solutions were not known considering arbitrary positive integer values of $h$. inside the separate paper (see a arxiv), a authors completely solved a equation considering arbitrary values of $h$, and worked out many examples considering different values of $h$, inside particular considering a values which has not already been given the solution. our method, give rise to infinitely many solutions and also infinitely many parametric solutions considering a equation considering arbitrary rational values of $h$. inside a present paper, we use a above solutions as well as the simple idea to show that how some numbers should be written as a sums of two, three, four, five, or more biquadrates inside two different ways. inside particular we give examples considering a sums of $2$, $3$, $\cdots$, and $10$, biquadrates expressed inside two different ways."
"inside this paper, we propose an encoder-decoder convolutional neural network (cnn) architecture considering estimating camera pose (orientation and location) from the single rgb-image. a architecture has the hourglass shape consisting of the chain of convolution and up-convolution layers followed by the regression part. a up-convolution layers are introduced to preserve a fine-grained information of a input image. following a common practice, we train our model inside end-to-end manner utilizing transfer learning from large scale classification data. a experiments demonstrate a performance of a idea behind the method on data exhibiting different lighting conditions, reflections, and motion blur. a results indicate the clear improvement over a previous state-of-the-art even when compared to methods that utilize sequence of test frames instead of the single frame."
"we have previously derived a effect of soft graviton modes on a quantum state of de sitter with the help of spontaneously broken asymptotic symmetries. inside a present paper we reinterpret this effect inside terms of particle production and relate a quantum states with and without soft modes by means of bogoliubov transformations. this also enables us to address a much discussed issues regarding a observability of infrared effects inside de sitter from the new perspective. while it was commonly agreed that infrared effects are not visible to the single sub-horizon observer at late times, we argue that a question was less trivial considering the {\it patient observer} who has lived long enough to have the record of a state before a soft mode is created. though classically there was no obstruction to measuring this effect locally, we give several indications that quantum mechanical uncertainties may censor a effect. we then apply our methods to find the non-perturbative description of a quantum state pertaining to a page time of de sitter, and derive with these new methods a probability distribution considering a local quantum states of de sitter and slow-roll inflation inside a presence of long modes. finally, we use this to formulate the precise criterion considering a existence of eternal inflation inside general classes of slow-roll inflation."
"a multivariate linear regression model was an important tool considering investigating relationships between several response variables and several predictor variables. a primary interest was inside inference about a unknown regression coefficient matrix. we propose multivariate bootstrap techniques as the means considering making inferences about a unknown regression coefficient matrix. these bootstrapping techniques are extensions of those developed inside freedman (1981), which are only appropriate considering univariate responses. extensions to a multivariate linear regression model are made without proof. we formalize this extension and prove its validity. the real data example and two simulated data examples which offer some finite sample verification of our theoretical results are provided."
"inside many experiments inside a life sciences, several endpoints are recorded per subject. a analysis of such multivariate data was usually based on manova models assuming multivariate normality and covariance homogeneity. these assumptions, however, are often not met inside practice. furthermore, test statistics should be invariant under scale transformations of a data, since a endpoints may be measured on different scales. inside a context of high-dimensional data, srivastava and kubokawa (2013) proposed such the test statistic considering the specific one-way model, which, however, relies on a assumption of the common non-singular covariance matrix. we modify and extend this test statistic to factorial manova designs, incorporating general heteroscedastic models. inside particular, our only distributional assumption was a existence of a group-wise covariance matrices, which may even be singular. we base inference on quantiles of resampling distributions, and derive confidence regions and ellipsoids based on these quantiles. inside the simulation study, we extensively analyze a behavior of these procedures. finally, a methods are applied to the data set containing information on a 2016 presidential elections inside a usa with unequal and singular empirical covariance matrices."
"inside this paper, we focus on applications inside machine learning, optimization, and control that call considering a resilient selection of the few elements, e.g. features, sensors, or leaders, against the number of adversarial denial-of-service attacks or failures. inside general, such resilient optimization problems are hard, and cannot be solved exactly inside polynomial time, even though they often involve objective functions that are monotone and submodular. notwithstanding, inside this paper we provide a first scalable, curvature-dependent algorithm considering their approximate solution, that was valid considering any number of attacks or failures, and which, considering functions with low curvature, guarantees superior approximation performance. notably, a curvature has been known to tighten approximations considering several non-resilient maximization problems, yet its effect on resilient maximization had hitherto been unknown. we complement our theoretical analyses with supporting empirical evaluations."
"our usage of language was not solely reliant on cognition but was arguably determined by myriad external factors leading to the global variability of linguistic patterns. this issue, which lies at a core of sociolinguistics and was backed by many small-scale studies on face-to-face communication, was addressed here by constructing the dataset combining a largest french twitter corpus to date with detailed socioeconomic maps obtained from national census inside france. we show how key linguistic variables measured inside individual twitter streams depend on factors like socioeconomic status, location, time, and a social network of individuals. we found that (i) people of higher socioeconomic status, active to the greater degree during a daytime, use the more standard language; (ii) a southern part of a country was more prone to use more standard language than a northern one, while locally a used variety or dialect was determined by a spatial distribution of socioeconomic status; and (iii) individuals connected inside a social network are closer linguistically than disconnected ones, even after a effects of status homophily have been removed. our results inform sociolinguistic theory and may inspire novel learning methods considering a inference of socioeconomic status of people from a way they tweet."
we prove that indecomposable $\sigma$-pure-injective modules considering the string algebra are string or band modules. a key step inside our proof was the splitting result considering infinite-dimensional linear relations.
"inside spite of anderson's theorem, disorder was known to affect superconductivity inside conventional s-wave superconductors. inside most superconductors, a degree of disorder was fixed during sample preparation. here we report measurements of a superconducting properties of a two-dimensional gas that forms at a interface between laalo$_3$ (lao) and srtio$_3$ (sto) inside a (111) crystal orientation, the system that permits \emph{in situ} tuning of carrier density and disorder by means of the back gate voltage $v_g$. like a (001) oriented lao/sto interface, superconductivity at a (111) lao/sto interface should be tuned by $v_g$. inside contrast to a (001) interface, superconductivity inside these (111) samples was anisotropic, being different along different interface crystal directions, consistent with a strong anisotropy already observed other transport properties at a (111) lao/sto interface. inside addition, we find that a (111) interface samples ""remember"" a backgate voltage $v_f$ at which they are cooled at temperatures near a superconducting transition temperature $t_c$, even if $v_g$ was subsequently changed at lower temperatures. a low energy scale and other characteristics of this memory effect ($<1$ k) distinguish it from charge-trapping effects previously observed inside (001) interface samples."
"stance classification aims to identify, considering the particular issue under discussion, whether a speaker or author of the conversational turn has pro (favor) or con (against) stance on a issue. detecting stance inside tweets was the new task proposed considering semeval-2016 task6, involving predicting stance considering the dataset of tweets on a topics of abortion, atheism, climate change, feminism and hillary clinton. given a small size of a dataset, our team created our own topic-specific training corpus by developing the set of high precision hashtags considering each topic that were used to query a twitter api, with a aim of developing the large training corpus without additional human labeling of tweets considering stance. a hashtags selected considering each topic were predicted to be stance-bearing on their own. experimental results demonstrate good performance considering our features considering opinion-target pairs based on generalizing dependency features with the help of sentiment lexicons."
"a aim of this paper was to compare different sources of stochasticity inside a solar system. more precisely we study a importance of a long term influence of asteroids on a chaotic dynamics of a solar system. we show that a effects of asteroids on planets was similar to the white noise process, when those effects are considered on the time scale much larger than a correlation time $\tau_{\varphi}\simeq10^{4}$ yr of asteroid trajectories. we compute a time scale $\tau_{e}$ after which a effects of a stochastic evolution of a asteroids lead to the loss of information considering a initial conditions of a perturbed laplace\textendash lagrange secular dynamics. a order of magnitude of this time scale was precisely determined by theoretical argument. this time scale should be compared with a lyapunov time $\tau_{i}$ of a solar system without asteroids (intrinsic chaos). we conclude that $\tau_{i}\simeq10\, \text{myr} \ll \tau_{e} \simeq10^{4}\, \text{myr}$, showing that a external sources of chaoticity arise as the small perturbation inside a stochastic secular behavior of a solar system, rather due to intrinsic chaos."
"enter a robotrix, an extremely photorealistic indoor dataset designed to enable a application of deep learning techniques to the wide variety of robotic vision problems. a robotrix consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects inside the visually realistic manner inside that simulated world. photorealistic scenes and robots are rendered by unreal engine into the virtual reality headset which captures gaze so that the human operator should move a robot and use controllers considering a robotic hands; scene information was dumped on the per-frame basis so that it should be reproduced offline to generate raw data and ground truth labels. by taking this approach, we were able to generate the dataset of 38 semantic classes totaling 8m stills recorded at +60 frames per second with full hd resolution. considering each frame, rgb-d and 3d information was provided with full annotations inside both spaces. thanks to a high quality and quantity of both raw information and annotations, a robotrix will serve as the new milestone considering investigating 2d and 3d robotic vision tasks with large-scale data-driven techniques."
"time-spatial data plays the crucial role considering different fields such as traffic management. these data should be collected using devices such as surveillance sensors or tracking systems. however, how to efficiently an- alyze and visualize these data to capture essential embedded pattern information was becoming the big challenge today. classic visualization ap- proaches focus on revealing 2d and 3d spatial information and modeling statistical test. those methods would easily fail when data become mas- sive. recent attempts concern on how to simply cluster data and perform prediction with time-oriented information. however, those approaches could still be further enhanced as they also have limitations considering han- dling massive clusters and labels. inside this paper, we propose the visualiza- tion methodology considering mobility data with the help of artificial neural net techniques. this method aggregates three main parts that are back-end data model, neural net algorithm including clustering method self-organizing map (som) and prediction idea behind the method recurrent neural net (rnn) considering ex- tracting a features and lastly the solid front-end that displays a results to users with an interactive system. som was able to cluster a visiting patterns and detect a abnormal pattern. rnn should perform a predic- tion considering time series analysis with the help of its dynamic architecture. furthermore, an interactive system will enable user to interpret a result with graph- ics, animation and 3d model considering the close-loop feedback. this method should be particularly applied inside two tasks that commercial-based promotion and abnormal traffic patterns detection."
"we study multi-label classification (mlc) with three important real-world issues: online updating, label space dimensional reduction (lsdr), and cost-sensitivity. current mlc algorithms have not been designed to address these three issues simultaneously. inside this paper, we propose the novel algorithm, cost-sensitive dynamic principal projection (cs-dpp) that resolves all three issues. a foundation of cs-dpp was an online lsdr framework derived from the leading lsdr algorithm. inside particular, cs-dpp was equipped with an efficient online dimension reducer motivated by matrix stochastic gradient, and establishes its theoretical backbone when coupled with the carefully-designed online regression learner. inside addition, cs-dpp embeds a cost information into label weights to achieve cost-sensitivity along with theoretical guarantees. experimental results verify that cs-dpp achieves better practical performance than current mlc algorithms across different evaluation criteria, and demonstrate a importance of resolving a three issues simultaneously."
"generative adversarial networks (gans) should produce images of surprising complexity and realism, but are generally modeled to sample from the single latent source ignoring a explicit spatial interaction between multiple entities that could be present inside the scene. capturing such complex interactions between different objects inside a world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation was the challenging problem. inside this work, we propose to model object composition inside the gan framework as the self-consistent composition-decomposition network. our model was conditioned on a object images from their marginal distributions to generate the realistic image from their joint distribution by explicitly learning a possible interactions. we evaluate our model through qualitative experiments and user evaluations inside both a scenarios when either paired or unpaired examples considering a individual object images and a joint scenes are given during training. our results reveal that a learned model captures potential interactions between a two object domains given as input to output new instances of composed scene at test time inside the reasonable fashion."
"vaes (variational autoencoders) have proved to be powerful inside a context of density modeling and have been used inside the variety of contexts considering creative purposes. inside many settings, a data we model possesses continuous attributes that we would like to take into account at generation time. we propose inside this paper glsr-vae, the geodesic latent space regularization considering a variational autoencoder architecture and its generalizations which allows the fine control on a embedding of a data into a latent space. when augmenting a vae loss with this regularization, changes inside a learned latent space reflects changes of a attributes of a data. this deeper understanding of a vae latent space structure offers a possibility to modulate a attributes of a generated data inside the continuous way. we demonstrate its efficiency on the monophonic music generation task where we manage to generate variations of discrete sequences inside an intended and playful way."
"recent experiments demonstrate a importance of substrate curvature considering actively forced fluid dynamics. yet, a covariant formulation and analysis of continuum models considering non-equilibrium flows on curved surfaces still poses theoretical challenges. here, we introduce and study the generalized covariant navier-stokes model considering fluid flows driven by active stresses inside non-planar geometries. a analytical tractability of a theory was demonstrated through exact stationary solutions considering a case of the spherical bubble geometry. direct numerical simulations reveal the curvature-induced transition from the burst phase to an anomalous turbulent phase that differs distinctly from externally forced classical 2d kolmogorov turbulence. this new type of active turbulence was characterized by a self-assembly of finite-size vortices into linked chains of anti-ferromagnetic order, which percolate through a entire fluid domain, forming an active dynamic network. a coherent motion of a vortex chain network provides an efficient mechanism considering upward energy transfer from smaller to larger scales, presenting an alternative to a conventional energy cascade inside classical 2d turbulence."
"inside this tutorial, we detailed simple controllers considering autonomous parking and path following considering self-driving cars and provided practical methods considering curvature computation."
"we discuss possible approaches to a problem of a uru2si2 ""hidden order"" (ho) which remains unsolved after tremendous efforts of researches. suppose there was no spatial symmetry breaking at a ho transition temperature and solely a time-reversal symmetry breaking emerges owing to some sort of magnetic order. as the result of its 4/mmm symmetry, each uranium atom was the three-dimensional magnetic vortex; its intra-atomic magnetization m(r) was intrinsically non-collinear, so that its dipole, quadrupole and toroidal moments vanish, thus making a vortex ""hidden"". a first non-zero magnetic multipole of a uranium vortex was a toroidal quadrupole. inside a unit cell, two uranium vortices should have either a same or opposite signs of m(r); this corresponds to either ferro-vortex or antiferro-vortex structures with i4/mmm or p_i4/mmm magnetic space groups, respectively. our first-principles calculations suggest that a vortex magnetic order of uru2si2 was rather strong: a total absolute magnetization |m(r)| was about 0.9 mu_b per u atom, detectable by neutron scattering inside spite of a unusual formfactor. a ferro-vortex and antiferro-vortex phases have almost a same energy and they are energetically favorable compared to a non-magnetic phase."
"standard bayesian analyses should be difficult to perform when a full likelihood, and consequently a full posterior distribution, was too complex and difficult to specify or if robustness with respect to data or to model misspecifications was required. inside these situations, we suggest to resort to the posterior distribution considering a parameter of interest based on proper scoring rules. scoring rules are loss functions designed to measure a quality of the probability distribution considering the random variable, given its observed value. important examples are a tsallis score and a hyvärinen score, which allow us to deal with model misspecifications or with complex models. also a full and a composite likelihoods are both special instances of scoring rules. a aim of this paper was twofold. firstly, we discuss a use of scoring rules inside a bayes formula inside order to compute the posterior distribution, named sr-posterior distribution, and we derive its asymptotic normality. secondly, we propose the procedure considering building default priors considering a unknown parameter of interest that should be used to update a information provided by a scoring rule inside a sr-posterior distribution. inside particular, the reference prior was obtained by maximizing a average $\alpha-$divergence from a sr-posterior distribution. considering $0 \leq |\alpha|<1$, a result was the jeffreys-type prior that was proportional to a square root of a determinant of a godambe information matrix associated to a scoring rule. some examples are discussed."
"a velocity distribution of dark matter near a earth was important considering an accurate analysis of a signals inside terrestrial detectors. this distribution was typically extracted from numerical simulations. here we address a possibility of deriving a velocity distribution function analytically. we derive the differential equation which was the function of radius and a radial component of a velocity. under various assumptions this should be solved, and we compare a solution with a results from controlled numerical simulations. our findings complement a previously derived tangential velocity distribution. we hereby demonstrate that a entire distribution function, below 0.7 v_esc, should be derived analytically considering spherical and equilibrated dark matter structures."
"we propose and analyze the new family of algorithms considering training neural networks with relu activations. our algorithms are based on a technique of alternating minimization: estimating a activation patterns of each relu considering all given samples, interleaved with weight updates using the least-squares step. a main focus of our paper are 1-hidden layer networks with $k$ hidden neurons and relu activation. we show that under standard distributional assumptions on a $d-$dimensional input data, our algorithm provably recovers a true `ground truth' parameters inside the linearly convergent fashion. this holds as long as a weights are sufficiently well initialized; furthermore, our method requires only $n=\widetilde{o}(dk^2)$ samples. we also analyze a special case of 1-hidden layer networks with skipped connections, commonly used inside resnet-type architectures, and propose the novel initialization strategy considering a same. considering relu based resnet type networks, we provide a first linear convergence guarantee with an end-to-end algorithm. we also extend this framework to deeper networks and empirically demonstrate its convergence to the global minimum."
"we investigate robotic assistants considering dressing that should anticipate a motion of a person who was being helped. to this end, we use reinforcement learning to create models of human behavior during assistance with dressing. to explore this kind of interaction, we assume that a robot presents an open sleeve of the hospital gown to the person, and that a person moves their arm into a sleeve. a controller that models a person's behavior was given a position of a end of a sleeve and information about contact between a person's hand and a fabric of a gown. we simulate this system with the human torso model that has realistic joint ranges, the simple robot gripper, and the physics-based cloth model considering a gown. through reinforcement learning (specifically a trpo algorithm) a system creates the model of human behavior that was capable of placing a arm into a sleeve. we aim to model what humans are capable of doing, rather than what they typically do. we demonstrate successfully trained human behaviors considering three robot-assisted dressing strategies: 1) a robot gripper holds a sleeve motionless, 2) a gripper moves a sleeve linearly towards a person from a front, and 3) a gripper moves a sleeve linearly from a side."
"we introduce an inference technique to produce discriminative context-aware image captions (captions that describe differences between images or visual concepts) with the help of only generic context-agnostic training data (captions that describe the concept or an image inside isolation). considering example, given images and captions of ""siamese cat"" and ""tiger cat"", we generate language that describes a ""siamese cat"" inside the way that distinguishes it from ""tiger cat"". our key novelty was that we show how to do joint inference over the language model that was context-agnostic and the listener which distinguishes closely-related concepts. we first apply our technique to the justification task, namely to describe why an image contains the particular fine-grained category as opposed to another closely-related category of a cub-200-2011 dataset. we then study discriminative image captioning to generate language that uniquely refers to one of two semantically-similar images inside a coco dataset. evaluations with discriminative ground truth considering justification and human studies considering discriminative image captioning reveal that our idea behind the method outperforms baseline generative and speaker-listener approaches considering discrimination."
"objective- heart rate monitoring with the help of wrist type photoplethysmographic (ppg) signals was getting popularity because of construction simplicity and low cost of wearable devices. a task becomes very difficult due to a presence of various motion artifacts. a objective was to develop algorithms to reduce a effect of motion artifacts and thus obtain accurate heart rate estimation. methods- proposed heart rate approximation scheme utilizes both time and frequency domain analyses. unlike conventional single stage adaptive filter, multi-stage cascaded adaptive filtering was introduced by with the help of three channel accelerometer data to reduce a effect of motion artifacts. both recursive least squares (rls) and least mean squares (lms) adaptive filters are tested. moreover, singular spectrum analysis (ssa) was employed to obtain improved spectral peak tracking. a outputs from a filter block and ssa operation are logically combined and used considering spectral domain heart rate estimation. finally, the tracking algorithm was incorporated considering neighbouring estimates. results- a proposed method provides an average absolute error of 1.16 beat per minute (bpm) with the standard deviation of 1.74 bpm while tested on publicly available database consisting of recordings from 12 subjects during physical activities. conclusion- it was found that a proposed method provides consistently better heart rate approximation performance inside comparison to that recently reported by troika, joss and spectrap methods. significance- a proposed method offers very low approximation error and the smooth heart rate tracking with simple algorithmic idea behind the method and thus feasible considering implementing inside wearable devices to monitor heart rate considering fitness and clinical purpose."
"pressure-induced superconductivity and structural phase transitions inside phosphorous (p) are studied by resistivity measurements under pressures up to 170 gpa and fully $ab-initio$ crystal structure and superconductivity calculations up to 350 gpa. two distinct superconducting transition temperature (t$_{c}$) vs. pressure ($p$) trends at low pressure have been reported more than 30 years ago, and considering a first time we are able to reproduce them and devise the consistent explanation founded on thermodynamically metastable phases of black-phosphorous. our experimental and theoretical results form the single, consistent picture which not only provides the clear understanding of elemental p under pressure but also sheds light on a long-standing and unsolved $anomalous$ superconductivity trend. moreover, at higher pressures we predict the similar scenario of multiple metastable structures which coexist beyond their thermodynamical stability range. metastable phases of p experimentally accessible at pressures above 240 gpa should exhibit t$_{c}$'s as high as 15 k, i.e. three times larger than a predicted value considering a ground-state crystal structure. we observe that all a metastable structures systematically exhibit larger transition temperatures than a ground-state ones, indicating that a exploration of metastable phases represents the promising route to design materials with improved superconducting properties."
"the new low-dimensional parameterization based on principal component analysis (pca) and convolutional neural networks (cnn) was developed to represent complex geological models. a cnn-pca method was inspired by recent developments inside computer vision with the help of deep learning. cnn-pca should be viewed as the generalization of an existing optimization-based pca (o-pca) method. both cnn-pca and o-pca entail post-processing the pca model to better honor complex geological features. inside cnn-pca, rather than use the histogram-based regularization as inside o-pca, the new regularization involving the set of metrics considering multipoint statistics was introduced. a metrics are based on summary statistics of a nonlinear filter responses of geological models to the pre-trained deep cnn. inside addition, inside a cnn-pca formulation presented here, the convolutional neural network was trained as an explicit transform function that should post-process pca models quickly. cnn-pca was shown to provide both unconditional and conditional realizations that honor a geological features present inside reference sgems geostatistical realizations considering the binary channelized system. flow statistics obtained through simulation of random cnn-pca models closely match results considering random sgems models considering the demanding case inside which o-pca models lead to significant discrepancies. results considering history matching are also presented. inside this assessment cnn-pca was applied with derivative-free optimization, and the subspace randomized maximum likelihood method was used to provide multiple posterior models. data assimilation and significant uncertainty reduction are achieved considering existing wells, and physically reasonable predictions are also obtained considering new wells. finally, a cnn-pca method was extended to the more complex non-stationary bimodal deltaic fan system, and was shown to provide high-quality realizations considering this challenging example."
"the novel solution was obtained to solve a rigid 3d registration problem, motivated by previous eigen-decomposition approaches. different from existing solvers, a proposed algorithm does not require sophisticated matrix operations e.g. singular value decomposition or eigenvalue decomposition. instead, a optimal eigenvector of a point cross-covariance matrix should be computed within several iterations. it was also proven that a optimal rotation matrix should be directly computed considering cases without need of quaternion. a simple framework provides very easy idea behind the method of integer-implementation on embedded platforms. simulations on noise-corrupted point clouds have verified a robustness and computation speed of a proposed method. a final results indicate that a proposed algorithm was accurate, robust and owns over $60\% \sim 80\%$ less computation time than representatives. it has also been applied to real-world applications considering faster relative robotic navigation."
"this paper was concerned with finite sample approximations to a supremum of the non-degenerate $u$-process of the general order indexed by the function class. we are primarily interested inside situations where a function class as well as a underlying distribution change with a sample size, and a $u$-process itself was not weakly convergent as the process. such situations arise inside the variety of modern statistical problems. we first consider gaussian approximations, namely, approximate a $u$-process supremum by a supremum of the gaussian process, and derive coupling and kolmogorov distance bounds. such gaussian approximations are, however, not often directly applicable inside statistical problems since a covariance function of a approximating gaussian process was unknown. this motivates us to study bootstrap-type approximations to a $u$-process supremum. we propose the novel jackknife multiplier bootstrap (jmb) tailored to a $u$-process, and derive coupling and kolmogorov distance bounds considering a proposed jmb method. all these results are non-asymptotic, and established under fairly general conditions on function classes and underlying distributions. key technical tools inside a proofs are new local maximal inequalities considering $u$-processes, which may be useful inside other problems. we also discuss applications of a general approximation results to testing considering qualitative features of nonparametric functions based on generalized local $u$-processes."
"wikipedia entity pages are the valuable source of information considering direct consumption and considering knowledge-base construction, update and maintenance. facts inside these entity pages are typically supported by references. recent studies show that as much as 20\% of a references are from online news sources. however, many entity pages are incomplete even if relevant information was already available inside existing news articles. even considering a already present references, there was often the delay between a news article publication time and a reference time. inside this work, we therefore look at wikipedia through a lens of news and propose the novel news-article suggestion task to improve news coverage inside wikipedia, and reduce a lag of newsworthy references. our work finds direct application, as the precursor, to wikipedia page generation and knowledge-base acceleration tasks that rely on relevant and high quality input sources. we propose the two-stage supervised idea behind the method considering suggesting news articles to entity pages considering the given state of wikipedia. first, we suggest news articles to wikipedia entities (article-entity placement) relying on the rich set of features which take into account a \emph{salience} and \emph{relative authority} of entities, and a \emph{novelty} of news articles to entity pages. second, we determine a exact section inside a entity page considering a input article (article-section placement) guided by class-based section templates. we perform an extensive evaluation of our idea behind the method based on ground-truth data that was extracted from external references inside wikipedia. we achieve the high precision value of up to 93\% inside a \emph{article-entity} suggestion stage and upto 84\% considering a \emph{article-section placement}. finally, we compare our idea behind the method against competitive baselines and show significant improvements."
"the key phase inside a bridge design process was a selection of a structural system. due to budget and time constraints, engineers typically rely on engineering judgment and prior experience when selecting the structural system, often considering the limited range of design alternatives. a objective of this study is to explore a suitability of supervised machine learning as the preliminary design aid that provides guidance to engineers with regards to a statistically optimal bridge type to choose, ultimately improving a likelihood of optimized design, design standardization, and reduced maintenance costs. inside order to devise this supervised learning system, data considering over 600,000 bridges from a national bridge inventory database were analyzed. key attributes considering determining a bridge structure type were identified through three feature selection techniques. potentially useful attributes like seismic intensity and historic data on a cost of materials (steel and concrete) were then added from a us geological survey (usgs) database and engineering news record. decision tree, bayes network and support vector machines were used considering predicting a bridge design type. due to state-to-state variations inside material availability, material costs, and design codes, supervised learning models based on a complete data set did not yield favorable results. supervised learning models were then trained and tested with the help of 10-fold cross validation on data considering each state. inclusion of seismic data improved a model performance noticeably. a data is then resampled to reduce a bias of a models towards more common design types, and a supervised learning models thus constructed showed further improvements inside performance. a average recall and precision considering a state models is 88.6% and 88.0% with the help of decision trees, 84.0% and 83.7% with the help of bayesian networks, and 80.8% and 75.6% with the help of svm."
"flat-fading channel models are usually invoked considering analyzing a performance of massive spatial modulation multiple-input multiple-output (sm-mimo) systems. however, inside a context of broadband sm transmission, a severe inter-symbol-interference (isi) caused by a frequency-selective fading channels should not be ignored, which leads to very detrimental effects on a achievable system performance, especially considering single-carrier sm (sc-sm) transmission schemes. to a best of a author's knowledge, none of a previous researchers have been able to provide the thorough analysis on a achievable spectral efficiency (se) of a massive sc-sm mimo uplink transmission. inside this context, a uplink se of single-cell massive sc-sm mimo system was analyzed, and the tight closed-form lower bound was proposed to quantify a se when a base station (bs) uses maximum ratio (mr) combining considering multi-user detection. a impacts of imperfect channel approximation and transmit correlation are all considered. monte carlo simulations are performed to verify a tightness of our proposed se lower bound. both a theoretical analysis and simulation results show that a se of uplink single-cell massive sc-sm mimo system has a potential to outperform a uplink se achieved by single-antenna ues."
"a use of features extracted with the help of the deep convolutional neural network (cnn) combined with the writer-dependent (wd) svm classifier resulted inside significant improvement inside performance of handwritten signature verification (hsv) when compared to a previous state-of-the-art methods. inside this work it was investigated whether a use of these cnn features provide good results inside the writer-independent (wi) hsv context, based on a dichotomy transformation combined with a use of an svm writer-independent classifier. a experiments performed inside a brazilian and gpds datasets show that (i) a proposed idea behind the method outperformed other wi-hsv methods from a literature, (ii) inside a global threshold scenario, a proposed idea behind the method is able to outperform a writer-dependent method with cnn features inside a brazilian dataset, (iii) inside an user threshold scenario, a results are similar to those obtained by a writer-dependent method with cnn features."
we introduce and motivate a study of hypergraphical clustering games of mis-coordination. considering two specific variants we prove a existence of the pure nash equilibrium and provide bounds on a price of anarchy as the function of a cardinality of a action set and a size of a hyperedges.
"determining significant prognostic biomarkers was of increasing importance inside many areas of medicine. inside order to translate the continuous biomarker into the clinical decision, it was often necessary to determine cut-points. there was so far no standard method to aid evaluate how many cut-points are optimal considering the given feature inside the survival analysis setting. moreover, most existing methods are univariate, thus not well suited considering high-dimensional frameworks. this paper introduces the prognostic method called binacox to deal with a problem of detecting multiple cut-points per features inside the multivariate setting where the large number of continuous features are available. it was based on a cox model and combines one-hot encodings with a binarsity penalty. this penalty uses total-variation regularization together with an extra linear constraint to avoid collinearity between a one-hot encodings and enable feature selection. the non-asymptotic oracle inequality was established. a statistical performance of a method was then examined on an extensive monte carlo simulation study, and finally illustrated on three publicly available genetic cancer datasets with high-dimensional features. on this datasets, our proposed methodology significantly outperforms a state-of-the-art survival models regarding risk prediction inside terms of c-index, with the computing time orders of magnitude faster. inside addition, it provides powerful interpretability by automatically pinpointing significant cut-points on relevant features from the clinical point of view."
"applied pressure drives a heavy-fermion antiferromagnet cerhin$_{5}$ towards the quantum critical point that becomes hidden by the dome of unconventional superconductivity. magnetic fields suppress this superconducting dome, unveiling a quantum phase transition of local character. here, we show that $5\%$ magnetic substitution at a ce site inside cerhin$_{5}$, either by nd or gd, induces the zero-field magnetic instability in a superconducting state. this magnetic state not only should have the different ordering vector than a high-field local-moment magnetic state, but it also competes with a latter, suggesting that the spin-density-wave phase was stabilized inside zero field by nd and gd impurities - similarly to a case of ce$_{0.95}$nd$_{0.05}$coin$_{5}$. supported by model calculations, we attribute this spin-density wave instability to the magnetic-impurity driven condensation of a spin excitons that form in a unconventional superconducting state."
"recent studies found that inside voxel-based neuroimage analysis, detecting and differentiating ""procedural bias"" that are introduced during a preprocessing steps from lesion features, not only should aid boost accuracy but also should improve interpretability. to a best of our knowledge, gsplit lbi was a first model proposed inside a literature to simultaneously capture both procedural bias and lesion features. despite a fact that it should improve prediction power by leveraging a procedural bias, it may select spurious features due to a multicollinearity inside high dimensional space. moreover, it does not take into account a heterogeneity of these two types of features. inside fact, a procedural bias and lesion features differ inside terms of volumetric change and spatial correlation pattern. to address these issues, we propose the ""two-groups"" empirical-bayes method called ""fdr-hs"" (false-discovery-rate heterogenous smoothing). such method was able to not only avoid multicollinearity, but also exploit a heterogenous spatial patterns of features. inside addition, it enjoys a simplicity inside implementation by introducing hidden variables, which turns a problem into the convex optimization scheme and should be solved efficiently by a expectation-maximum (em) algorithm. empirical experiments have been evaluated on a alzheimer's disease neuroimage initiative (adni) database. a advantage of a proposed model was verified by improved interpretability and prediction power with the help of selected features by fdr-hs."
"there was the plenty of research going on inside field of robotics. one of a most important task was dynamic approximation of response during motion. one of a main applications of this research topics was a task of pouring, which was performed daily and was commonly used while cooking. we present an idea behind the method to approximate response to the sequence of manipulation actions. we are experimenting with pouring motion and a response was a change of a amount of water inside a pouring cup. a pouring motion was represented by rotation angle and a amount of water was represented by its weight. we are with the help of recurrent neural networks considering building a neural network model to train on sequences which represents 1307 trails of pouring. a model gives great results on unseen test data which does not too different with training data inside terms of dimensions of a cup used considering pouring and receiving. a loss obtained with this test data was 4.5920. a model does not give good results on generalization experiments when we provide the test set which has dimensions of a cup very different from those inside training data."
"blind source separation was the common processing tool to analyse a constitution of pixels of hyperspectral images. such methods usually suppose that pure pixel spectra (endmembers) are a same inside all a image considering each class of materials. inside a framework of remote sensing, such an assumption was no more valid inside a presence of intra-class variabilities due to illumination conditions, weathering, slight variations of a pure materials, etc... inside this paper, we first describe a results of investigations highlighting intra-class variability measured inside real images. considering these results, the new formulation of a linear mixing model was presented leading to two new methods. unconstrained pixel-by-pixel nmf (up-nmf) was the new blind source separation method based on a assumption of the linear mixing model, which should deal with intra-class variability. to overcome up-nmf limitations an extended method was proposed, named inertia-constrained pixel-by-pixel nmf (ip-nmf). considering each sensed spectrum, these extended versions of nmf extract the corresponding set of source spectra. the constraint was set to limit a spreading of each source's estimates inside ip-nmf. a methods are tested on the semi-synthetic data set built with spectra extracted from the real hyperspectral image and then numerically mixed. we thus demonstrate a interest of our methods considering realistic source variabilities. finally, ip-nmf was tested on the real data set and it was shown to yield better performance than state of a art methods."
"activity analysis inside which multiple people interact across the large space was challenging due to a interplay of individual actions and collective group dynamics. we propose an end-to-end idea behind the method considering learning person trajectory representations considering group activity analysis. a learned representations encode rich spatio-temporal dependencies and capture useful motion patterns considering recognizing individual events, as well as characteristic group dynamics that should be used to identify groups from their trajectories alone. we develop our deep learning idea behind the method inside a context of team sports, which provide well-defined sets of events (e.g. pass, shot) and groups of people (teams). analysis of events and team formations with the help of nhl hockey and nba basketball datasets demonstrate a generality of our approach."
"we develop the unified description, using a boltzmann equation, of damping of gravitational waves by matter, incorporating collisions. we identify two physically distinct damping mechanisms -- collisional and landau damping. we first consider damping inside flat spacetime, and then generalize a results to allow considering cosmological expansion. inside a first regime, maximal collisional damping of the gravitational wave, independent of a details of a collisions inside a matter is, as we show, significant only when its wavelength was comparable to a size of a horizon. thus damping by intergalactic or interstellar matter considering all but primordial gravitational radiation should be neglected. although collisions inside matter lead to the shear viscosity, they also act to erase anisotropic stresses, thus suppressing a damping of gravitational waves. damping of primordial gravitational waves remains possible. we generalize weinberg's calculation of gravitational wave damping, now including collisions and particles of finite mass, and interpret a collisionless limit inside terms of landau damping. while landau damping of gravitational waves cannot occur inside flat spacetime, a expansion of a universe allows such damping by spreading a frequency of the gravitational wave of given wavevector."
"with a rapid growth inside renewable energy and battery storage technologies, there exists significant opportunity to improve energy efficiency and reduce costs through optimization. however, optimization algorithms must take into account a underlying dynamics and uncertainties of a various interconnected subsystems inside order to fully realize this potential. to this end, we formulate and solve an energy management optimization problem as the markov decision process (mdp) consisting of battery storage dynamics, the stochastic demand model, the stochastic solar generation model, and an electricity pricing scheme. a stochastic model considering predicting solar generation was constructed based on weather forecast data from a national oceanic and atmospheric administration. the near-optimal policy design was proposed using stochastic dynamic programming. simulation results are presented inside a context of the storage and solar-integrated residential and commercial building environments. results indicate that a near-optimal policy significantly reduces a operating costs compared to several heuristic alternatives. a proposed framework facilitates a design and evaluation of energy management policies with configurable demand-supply-storage parameters inside a presence of weather-induced uncertainties."
"we present the multi-modal dialogue system considering interactive learning of perceptually grounded word meanings from the human tutor. a system integrates an incremental, semantic parsing/generation framework - dynamic syntax and type theory with records (ds-ttr) - with the set of visual classifiers that are learned throughout a interaction and which ground a meaning representations that it produces. we use this system inside interaction with the simulated human tutor to study a effects of different dialogue policies and capabilities on a accuracy of learned meanings, learning rates, and efforts/costs to a tutor. we show that a overall performance of a learning agent was affected by (1) who takes initiative inside a dialogues; (2) a ability to express/use their confidence level about visual attributes; and (3) a ability to process elliptical and incrementally constructed dialogue turns. ultimately, we train an adaptive dialogue policy which optimises a trade-off between classifier accuracy and tutoring costs."
"we present an online landmark selection method considering distributed long-term visual localization systems inside bandwidth-constrained environments. sharing the common map considering online localization provides the fleet of au- tonomous vehicles with a possibility to maintain and access the consistent map source, and therefore reduce redundancy while increasing efficiency. however, connectivity over the mobile network imposes strict bandwidth constraints and thus a need to minimize a amount of exchanged data. a wide range of varying appearance conditions encountered during long-term visual localization offers a potential to reduce data usage by extracting only those visual cues which are relevant at a given time. motivated by this, we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under a prevailing appear- ance condition. a ranking function this selection was based upon exploits landmark co-observability statistics collected inside past traversals through a mapped area. evaluation was per- formed over different outdoor environments, large time-scales and varying appearance conditions, including a extreme tran- sition from day-time to night-time, demonstrating that with our appearance-dependent selection method, we should significantly reduce a amount of landmarks used considering localization while maintaining or even improving a localization performance."
"given the complete hereditary cotorsion pair $(\mathcal{a}, \mathcal{b})$ inside an abelian category $\mathcal{c}$ satisfying certain conditions, we study a completeness of a induced cotorsion pairs $(\phi(\mathcal{a}), \phi(\mathcal{a})^{\perp})$ and $(^{\perp}\psi(\mathcal{b}), \psi(\mathcal{b}) )$ inside a category $\mbox{rep}(q, \mathcal{c})$ of $\mathcal{c}$-valued representations of the given quiver $q$. we show that if $q$ was left rooted, then a cotorsion pair $(\phi(\mathcal{a}), \phi(\mathcal{a})^{\perp})$ was complete, and if $q$ was right rooted, then a cotorsion pair $(^{\perp}\psi(\mathcal{b}), \psi(\mathcal{b}) )$ was complete. besides, we work on a infinite line quiver $a_{\infty}^{\infty}$, which was neither left rooted nor right rooted. we prove that these cotorsion pairs inside $\mbox{rep}(a_{\infty}^{\infty}, r)$ are complete, as well."
"considering a next step inside human to machine interaction, artificial intelligence (ai) should interact predominantly with the help of natural language because, if it worked, it would be a fastest way to communicate. facebook's toy tasks (babi) provide the useful benchmark to compare implementations considering conversational ai. while a published experiments so far have been based on exploiting a distributional hypothesis with machine learning, our model exploits natural language understanding (nlu) with a decomposition of language based on role and reference grammar (rrg) and a brain-based patom theory. our combinatorial system considering conversational ai based on linguistics has many advantages: passing babi task tests without parsing or statistics while increasing scalability. our model validates both a training and test data to find 'garbage' input and output (gigo). it was not rules-based, nor does it use parts of speech, but instead relies on meaning. while deep learning was difficult to debug and fix, every step inside our model should be understood and changed like any non-statistical computer program. deep learning's lack of explicable reasoning has raised opposition to ai, partly due to fear of a unknown. to support a goals of ai, we propose extended tasks to use human-level statements with tense, aspect and voice, and embedded clauses with junctures: and answers to be natural language generation (nlg) instead of keywords. while machine learning permits invalid training data to produce incorrect test responses, our system cannot because a context tracking would need to be intentionally broken. we believe no existing learning systems should currently solve these extended natural language tests. there appears to be the knowledge gap between nlp researchers and linguists, but ongoing competitive results such as these promise to narrow that gap."
"we study a problem of computing a capacity of the discrete memoryless channel under uncertainty affecting a channel law matrix, and possibly with the constraint on a average cost of a input distribution. a problem has been formulated inside a literature as the max-min problem. we use a robust optimization methodology to convert a max-min problem to the standard convex optimization problem. considering small-sized problems, and considering many types of uncertainty, such the problem should be solved inside principle with the help of interior point methods (ipm). however, considering large-scale problems, ipm are not practical. here, we suggest an $\mathcal{o}(1/t)$ first-order algorithm based on nemirovski (2004) which was applied directly to a max-min problem."
"inside all-oxide ferroelectric (fe) - superconductor (s) bilayers, due to a low carrier concentration of oxides compared to transition metals, a fe interfacial polarization charges induce an accumulation (or depletion) of charge carriers inside a s. this leads either to an enhancement or the depression of its critical temperature depending on fe polarization direction.here we exploit this effect at the local scale to define planar weak-links inside high-temperature superconducting wires. this was realized inside bifeo3(fe)/yba2cu3o7(s)bilayers inside which a remnant fe domain structure was written at will by locally applying voltage pulses with the conductive-tip atomic force microscope. inside this fashion, a fe domain pattern defines the spatial modulation of superconductivity. this allows us to write the device whose electrical transport shows different temperature regimes and magnetic field matching effects that are characteristic of josephson coupled weak-links. this illustrates a potential of a ferroelectric idea behind the method considering a realization of high-temperature superconducting devices."
"a main contribution of this paper was the simple semi-supervised pipeline that only uses a original training set without collecting extra data. it was challenging inside 1) how to obtain more training data only from a training set and 2) how to use a newly generated data. inside this work, a generative adversarial network (gan) was used to generate unlabeled samples. we propose a label smoothing regularization considering outliers (lsro). this method assigns the uniform label distribution to a unlabeled images, which regularizes a supervised model and improves a baseline. we verify a proposed method on the practical problem: person re-identification (re-id). this task aims to retrieve the query person from other cameras. we adopt a deep convolutional generative adversarial network (dcgan) considering sample generation, and the baseline convolutional neural network (cnn) considering representation learning. experiments show that adding a gan-generated data effectively improves a discriminative ability of learned cnn embeddings. on three large-scale datasets, market-1501, cuhk03 and dukemtmc-reid, we obtain +4.37%, +1.6% and +2.46% improvement inside rank-1 precision over a baseline cnn, respectively. we additionally apply a proposed method to fine-grained bird recognition and achieve the +0.6% improvement over the strong baseline. a code was available at this https url."
"a discovery of a exoplanet proxima b highlights a potential considering a coming generation of giant segmented mirror telescopes (gsmts) to characterize terrestrial --- potentially habitable --- planets orbiting nearby stars with direct imaging. this will require continued development and implementation of optimized adaptive optics systems feeding coronagraphs on a gsmts. such development should proceed with an understanding of a fundamental limits imposed by atmospheric turbulence. here we seek to address this question with the semi-analytic framework considering calculating a post-coronagraph contrast inside the closed-loop ao system. we do this starting with a temporal power spectra of a fourier basis calculated assuming frozen flow turbulence, and then apply closed-loop transfer functions. we include a benefits of the simple predictive controller, which we show could provide over the factor of 1400 gain inside raw psf contrast at 1 $\lambda/d$ on bright stars, and more than the factor of 30 gain on an i = 7.5 mag star such as proxima. more sophisticated predictive control should be expected to improve this even further. assuming the photon noise limited observing technique such as high dispersion coronagraphy, these gains inside raw contrast will decrease integration times by a same large factors. predictive control of atmospheric turbulence should therefore be seen as one of a key technologies which will enable ground-based telescopes to characterize terrrestrial planets."
"inside robotics, it was essential to be able to plan efficiently inside high-dimensional continuous state-action spaces considering long horizons. considering such complex planning problems, unguided uniform sampling of actions until the path to the goal was found was hopelessly inefficient, and gradient-based approaches often fall short when a optimization manifold of the given problem was not smooth. inside this paper we present an idea behind the method that guides a search of the state-space planner, such as a*, by learning an action-sampling distribution that should generalize across different instances of the planning problem. a motivation was that, unlike typical learning approaches considering planning considering continuous action space that approximate the policy, an estimated action sampler was more robust to error since it has the planner to fall back on. we use the generative adversarial network (gan), and address an important issue: search experience consists of the relatively large number of actions that are not on the solution path and the relatively small number of actions that actually are on the solution path. we introduce the new technique, based on an importance-ratio approximation method, considering with the help of samples from the non-target distribution to make gan learning more data-efficient. we provide theoretical guarantees and empirical evaluation inside three challenging continuous robot planning problems to illustrate a effectiveness of our algorithm."
"selecting the representative vector considering the set of vectors was the very common requirement inside many algorithmic tasks. traditionally, a mean or median vector was selected. ontology classes are sets of homogeneous instance objects that should be converted to the vector space by word vector embeddings. this study proposes the methodology to derive the representative vector considering ontology classes whose instances were converted to a vector space. we start by deriving five candidate vectors which are then used to train the machine learning model that would calculate the representative vector considering a class. we show that our methodology out-performs a traditional mean and median vector representations."
"considering any $p\in(0,\,1]$, let $h^{\phi_p}(\mathbb{r}^n)$ be a musielak-orlicz hardy space associated with a musielak-orlicz growth function $\phi_p$, defined by setting, considering any $x\in\mathbb{r}^n$ and $t\in[0,\,\infty)$, $$ \phi_{p}(x,\,t):= \begin{cases} \frac{t}{\log(e+t)+[t(1+|x|)^n]^{1-p}} & \qquad \text{when } n(1/p-1)\notin \mathbb{n} \cup \{0\}; \\ \frac{t}{\log(e+t)+[t(1+|x|)^n]^{1-p}[\log(e+|x|)]^p} & \qquad \text{when } n(1/p-1)\in \mathbb{n}\cup\{0\},\\ \end{cases} $$ which was a sharp target space of a bilinear decomposition of a product of a hardy space $h^p(\mathbb{r}^n)$ and its dual. moreover, $h^{\phi_1}(\mathbb{r}^n)$ was a prototype appearing inside a real-variable theory of general musielak-orlicz hardy spaces. inside this article, a authors find the new structure of a space $h^{\phi_p}(\mathbb{r}^n)$ by showing that, considering any $p\in(0,\,1]$, $h^{\phi_p}(\mathbb{r}^n)=h^{\phi_0}(\mathbb{r}^n) +h_{w_p}^p(\mathbb{r}^n)$ and, considering any $p\in(0,\,1)$, $h^{\phi_p}(\mathbb{r}^n)=h^{1}(\mathbb{r}^n) +h_{w_p}^p(\mathbb{r}^n)$, where $h^1(\mathbb{r}^n)$ denotes a classical real hardy space, $h^{\phi_0}(\mathbb{r}^n)$ a orlicz-hardy space associated with a orlicz function $\phi_0(t):=t/\log(e+t)$ considering any $t\in [0,\infty)$ and $h_{w_p}^p(\mathbb{r}^n)$ a weighted hardy space associated with certain weight function $w_p(x)$ that was comparable to $\phi_p(x,1)$ considering any $x\in\mathbb{r}^n$. as an application, a authors further establish an interpolation theorem of quasilinear operators based on this new structure."
"inside this work we apply and expand on the recently introduced outlier detection algorithm that was based on an unsupervised random forest. we use a algorithm to calculate the similarity measure considering stellar spectra from a apache point observatory galactic evolution experiment (apogee). we show that a similarity measure traces non-trivial physical properties and contains information about complex structures inside a data. we use it considering visualization and clustering of a dataset, and discuss its ability to find groups of highly similar objects, including spectroscopic twins. with the help of a similarity matrix to search a dataset considering objects allows us to find objects that are impossible to find with the help of their best fitting model parameters. this includes extreme objects considering which a models fail, and rare objects that are outside a scope of a model. we use a similarity measure to detect outliers inside a dataset, and find the number of previously unknown be-type stars, spectroscopic binaries, carbon rich stars, young stars, and the few that we cannot interpret. our work further demonstrates a potential considering scientific discovery when combining machine learning methods with modern survey data."
"grigni and hung~\cite{gh12} conjectured that h-minor-free graphs have $(1+\epsilon)$-spanners that are light, that is, of weight $g(|h|,\epsilon)$ times a weight of a minimum spanning tree considering some function $g$. this conjecture implies a {\em efficient} polynomial-time approximation scheme (ptas) of a traveling salesperson problem inside $h$-minor free graphs; that is, the ptas whose running time was of a form $2^{f(\epsilon)}n^{o(1)}$ considering some function $f$. a state of a art ptas considering tsp inside h-minor-free-graphs has running time $n^{1/\epsilon^c}$. we take the further step toward proving this conjecture by showing that if a bounded treewidth graphs have light greedy spanners, then a conjecture was true. we also prove that a greedy spanner of the bounded pathwidth graph was light and discuss a possibility of extending our proof to bounded treewidth graphs."
"we propose the data-driven method to solve the stochastic optimal power flow (opf) problem based on limited information about forecast error distributions. a objective was to determine power schedules considering controllable devices inside the power network to balance operation cost and conditional value-at-risk (cvar) of device and network constraint violations. these decisions include scheduled power output adjustments and reserve policies, which specify planned reactions to forecast errors inside order to accommodate fluctuating renewable energy sources. instead of assuming a uncertainties across a networks follow prescribed probability distributions, we assume a distributions are only observable through the finite training dataset. by utilizing a wasserstein metric to quantify differences between a empirical data-based distribution and a real data-generating distribution, we formulate the distributionally robust optimization opf problem to search considering power schedules and reserve policies that are robust to sampling errors inherent inside a dataset. the simple numerical example illustrates inherent tradeoffs between operation cost and risk of constraint violation, and we show how our proposed method offers the data-driven framework to balance these objectives."
"surveying 3d scenes was the common task inside robotics. systems should do so autonomously by iteratively obtaining measurements. this process of planning observations to improve a model of the scene was called next best view (nbv) planning. nbv planning approaches often use either volumetric (e.g., voxel grids) or surface (e.g., triangulated meshes) representations. volumetric approaches generalise well between scenes as they do not depend on surface geometry but do not scale to high-resolution models of large scenes. surface representations should obtain high-resolution models at any scale but often require tuning of unintuitive parameters or multiple survey stages. this paper presents the scene-model-free nbv planning idea behind the method with the density representation. a surface edge explorer (see) uses a density of current measurements to detect and explore observed surface boundaries. this idea behind the method was shown experimentally to provide better surface coverage inside lower computation time than a evaluated state-of-the-art volumetric approaches while moving equivalent distances."
"bayesian probabilistic numerical methods are the set of tools providing posterior distributions on a output of numerical methods. a use of these methods was usually motivated by a fact that they should represent our uncertainty due to incomplete/finite information about a continuous mathematical problem being approximated. inside this paper, we demonstrate that this paradigm should provide additional advantages, such as a possibility of transferring information between several numerical methods. this allows users to represent uncertainty inside the more faithful manner and, as the by-product, provide increased numerical efficiency. we propose a first such numerical method by extending a well-known bayesian quadrature algorithm to a case where we are interested inside computing a integral of several related functions. we then prove convergence rates considering a method inside a well-specified and misspecified cases, and demonstrate its efficiency inside a context of multi-fidelity models considering complex engineering systems and the problem of global illumination inside computer graphics."
"optimizing nonlinear systems involving expensive (computer) experiments with regard to conflicting objectives was the common challenge. when a number of experiments was severely restricted and/or when a number of objectives increases, uncovering a whole set of optimal solutions (the pareto front) was out of reach, even considering surrogate-based approaches. as non-compromising pareto optimal solutions have usually little point inside applications, this work restricts a search to relevant solutions that are close to a pareto front center. a article starts by characterizing this center. next, the bayesian multi-objective optimization method considering directing a search towards it was proposed. the criterion considering detecting convergence to a center was described. if a criterion was triggered, the widened central part of a pareto front was targeted such that sufficiently accurate convergence to it was forecasted within a remaining budget. numerical experiments show how a resulting algorithm, c-ehi, better locates a central part of a pareto front when compared to state-of-the-art bayesian algorithms."
"disordered thin films close to a superconducting-insulating phase transition (sit) hold a key to understanding quantum phase transition inside strongly correlated materials. a sit was governed by superconducting quantum fluctuations, which should be revealed considering example by tunneling measurements. these experiments detect the spectral gap, accompanied by suppressed coherence peaks that do not fit a bcs prediction. to explain these observations, we consider a effect of finite-range superconducting fluctuations on a density of states, focusing on a insulating side of a sit. we perform the controlled diagrammatic resummation and derive analytic expressions considering a tunneling differential conductance. we find that short-range superconducting fluctuations suppress a coherence peaks, even inside a presence of long-range correlations. our idea behind the method offers the quantitative description of existing measurements on disordered thin films and accounts considering tunneling spectra with suppressed coherence peaks observed, considering example, inside a pseudo gap regime of high-temperature superconductors."
"under a riemann hypothesis, we improve a error term inside a asymptotic formula related to a counting lattice problem studied inside the first part of this work. a improvement comes from a use of weyl's bound considering exponential sums of polynomials and the device due to popov allowing us to get an improved main term inside a sums of certain fractional parts of polynomials."
"inside biostatistics, propensity score was the common idea behind the method to analyze a imbalance of covariate and process confounding covariates to eliminate differences between groups. while there are an abundant amount of methods to compute propensity score, the common issue of them was a corrupted labels inside a dataset. considering example, a data collected from a patients could contain samples that are treated mistakenly, and a computing methods could incorporate them as the misleading information. inside this paper, we propose the machine learning-based method to handle a problem. specifically, we utilize a fact that a majority of sample should be labeled with a correct instance and design an idea behind the method to first cluster a data with spectral clustering and then sample the new dataset with the distribution processed from a clustering results. a propensity score was computed by xgboost, and the mathematical justification of our method was provided inside this paper. a experimental results illustrate that xgboost propensity scores computing with a data processed by our method could outperform a same method with original data, and a advantages of our method increases as we add some artificial corruptions to a dataset. meanwhile, a implementation of xgboost to compute propensity score considering multiple treatments was also the pioneering work inside a area."
"a theory of compressive sensing (cs) asserts that an unknown signal $\mathbf{x} \in \mathbb{c}^n$ should be accurately recovered from $m$ measurements with $m\ll n$ provided that $\mathbf{x}$ was sparse. most of a recovery algorithms need a sparsity $s=\lvert\mathbf{x}\rvert_0$ as an input. however, generally $s$ was unknown, and directly estimating a sparsity has been an open problem. inside this study, an estimator of sparsity was proposed by with the help of bayesian hierarchical model. its statistical properties such as unbiasedness and asymptotic normality are proved. inside a simulation study and real data study, magnetic resonance image data was used as input signal, which becomes sparse after sparsified transformation. a results from a simulation study confirm a theoretical properties of a estimator. inside practice, a approximate from the real mr image should be used considering recovering future mr images under a framework of cs if they are believed to have a same sparsity level after sparsification."
"we present the novel idea behind the method considering the combined analysis of x-ray and gravitational lensing data and apply this technique to a merging galaxy cluster macs j0416.1$-$2403. a method exploits a information on a intracluster gas distribution that comes from the fit of a x-ray surface brightness, and then includes a hot gas as the fixed mass component inside a strong lensing analysis. with our new technique, we should separate a collisional from a collision-less diffuse mass components, thus obtaining the more accurate reconstruction of a dark matter distribution inside a core of the cluster. we introduce an analytical description of a x-ray emission coming from the set of dual pseudo-isothermal elliptical (dpie) mass distributions, which should be directly used inside most lensing softwares. by combining \emph{chandra} observations with hubble frontier fields imaging and muse spectroscopy inside macs j0416.1$-$2403, we measure the projected gas over total mass fraction of approximately $10\%$ at $350$ kpc from a cluster center. compared to a results of the more traditional cluster mass model (diffuse halos plus member galaxies), we find the significant difference inside a cumulative projected mass profile of a dark matter component and that a dark matter to total mass fraction was almost constant, out to more than $350$ kpc. inside a coming era of large surveys, these results show a need of multi-probe analyses considering detailed dark matter studies inside galaxy clusters."
"traffic signals are ubiquitous devices that first appeared inside 1868. recent advances inside information and communications technology (ict) have led to unprecedented improvements inside such areas as mobile handheld devices (i.e., smartphones), a electric power industry (i.e., smart grids), transportation infrastructure, and vehicle area networks. given a trend towards interconnectivity, it was only the matter of time before vehicles communicate with one another and with infrastructure. inside fact, several pilots of such vehicle-to-vehicle and vehicle-to-infrastructure (e.g. traffic lights and parking spaces) communication systems are already operational. this survey of autonomous and self-organized traffic signaling control has been undertaken with these potential developments inside mind. our research results indicate that, while many sophisticated techniques have attempted to improve a scheduling of traffic signal control, either real-time sensing of traffic patterns or the priori knowledge of traffic flow was required to optimize traffic. once this was achieved, communication between traffic signals will serve to vastly improve overall traffic efficiency."
"we study a evolution of strictly mean-convex entire graphs over $r^n$ by inverse mean curvature flow. first we establish a global existence of starshaped entire graphs with superlinear growth at infinity. a main result inside this work concerns a critical case of asymptotically conical entire convex graphs. inside this case we show that there exists the time $ t < +\infty$, which depends on a growth at infinity of a initial data, such that a unique solution of a flow exists considering all $t < t$. moreover, as $t \to t$ a solution converges to the flat plane. our techniques exploit a ultra-fast diffusion character of a fully-nonlinear flow, the property that implies that a asymptotic behavior at spatial infinity of our solution plays the crucial influence on a maximal time of existence, as such behavior propagates infinitely fast towards a interior."
"a compounds sr$_{3}$cr$_{2}$o$_{8}$ and ba$_{3}$cr$_{2}$o$_{8}$ are insulating dimerized antiferromagnets with cr$^{5+}$ magnetic ions. these spin-$\frac{1}{2}$ ions form hexagonal bilayers with the strong intradimer antiferromagnetic interaction, that leads to the singlet ground state and gapped triplet states. we report on a effect on a magnetic properties of sr$_{3}$cr$_{2}$o$_{8}$ by introducing chemical disorder upon replacing sr by ba. two single crystals of ba$_{3-x}$sr$_{x}$cr$_{2}$o$_{8}$ with $x=2.9$ (3.33\% of $mixing$) and $x=2.8$ (6.66\%) were grown inside the four-mirror type optical floating-zone furnace. a magnetic properties on these compounds were studied by magnetization measurements. inelastic neutron scattering measurements on ba$_{0.1}$sr$_{2.9}$cr$_{2}$o$_{8}$ were performed inside order to determine a interaction constants and a spin gap considering $x=2.9$. a intradimer interaction constant was found to be $j_0$=5.332(2) mev, about 4\% smaller than that of pure sr$_{3}$cr$_{2}$o$_{8}$, while a interdimer exchange interaction $j_e$ was smaller by 6.9\%. these results indicate the noticeable change inside a magnetic properties by the random substitution effect."
"deep learning techniques are increasingly popular inside a textual entailment task, overcoming a fragility of traditional discrete models with hard alignments and logics. inside particular, a recently proposed attention models (rocktäschel et al., 2015; wang and jiang, 2015) achieves state-of-the-art accuracy by computing soft word alignments between a premise and hypothesis sentences. however, there remains the major limitation: this line of work completely ignores syntax and recursion, which was helpful inside many traditional efforts. we show that it was beneficial to extend a attention model to tree nodes between premise and hypothesis. more importantly, this subtree-level attention reveals information about entailment relation. we study a recursive composition of this subtree-level entailment relation, which should be viewed as the soft version of a natural logic framework (maccartney and manning, 2009). experiments show that our structured attention and entailment composition model should correctly identify and infer entailment relations from a bottom up, and bring significant improvements inside accuracy."
"a deployment of deep convolutional neural networks (cnns) inside many real world applications was largely hindered by their high computational cost. inside this paper, we propose the novel learning scheme considering cnns to simultaneously 1) reduce a model size; 2) decrease a run-time memory footprint; and 3) lower a number of computing operations, without compromising accuracy. this was achieved by enforcing channel-level sparsity inside a network inside the simple but effective way. different from many existing approaches, a proposed method directly applies to modern cnn architectures, introduces minimum overhead to a training process, and requires no special software/hardware accelerators considering a resulting models. we call our idea behind the method network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. we empirically demonstrate a effectiveness of our idea behind the method with several state-of-the-art cnn models, including vggnet, resnet and densenet, on various image classification datasets. considering vggnet, the multi-pass version of network slimming gives the 20x reduction inside model size and the 5x reduction inside computing operations."
"200 nm thick sio2 layers grown on si substrates and ge ions of 150 kev energy were implanted into sio2 matrix with different fluences. a implanted samples were annealed at 950 c considering 30 minutes inside ar ambience. topographical studies of implanted as well as annealed samples were captured by a atomic force microscopy (afm). two dimension (2d) multifractal detrended fluctuation analysis (mfdfa) based on a partition function idea behind the method has been used to study a surfaces of ion implanted and annealed samples. a partition function was used to calculate generalized hurst exponent with a segment size. moreover, it was seen that a generalized hurst exponents vary nonlinearly with a moment, thereby exhibiting a multifractal nature. a multifractality of surface was pronounced after annealing considering a surface implanted with fluence 7.5x1016 ions/cm^2."
"this paper explores a information-theoretic measure entropy to detect metaphoric change, transferring ideas from hypernym detection to research on language change. we also build a first diachronic test set considering german as the standard considering metaphoric change annotation. our model shows high performance, was unsupervised, language-independent and generalizable to other processes of semantic change."
"finite gaussian mixture models are widely used considering model-based clustering of continuous data. nevertheless, since a number of model parameters scales quadratically with a number of variables, these models should be easily over-parameterized. considering this reason, parsimonious models have been developed using covariance matrix decompositions or assuming local independence. however, these remedies do not allow considering direct approximation of sparse covariance matrices nor do they take into account that a structure of association among a variables should vary from one cluster to a other. to this end, we introduce mixtures of gaussian covariance graph models considering model-based clustering with sparse covariance matrices. the penalized likelihood idea behind the method was employed considering approximation and the general penalty term on a graph configurations should be used to induce different levels of sparsity and incorporate prior knowledge. model approximation was carried out with the help of the structural-em algorithm considering parameters and graph structure estimation, where two alternative strategies based on the genetic algorithm and an efficient stepwise search are proposed considering inference. with this approach, sparse component covariance matrices are directly obtained. a framework results inside the parsimonious model-based clustering of a data using the flexible model considering a within-group joint distribution of a variables. extensive simulated data experiments and application to illustrative datasets show that a method attains good classification performance and model quality."
"beryllium (be) was an important material with wide applications ranging from aerospace components to x-ray equipments. yet the precise understanding of its phase diagram remains elusive. we have investigated a phase stability of be with the help of the recently developed hybrid free energy computation method that accounts considering anharmonic effects by invoking phonon quasiparticles. we find that a hcp to bcc transition occurs near a melting curve at 0<p<11 gpa with the positive clapeyron slope of 41 k/gpa. a bcc phase exists inside the narrow temperature range that shrinks with increasing pressure, explaining a difficulty inside observing this phase experimentally. this work also demonstrates a validity of this theoretical framework based on phonon quasiparticle to study structural stability and phase transitions inside strongly anharmonic materials."
"current machine learning systems operate, almost exclusively, inside the statistical, or model-free mode, which entails severe theoretical limits on their power and performance. such systems cannot reason about interventions and retrospection and, therefore, cannot serve as a basis considering strong ai. to achieve human level intelligence, learning machines need a guidance of the model of reality, similar to a ones used inside causal inference tasks. to demonstrate a essential role of such models, i will present the summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished with the help of a tools of causal modeling."
"this paper describes the novel communication-spare cooperative localization algorithm considering the team of mobile unmanned robotic vehicles. exploiting an event-based approximation paradigm, robots only send measurements to neighbors when a expected innovation considering state approximation was high. since agents know a event-triggering condition considering measurements to be sent, a lack of the measurement was thus also informative and fused into state estimates. a robots use the covariance intersection (ci) mechanism to occasionally synchronize their local estimates of a full network state. inside addition, heuristic balancing dynamics on a robots' ci-triggering thresholds ensure that, inside large diameter networks, a local error covariances remains below desired bounds across a network. simulations on both linear and nonlinear dynamics/measurement models show that a event-triggering idea behind the method achieves nearly optimal state approximation performance inside the wide range of operating conditions, even when with the help of only the fraction of a communication cost required by conventional full data sharing. a robustness of a proposed idea behind the method to lossy communications, as well as a relationship between network topology and ci-based synchronization requirements, are also examined."
"thermodynamic integration (ti) considering computing marginal likelihoods was based on an inverse annealing path from a prior to a posterior distribution. inside many cases, a resulting estimator suffers from high variability, which particularly stems from a prior regime. when comparing complex models with differences inside the comparatively small number of parameters, intrinsic errors from sampling fluctuations may outweigh a differences inside a log marginal likelihood estimates. inside a present article, we propose the thermodynamic integration scheme that directly targets a log bayes factor. a method was based on the modified annealing path between a posterior distributions of a two models compared, which systematically avoids a high variance prior regime. we combine this scheme with a concept of non-equilibrium ti to minimise discretisation errors from numerical integration. results obtained on bayesian regression models applied to standard benchmark data, and the complex hierarchical model applied to biopathway inference, demonstrate the significant reduction inside estimator variance over state-of-the-art ti methods."
"given the web-scale graph that grows over time, how should its edges be stored and processed on multiple machines considering rapid and accurate approximation of a count of triangles? a count of triangles (i.e., cliques of size three) has proven useful inside many applications, including anomaly detection, community detection, and link recommendation. considering triangle counting inside large and dynamic graphs, recent work has focused largely on streaming algorithms and distributed algorithms. to achieve a advantages of both approaches, we propose dislr, the distributed streaming algorithm that estimates a counts of global triangles and local triangles associated with each node. making one pass over a input stream, dislr carefully processes and stores a edges across multiple machines so that a redundant use of computational and storage resources was minimized. compared to its best competitors, dislr was (a) accurate: giving up to 39x smaller approximation error, (b) fast: up to 10.4x faster, scaling linearly with a number of edges inside a input stream, and (c) theoretically sound: yielding unbiased estimates with variances decreasing faster as a number of machines was scaled up."
"we consider the network of interconnected dynamical systems. spectral network identification consists inside recovering a eigenvalues of a network laplacian from a measurements of the very limited number (possibly one) of signals. these eigenvalues allow to deduce some global properties of a network, such as bounds on a node degree. having recently introduced this idea behind the method considering autonomous networks of nonlinear systems, we extend it here to treat networked systems with external inputs on a nodes, inside a case of linear dynamics. this was more natural inside several applications, and removes a need to sometimes use several independent trajectories. we illustrate our framework with several examples, where we approximate a mean, minimum, and maximum node degree inside a network. inferring some information on a leading laplacian eigenvectors, we also use our framework inside a context of network clustering."
"mass around dark matter halos should be divided into ""infalling"" material and ""collapsed"" material that has passed through at least one pericenter. analytical models and simulations predict the rapid drop inside a halo density profile associated with a transition between these two regimes. with the help of data from sdss, we explore a evidence considering such the feature inside a density profiles of galaxy clusters and investigate a connection between this feature and the possible phase space boundary. we first approximate a steepening of a outer galaxy density profile around clusters: a profiles show an abrupt steepening, providing evidence considering truncation of a halo profile. next, we measure a galaxy density profile around clusters with the help of two sets of galaxies selected based on color. we find evidence of an abrupt change inside a galaxy colors that coincides with a location of a steepening of a density profile. since galaxies are likely to be quenched of star formation and turn red in of clusters, this change inside a galaxy color distribution should be interpreted as a transition from an infalling regime to the collapsed regime. we also measure this transition with the help of the model comparison idea behind the method which has been used recently inside studies of a ""splashback"" phenomenon, but find that this idea behind the method was not the robust way to quantify a significance of detecting the splashback-like feature. finally, we perform measurements with the help of an independent cluster catalog to test considering potential systematic errors associated with cluster selection. we identify several avenues considering future work: improved understanding of a small-scale galaxy profile, lensing measurements, identification of proxies considering a halo accretion rate, and other tests. with upcoming data from a des, kids and hsc surveys, we should expect significant improvements inside a study of halo boundaries."
"inside many real-world applications, data usually contain outliers. one popular idea behind the method was to use l2,1 norm function as the robust error/loss function. however, a robustness of l2,1 norm function was not well understood so far. inside this paper, we propose the new vector outlier regularization (vor) framework to understand and analyze a robustness of l2,1 norm function. our vor function defines the data point to be outlier if it was outside the threshold with respect to the theoretical prediction, and regularize it-pull it back to a threshold line. we then prove that l2,1 function was a limiting case of this vor with a usual least square/l2 error function as a threshold shrinks to zero. one interesting property of vor was that how far an outlier lies away from its theoretically predicted value does not affect a final regularization and analysis results. this vor property unmasks one of a most peculiar property of l2,1 norm function: a effects of outliers seem to be independent of how outlying they are-if an outlier was moved further away from a intrinsic manifold/subspace, a final analysis results do not change. vor provides the new way to understand and analyze a robustness of l2,1 norm function. applying vor to matrix factorization leads to the new vorpca model. we give the comprehensive comparison with trace-norm based l21-norm pca to demonstrate a advantages of vorpca."
"a trinity of so-called ""canonical"" wall-bounded turbulent flows, comprising a zero pressure gradient turbulent boundary layer, abbreviated zpg tbl, turbulent pipe flow and channel/duct flows has continued to receive intense attention as new and more reliable experimental data have become available. nevertheless, a debate on whether a logarithmic part of a mean velocity profile, inside particular a kármán constant $\kappa$, was identical considering these three canonical flows or flow-dependent was still ongoing. inside this paper, which expands upon monkewitz and nagib (24th ictam conf., montreal, 2016), a asymptotic matching requirement of equal $\kappa$ inside a log-law and inside a expression considering a centerline/free-stream velocity was reiterated and shown to preclude the single universal log-law inside a three canonical flows or at least make it very unlikely. a current re-analysis of high quality mean velocity profiles inside zpg tbl's, a princeton ""superpipe"" and inside channels and ducts leads to the coherent description of (almost) all seemingly contradictory data interpretations inside terms of two logarithmic regions inside pipes and channels: the universal interior, near-wall logarithmic region with a same parameters as inside a zpg tbl, inside particular $\kappa_{\mathrm{wall}} \cong 0.384$, but only extending from around $150$ to around $10^3$ wall units, and shrinking with increasing pressure gradient, followed by an exterior logarithmic region with the flow specific $\kappa$ matching a logarithmic slope of a respective free-stream or centerline velocity. a log-law parameters of a exterior logarithmic region inside channels and pipes are shown to depend monotonically on a pressure gradient."
"a goal of a present study is: (i) to demonstrate a two-dimensional nature of a elasto-inertial instability inside elasto-inertial turbulence (eit), (ii) to identify a role of a bi-dimensional instability inside three-dimensional eit flows and (iii) to establish a role of a small elastic scales inside a mechanism of self-sustained eit. direct numerical simulations of fene-p fluid flows are performed inside two- and three-dimensional channels. a reynolds number was set to $\mathrm{re}_\tau = 85$ which was sub-critical considering 2d flows but beyond transition considering 3d ones. a polymer properties correspond to those of typical dilute polymer solutions and two moderate weissenberg numbers, $\mathrm{wi}_\tau = 40, 100$, are considered. a simulation results show that sustained turbulence should be observed inside 2d sub-critical flows, confirming a existence of the bi-dimensional elasto-inertial instability. a same type of instability was also observed inside 3d simulations where both newtonian and elasto-inertial turbulent structures co-exist. depending on a wi number, one type of structure should dominate and drive a flow. considering large wi values, a elasto-inertial instability tends to prevail over a newtonian turbulence. this statement was supported by (i) a absence of a typical newtonian near-wall vortices and (ii) strong similarities between two- and three-dimensional flows when considering larger wi numbers. a role of a small elastic scales was investigated by introducing global artificial diffusion inside a hyperbolic transport equation considering polymers. a study results show that a introduction of large polymer diffusion inside a system strongly damps the significant part of a elastic scales that are necessary to feed turbulence, eventually leading to a flow laminarization. the sufficiently high schmidt number was necessary to allow self-sustained turbulence to settle."
"we numerically investigate a effects of disorder on a quantum hall effect (qhe) and a quantum phase transitions inside silicene based on the lattice model. it was shown that considering the clean sample, silicene exhibits an unconventional qhe near a band center, with plateaus developing at $\nu=0,\pm2,\pm6,\ldots,$ and the conventional qhe near a band edges. inside a presence of disorder, a hall plateaus should be destroyed through a float-up of extended levels toward a band center, inside which higher plateaus disappear first. however, a center $\nu=0$ hall plateau was more sensitive to disorder and disappears at the relatively weak disorder strength. moreover, a combination of an electric field and a intrinsic spin-orbit interaction (soi) should lead to quantum phase transitions from the topological insulator to the band insulator at a charge neutrality point (cnp), accompanied by additional quantum hall conductivity plateaus."
"we consider the notion of conservation considering a heat semigroup associated to the generalized dirac laplacian acting on sections of the vector bundle over the noncompact manifold with the (possibly noncompact) boundary under mixed boundary conditions. assuming that a geometry of a underlying manifold was controlled inside the suitable way and imposing uniform lower bounds on a zero order (weitzenböck) piece of a dirac laplacian and on a endomorphism defining a mixed boundary condition we show that a corresponding conservation principle holds. the key ingredient inside a proof was the domination property considering a heat semigroup which follows from an extension to this setting of the feynman-kac formula recently proved inside \cite{dl1} inside a context of differential forms. when applied to a hodge laplacian acting on differential forms satisfying absolute boundary conditions, this extends previous results by vesentini \cite{ve} and masamune \cite{m} inside a boundaryless case. along a way we also prove the vanishing result considering $l^2$ harmonic sections inside a broader context of generalized (not necessarily dirac) laplacians. these results are further illustrated with applications to a dirac laplacian acting on spinors and to a jacobi operator acting on sections of a normal bundle of the free boundary minimal immersion."
"power prediction demand was vital inside power system and delivery engineering fields. by efficiently predicting a power demand, we should forecast a total energy to be consumed inside the certain city or district. thus, exact resources required to produce a demand power should be allocated. inside this paper, the stochastic gradient boosting (aka treeboost) model was used to predict a short term power demand considering a emirate of sharjah inside a united arab emirates (uae). results show that a proposed model gives promising results inside comparison to a model used by sharjah electricity and water authority (sewa)."
"inside this paper, we present the cooperative odometry scheme based on a detection of mobile markers inside line with a idea of cooperative positioning considering multiple robots [1]. to this end, we introduce the simple optimization scheme that realizes visual mobile marker odometry using accurate fixed marker-based camera positioning and analyse a characteristics of errors inherent to a method compared to classical fixed marker-based navigation and visual odometry. inside addition, we provide the specific uav-ugv configuration that allows considering continuous movements of a uav without doing stops and the minimal caterpillar-like configuration that works with one ugv alone. finally, we present the real-world implementation and evaluation considering a proposed uav-ugv configuration."
"we prove that a realization $a_p$ inside $l^p(\mathbb{r}^n),\,1<p<\infty$, of a elliptic operator $a=(1+|x|^{\alpha})\delta+b|x|^{\alpha-1}\frac{x}{|x|}\cdot \nabla-c|x|^{\beta}$ with domain $d(a_p) =\{ u \in w^{2,p}(\mathbb{r}^n)\, |\, au \in l^p(\mathbb{r}^n)\}$ generates the strongly continuous analytic semigroup $t(\cdot)$ provided that $\alpha >2,\,\beta >\alpha -2$ and any constants $b\in \mathbb{r}$ and $c>0$. this generalizes a recent results inside [a.canale, a. rhandi, c. tacelli, ann. sc. norm. super. pisa ci. sci. (5), 2016] and inside [g.metafune, c.spina, c.tacelli, adv. diff. equat., 2014]. moreover we show that $t(\cdot)$ was consistent, immediately compact and ultracontractive."
"humans are going to delegate a rights of driving to a autonomous vehicles inside near future. however, to fulfill this complicated task, there was the need considering the mechanism, which enforces a autonomous vehicles to obey a road and social rules that have been practiced by well-behaved drivers. this task should be achieved by introducing social norms compliance mechanism inside a autonomous vehicles. this research paper was proposing an artificial society of autonomous vehicles as an analogy of human social society. each av has been assigned the social personality having different social influence. social norms have been introduced which aid a avs inside making a decisions, influenced by emotions, regarding road collision avoidance. furthermore, social norms compliance mechanism, by artificial social avs, has been proposed with the help of prospect based emotion i.e. fear, which was conceived from occ model. fuzzy logic has been employed to compute a emotions quantitatively. then, with the help of simconnect approach, fuzzy values of fear has been provided to a netlogo simulation environment to simulate artificial society of avs. extensive testing has been performed with the help of a behavior space tool to find out a performance of a proposed idea behind the method inside terms of a number of collisions. considering comparison, a random-walk model based artificial society of avs has been proposed as well. the comparative study with the random walk, prove that proposed idea behind the method provides the better option to tailor a autopilots of future avs, which will be more socially acceptable and trustworthy by their riders inside terms of safe road travel."
"we give the survey of recent results on weak-strong uniqueness considering compressible and incompressible euler and navier-stokes equations, and also make some new observations. a importance of a weak-strong uniqueness principle stems, on a one hand, from a instances of non-uniqueness considering a euler equations exhibited inside a past years; and on a other hand from a question of convergence of singular limits, considering which weak-strong uniqueness represents an elegant tool."
"inside a range minimum query (rmq) problem, we are given an array $a$ of $n$ numbers and we are asked to answer queries of a following type: considering indices $i$ and $j$ between $0$ and $n-1$, query $\text{rmq}_a(i,j)$ returns a index of the minimum element inside a subarray $a[i..j]$. answering the small batch of rmqs was the core computational task inside many real-world applications, inside particular due to a connection with a lowest common ancestor (lca) problem. with small batch, we mean that a number $q$ of queries was $o(n)$ and we have them all at hand. it was therefore not relevant to build an $\omega(n)$-sized data structure or spend $\omega(n)$ time to build the more succinct one. it was well-known, among practitioners and elsewhere, that these data structures considering online querying carry high constants inside their pre-processing and querying time. we would thus like to answer this batch efficiently inside practice. with efficiently inside practice, we mean that we (ultimately) want to spend $n + \mathcal{o}(q)$ time and $\mathcal{o}(q)$ space. we write $n$ to stress that a number of operations per entry of $a$ should be the very small constant. here we show how existing algorithms should be easily modified to satisfy these conditions. a presented experimental results highlight a practicality of this new scheme. a most significant improvement obtained was considering answering the small batch of lca queries. the library implementation of a presented algorithms was made available."
"we introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. inside this two part treatise, we present our developments inside a context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. depending on a nature and arrangement of a available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. a resulting neural networks form the new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. inside this first part, we demonstrate how these networks should be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters."
"every network scientist knows that preferential attachment combines with growth to produce networks with power-law in-degree distributions. how, then, was it possible considering a network of american physical society journal collection citations to enjoy the log-normal citation distribution when it is found to have grown inside accordance with preferential attachment? this anomalous result, which we exalt as a preferential attachment paradox, has remained unexplained since a physicist sidney redner first made light of it over the decade ago. here we propose the resolution. a chief source of a mischief, we contend, lies inside redner having relied on the measurement procedure bereft of a accuracy required to distinguish preferential attachment from another form of attachment that was consistent with the log-normal in-degree distribution. there is the high-accuracy measurement procedure inside use at a time, but it would have have been difficult to use it to shed light on a paradox, due to a presence of the systematic error inducing design flaw. inside recent years a design flaw had been recognised and corrected. we show that a bringing of a newly corrected measurement procedure to bear on a data leads to the resolution of a paradox."
"humans use various social bonding methods known as social grooming, e.g. face to face communication, greetings, phone, and social networking sites (sns). sns have drastically decreased time and distance constraints of social grooming. inside this paper, i show that two types of social grooming (elaborate social grooming and lightweight social grooming) were discovered inside the model constructed by thirteen communication data-sets including face to face, sns, and chacma baboons. a separation of social grooming methods was caused by the difference inside a trade-off between a number and strength of social relationships. a trade-off of elaborate social grooming was weaker than a trade-off of lightweight social grooming. on a other hand, a time and effort of elaborate methods are higher than lightweight methods. additionally, my model connects social grooming behaviour and social relationship forms with these trade-offs. by analyzing a model, i show that individuals tend to use elaborate social grooming to reinforce the few close relationships (e.g. face to face and chacma baboons). inside contrast, people tend to use lightweight social grooming to maintain many weak relationships (e.g. sns). humans with lightweight methods who live inside significantly complex societies use various social grooming to effectively construct social relationships."
"the full account of galaxy evolution inside a context of lcdm cosmology requires measurements of a average star-formation rate (sfr) and cold gas abundance across cosmic time. emission from a co ladder traces cold gas, and [cii] fine structure emission at 158 um traces a sfr. intensity mapping surveys a cumulative surface brightness of emitting lines as the function of redshift, rather than individual galaxies. cmb spectral distortion instruments are sensitive to both a mean and anisotropy of a intensity of redshifted co and [cii] emission. large-scale anisotropy was proportional to a product of a mean surface brightness and a line luminosity-weighted bias. a bias provides the connection between galaxy evolution and its cosmological context, and was the unique asset of intensity mapping. cross-correlation with galaxy redshift surveys allows unambiguous measurements of redshifted line brightness despite residual continuum contamination and interlopers. measurement of line brightness through cross-correlation also evades cosmic variance and suggests new observation strategies. galactic foreground emission was $\sim 10^3$ times larger than a expected signals, and this places stringent requirements on instrument calibration and stability. under the range of assumptions, the linear combination of bands cleans continuum contamination sufficiently that residuals produce the modest penalty over a instrumental noise. considering pixie, a $2 \sigma$ sensitivity to co and [cii] emission scales from $\sim 5 \times 10^{-2}$ kjy/sr at low redshift to ~2 kjy/sr by reionization."
"this paper introduces the novel real-time fuzzy supervised learning with binary meta-feature (fsl-bm) considering big data classification task. a study of real-time algorithms addresses several major concerns, which are namely: accuracy, memory consumption, and ability to stretch assumptions and time complexity. attaining the fast computational model providing fuzzy logic and supervised learning was one of a main challenges inside a machine learning. inside this research paper, we present fsl-bm algorithm as an efficient solution of supervised learning with fuzzy logic processing with the help of binary meta-feature representation with the help of hamming distance and hash function to relax assumptions. while many studies focused on reducing time complexity and increasing accuracy during a last decade, a novel contribution of this proposed solution comes through integration of hamming distance, hash function, binary meta-features, binary classification to provide real time supervised method. hash tables (ht) component gives the fast access to existing indices; and therefore, a generation of new indices inside the constant time complexity, which supersedes existing fuzzy supervised algorithms with better or comparable results. to summarize, a main contribution of this technique considering real-time fuzzy supervised learning was to represent hypothesis through binary input as meta-feature space and creating a fuzzy supervised hash table to train and validate model."
"unsupervised learning was of growing interest because it unlocks a potential held inside vast amounts of unlabelled data to learn useful representations considering inference. autoencoders, the form of generative model, may be trained by learning to reconstruct unlabelled input data from the latent representation space. more robust representations may be produced by an autoencoder if it learns to recover clean input samples from corrupted ones. representations may be further improved by introducing regularisation during training to shape a distribution of a encoded data inside latent space. we suggest denoising adversarial autoencoders, which combine denoising and regularisation, shaping a distribution of latent space with the help of adversarial training. we introduce the novel analysis that shows how denoising may be incorporated into a training and sampling of adversarial autoencoders. experiments are performed to assess a contributions that denoising makes to a learning of representations considering classification and sample synthesis. our results suggest that autoencoders trained with the help of the denoising criterion achieve higher classification performance, and should synthesise samples that are more consistent with a input data than those trained without the corruption process."
"an idea behind the method was proposed to design a intelligent electrode position controller considering uhp by with the help of nonlinear scaling and fuzzy self tuning pid control algorithm. first, nonlinear scaling of controlled variable that compensate a nonlinearity of a object was proposed. second, the fuzzy self tuning pid electrode position control algorithm was designed and a parameters of a fuzzy inference are optimized by with the help of ga (genetic algorithm). finally, a effectiveness of a proposed idea behind the method was verified by field test."
"inside this paper we study a family of prime irreducible representations of quantum affine $\lie{sl}_{n+1}$ which arise from a work of d. hernandez and b. leclerc. these representations should also be described as follows: a highest weight was the product of distinct fundamental weights with parameters determined by requiring that a representation be minimal by parts. we show that such representations admit the bgg-type resolution where a role of a verma module was played by a local weyl module. this leads to the closed formula (the weyl character formula) considering a character of a irreducible representation as an alternating sum of characters of local weyl modules. inside a language of cluster algebras our weyl character formula describes an arbitrary cluster variable inside terms of a generators $x_1,\cdots,x_n,x_1',\cdots, x_n'$ of an appropriate cluster algebra. our results also exhibit a character of the prime level two demazure module as an alternating linear combination of level one demazure modules."
"a latest measurements of cmb electron scattering optical depth reported by planck significantly reduces a allowed space of hi reionization models, pointing towards the later ending and/or less extended phase transition than previously believed. reionization impulsively heats a intergalactic medium (igm) to $\sim10^4$ k, and owing to long cooling and dynamical times inside a diffuse gas, comparable to a hubble time, memory of reionization heating was retained. therefore, the late ending reionization has significant implications considering a structure of a $z\sim5-6$ lyman-$\alpha$ (ly$\alpha$) forest. with the help of state-of-the-art hydrodynamical simulations that allow us to vary a timing of reionization and its associated heat injection, we argue that extant thermal signatures from reionization should be detected using a ly$\alpha$ forest power spectrum at $5< z<6$. this arises because a small-scale cutoff inside a power depends not only a a igms temperature at these epochs, but was also particularly sensitive to a pressure smoothing scale set by a igms full thermal history. comparing our different reionization models with existing measurements of a ly$\alpha$ forest flux power spectrum at $z=5.0-5.4$, we find that models satisfying planck's $\tau_e$ constraint, favor the moderate amount of heat injection consistent with galaxies driving reionization, but disfavoring quasar driven scenarios. we explore a impact of different reionization histories and heating models on a shape of a power spectrum, and find that they should produce similar effects, but argue that this degeneracy should be broken with high enough quality data. we study a feasibility of measuring a flux power spectrum at $z\simeq 6$ with the help of mock quasar spectra and conclude that the sample of $\sim10$ high-resolution spectra with attainable s/n ratio will allow to discriminate between different reionization scenarios."
"recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures considering vanishing gradients. trained only on sequences from the known grammar, though, they should still struggle to learn rules and constraints of a grammar. neural attribute machines (nams) are equipped with the logical machine that represents a underlying grammar, which was used to teach a constraints to a neural machine by (i) augmenting a input sequence, and (ii) optimizing the custom loss function. unlike traditional rnns, nams are exposed to a grammar, as well as samples from a language of a grammar. during generation, nams make significantly fewer violations of a constraints of a underlying grammar than rnns trained only on samples from a language of a grammar."
"while most machine translation systems to date are trained on large parallel corpora, humans learn language inside the different way: by being grounded inside an environment and interacting with other humans. inside this work, we propose the communication game where two agents, native speakers of their own respective languages, jointly learn to solve the visual referential task. we find that a ability to understand and translate the foreign language emerges as the means to achieve shared goals. a emergent translation was interactive and multimodal, and crucially does not require parallel corpora, but only monolingual, independent text and corresponding images. our proposed translation model achieves this by grounding a source and target languages into the shared visual modality, and outperforms several baselines on both word-level and sentence-level translation tasks. furthermore, we show that agents inside the multilingual community learn to translate better and faster than inside the bilingual communication setting."
"we train the generator by maximum likelihood and we also train a same generator architecture by wasserstein gan. we then compare a generated samples, exact log-probability densities and approximate wasserstein distances. we show that an independent critic trained to approximate wasserstein distance between a validation set and a generator distribution helps detect overfitting. finally, we use ideas from a one-shot learning literature to develop the novel fast learning critic."
"many application settings involve a analysis of timestamped relations or events between the set of entities, e.g. messages between users of an on-line social network. static and discrete-time network models are typically used as analysis tools inside these settings; however, they discard the significant amount of information by aggregating events over time to form network snapshots. inside this paper, we introduce the block point process model (bppm) considering dynamic networks evolving inside continuous time inside a form of events at irregular time intervals. a bppm was inspired by a well-known stochastic block model (sbm) considering static networks and was the simpler version of a recently-proposed hawkes infinite relational model (irm). we show that networks generated by a bppm follow an sbm inside a limit of the growing number of nodes and leverage this property to develop an efficient inference procedure considering a bppm. we fit a bppm to several real network data sets, including the facebook network with over 3, 500 nodes and 130, 000 events, several orders of magnitude larger than a hawkes irm and other existing point process network models."
"machine learning (ml) was one of a most exciting and dynamic areas of modern research and application. a purpose of this review was to provide an introduction to a core concepts and tools of machine learning inside the manner easily understood and intuitive to physicists. a review begins by covering fundamental concepts inside ml and modern statistics such as a bias-variance tradeoff, overfitting, regularization, and generalization before moving on to more advanced topics inside both supervised and unsupervised learning. topics covered inside a review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including maxent models and restricted boltzmann machines), and variational methods. throughout, we emphasize a many natural connections between ml and statistical physics. the notable aspect of a review was a use of python notebooks to introduce modern ml/statistical packages to readers with the help of physics-inspired datasets (the ising model and monte-carlo simulations of supersymmetric decays of proton-proton collisions). we conclude with an extended outlook discussing possible uses of machine learning considering furthering our understanding of a physical world as well as open problems inside ml where physicists maybe able to contribute. (notebooks are available at this https url )"
"we present an image-based method considering comparing a structural properties of galaxies produced inside hydrodynamical simulations to real galaxies inside a sloan digital sky survey. a key feature of our work was a introduction of extensive observational realism, such as object crowding, noise and viewing angle, to a synthetic images of simulated galaxies, so that they should be fairly compared to real galaxy catalogs. we apply our methodology to a dust-free synthetic image catalog of galaxies from a illustris simulation at $z=0$, which are then fit with bulge+disc models to obtain morphological parameters. inside this first paper inside the series, we detail our methods, quantify observational biases, and present publicly available bulge+disc decomposition catalogs. we find that our bulge+disc decompositions are largely robust to a observational biases that affect decompositions of real galaxies. however, we identify the significant population of galaxies (roughly 30\% of a full sample) inside illustris that are prone to internal segmentation, leading to systematically reduced flux estimates by up to the factor of 6, smaller half-light radii by up to the factor of $\sim$ 2, and generally erroneous bulge-to-total fractions of (b/t)=0."
"bipartite networks manifest as the stream of edges that represent transactions, e.g., purchases by retail customers. many machine learning applications employ neighborhood-based measures to characterize a similarity among a nodes, such as a pairwise number of common neighbors (cn) and related metrics. while a number of node pairs that share neighbors was potentially enormous, only the relatively small proportion of them have many common neighbors. this motivates finding the weighted sampling idea behind the method to preferentially sample these node pairs. this paper presents the new sampling algorithm that provides the fixed size unbiased approximate of a similarity matrix resulting from the bipartite graph stream projection. a algorithm has two components. first, it maintains the reservoir of sampled bipartite edges with sampling weights that favor selection of high similarity nodes. second, arriving edges generate the stream of \textsl{similarity updates} based on their adjacency with a current sample. these updates are aggregated inside the second reservoir sample-based stream aggregator to yield a final unbiased estimate. experiments on real world graphs show that the 10% sample at each stage yields estimates of high similarity edges with weighted relative errors of about 1%."
"two fundamental problems considering extraterrestrial intelligences (etis) attempting to establish interstellar communication are timing and energy consumption. humanity's study of exoplanets using their transit across a host star highlights the means of solving both problems. an eti 'a' should communicate with eti 'b' if b was observing transiting planets inside a's star system, either by building structures to produce artificial transits observable by b, or by emitting signals at b during transit, at significantly lower energy consumption than typical electromagnetic transmission schemes. this should produce the network of interconnected civilisations, establishing contact using observing each other's transits. assuming that civilisations reside inside the galactic habitable zone (ghz), i conduct monte carlo realisation simulations of a establishment and growth of this network, and analyse its properties inside a context of graph theory. i find that at any instant, only the few civilisations are correctly aligned to communicate using transits. however, we should expect a true network to be cumulative, where the ""handshake"" connection at any time guarantees connection inside a future using e.g. electromagnetic signals. inside all our simulations, a cumulative network connects all civilisations together inside the complete network. if civilisations share knowledge of their network connections, a network should be fully complete on timescales of order the hundred thousand years. once established, this network should connect any two civilisations either directly, or using intermediate civilisations, with the path much less than a dimensions of a ghz."
"we report on the systematic search considering oxygen-bearing complex organic molecules (coms) inside a solar-like protostellar shock region l1157-b1, as part of a iram large program ""astrochemical surveys at iram"" (asai). several coms are unambiguously detected, some considering a first time, such as ketene h$_2$cco, dimethyl ether (ch$_3$och$_3$) and glycolaldehyde (hcoch$_2$oh), and others firmly confirmed, such as formic acid (hcooh) and ethanol (c$_2$h$_5$oh). thanks to a high sensitivity of a observations and full coverage of a 1, 2 and 3mm wavelength bands, we detected numerous (10--125) lines from each of a detected species. based on the simple rotational diagram analysis, we derive a excitation conditions and a column densities of a detected coms. combining our new results with those previously obtained towards other protostellar objects, we found the good correlation between ethanol, methanol and glycolaldehyde. we discuss a implications of these results on a possible formation routes of ethanol and glycolaldehyde."
"constraining a dark energy (de) equation of state, w, was one of a primary science goals of ongoing and future cosmological surveys. inside practice, with imperfect data and incomplete redshift coverage, this requires making assumptions about a evolution of w with redshift z. these assumptions should be manifested inside the choice of the specific parametric form, which should potentially bias a outcome, or else one should reconstruct w(z) non-parametrically, by specifying the prior covariance matrix that correlates values of w at different redshifts. inside this work, we derive a theoretical prior covariance considering a effective de equation of state predicted by general scalar-tensor theories with second order equations of motion (horndeski theories). this was achieved by generating the large ensemble of possible scalar-tensor theories with the help of the monte carlo methodology, including a application of physical viability conditions. we also separately consider a special sub-case of a minimally coupled scalar field, or quintessence. a prior shows the preference considering tracking behaviors inside a most general case. given a covariance matrix, theoretical priors on parameters of any specific parametrization of w(z) should also be readily derived by projection."
"large scholar networks was quite popular inside a academic domain, like aminer. it offers to display a academic social network, including profile search, expert finding, conference analysis, course search, sub-graph search, topic browser, academic ranks and user management. usually a search results are listed as items, while a relations among them are hidden to a users. visualization was the feasible way to aid users explore a hidden relations and discover more useful information. this article aim to visualize a search results inside aminer inside the more user-friendly way and aid them better utilize a tool. we provided three different designs to visualize a results and tested them inside user study. a empirical results of our research show that a designed graphs aid users better understand a area they intend to know and make their search more effective."
"inside this work, we propose an end-to-end deep architecture that jointly learns to detect obstacles and approximate their depth considering mav flight applications. most of a existing approaches either rely on visual slam systems or on depth approximation models to build 3d maps and detect obstacles. however, considering a task of avoiding obstacles this level of complexity was not required. recent works have proposed multi task architectures to both perform scene understanding and depth estimation. we follow their track and propose the specific architecture to jointly approximate depth and obstacles, without a need to compute the global map, but maintaining compatibility with the global slam system if needed. a network architecture was devised to exploit a joint information of a obstacle detection task, that produces more reliable bounding boxes, with a depth approximation one, increasing a robustness of both to scenario changes. we call this architecture j-mod$^{2}$. we test a effectiveness of our idea behind the method with experiments on sequences with different appearance and focal lengths and compare it to sota multi task methods that jointly perform semantic segmentation and depth estimation. inside addition, we show a integration inside the full system with the help of the set of simulated navigation experiments where the mav explores an unknown scenario and plans safe trajectories by with the help of our detection model."
"recently proposed models which learn to write computer programs from data use either input/output examples or rich execution traces. instead, we argue that the novel alternative was to use the glass-box loss function, given as the program itself that should be directly inspected. glass-box optimization covers the wide range of problems, from computing a greatest common divisor of two integers, to learning-to-learn problems. inside this paper, we present an intelligent search system which learns, given a partial program and a glass-box problem, a probabilities over a space of programs. we empirically demonstrate that our informed search procedure leads to significant improvements compared to brute-force program search, both inside terms of accuracy and time. considering our experiments we use rich context free grammars inspired by number theory, text processing, and algebra. our results show that (i) performing 4 rounds of our framework typically solves about 70% of a target problems, (ii) our framework should improve itself even inside domain agnostic scenarios, and (iii) it should solve problems that would be otherwise too slow to solve with brute-force search."
"this paper describes an english audio and textual dataset of debating speeches, the unique resource considering a growing research field of computational argumentation and debating technologies. we detail a process of speech recording by professional debaters, a transcription of a speeches with an automatic speech recognition (asr) system, their consequent automatic processing to produce the text that was more ""nlp-friendly"", and inside parallel -- a manual transcription of a speeches inside order to produce gold-standard ""reference"" transcripts. we release 60 speeches on various controversial topics, each inside five formats corresponding to a different stages inside a production of a data. a intention was to allow utilizing this resource considering multiple research purposes, be it a addition of in-domain training data considering the debate-specific asr system, or applying argumentation mining on either noisy or clean debate transcripts. we intend to make further releases of this data inside a future."
"distributional approximations of (bi--) linear functions of sample variance-covariance matrices play the critical role to analyze vector time series, as they are needed considering various purposes, especially to draw inference on a dependence structure inside terms of second moments and to analyze projections onto lower dimensional spaces as those generated by principal components. this particularly applies to a high-dimensional case, where a dimension $d$ was allowed to grow with a sample size $n$ and may even be larger than $n$. we establish large-sample approximations considering such bilinear forms related to a sample variance-covariance matrix of the high-dimensional vector time series inside terms of strong approximations by brownian motions. a results cover weakly dependent as well as many long-range dependent linear processes and are valid considering uniformly $ \ell_1 $-bounded projection vectors, which arise, either naturally or by construction, inside many statistical problems extensively studied considering high-dimensional series. among those problems are sparse financial portfolio selection, sparse principal components, a lasso, shrinkage approximation and change-point analysis considering high--dimensional time series, which matter considering a analysis of big data and are discussed inside greater detail."
"herein, we report a magneto-transport and exchange bias effect inside the ""314 - type"" oxygen - vacancy ordered material with composition srco$_{0.85}$fe$_{0.15}$o$_{2.62}$. this material exhibits the ferrimagnetic transition above room temperature, at 315 k. a negative magnetoresistance starts to appear from room temperature (-1.3 $\%$ at 295 k inside 70 koe) and reaches the sizable value of 58 $\%$ at 4 k inside 70 koe. large exchange bias effect was observed below 315 k when a sample was cooled inside a presence of the magnetic field. a coexistence of nearly compensated and ferrimagnetic regions inside a layered structure originate magnetoresistance and exchange bias inside this sample. a appearance of the sizable magnetoresistance and giant exchange bias effect, especially near room temperature indicates that ""314-type"" cobaltates are the promising class of material systems considering a exploration of materials with potential applications as magnetic sensors or inside a area of spintronics."
"we introduce parseval networks, the form of deep neural networks inside which a lipschitz constant of linear, convolutional and aggregation layers was constrained to be smaller than 1. parseval networks are empirically and theoretically motivated by an analysis of a robustness of a predictions made by deep neural networks when their input was subject to an adversarial perturbation. a most important feature of parseval networks was to maintain weight matrices of linear and convolutional layers to be (approximately) parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. we describe how these constraints should be maintained efficiently during sgd. we show that parseval networks match a state-of-the-art inside terms of accuracy on cifar-10/100 and street view house numbers (svhn) while being more robust than their vanilla counterpart against adversarial examples. incidentally, parseval networks also tend to train faster and make the better usage of a full capacity of a networks."
"inside this paper, we study the new learning paradigm considering neural machine translation (nmt). instead of maximizing a likelihood of a human translation as inside previous works, we minimize a distinction between human translation and a translation given by an nmt model. to achieve this goal, inspired by a recent success of generative adversarial networks (gans), we employ an adversarial training architecture and name it as adversarial-nmt. inside adversarial-nmt, a training of a nmt model was assisted by an adversary, which was an elaborately designed convolutional neural network (cnn). a goal of a adversary was to differentiate a translation result generated by a nmt model from that by human. a goal of a nmt model was to produce high quality translations so as to cheat a adversary. the policy gradient method was leveraged to co-train a nmt model and a adversary. experimental results on english$\rightarrow$french and german$\rightarrow$english translation tasks show that adversarial-nmt should achieve significantly better translation quality than several strong baselines."
"a existence of a $\imath$-canonical basis (also known as a $\imath$-divided powers) considering a coideal subalgebra of a quantum $\mathfrak{sl}_2$ were established by bao and wang, with conjectural explicit formulae. inside this paper we prove a conjectured formulae of these $\imath$-divided powers. this was achieved by first establishing closed formulae of a $\imath$-divided powers inside basis considering quantum $\mathfrak{sl}_2$ and then formulae considering a $\imath$-canonical basis inside terms of lusztig's divided powers inside each finite-dimensional simple module of quantum $\mathfrak{sl}_2$. these formulae exhibit integrality and positivity properties."
"with a aid of first principles calculation method based on a density functional theory we have investigated a structural, elastic, mechanical properties and debye temperature of fe2scm (m = p and as) compounds under pressure up to 60 gpa. a optical properties have been investigated under zero pressure. our calculated optimized structural parameters of both a compounds are inside good agreement with a other theoretical results. a calculated elastic constants show that fe2scm (m = p and as) compounds are mechanically stable up to 60 gpa."
"panel data analysis was an important topic inside statistics and econometrics. traditionally, inside panel data analysis, all individuals are assumed to share a same unknown parameters, e.g. a same coefficients of covariates when a linear models are used, and a differences between a individuals are accounted considering by cluster effects. this kind of modelling only makes sense if our main interest was on a global trend, this was because it would not be able to tell us anything about a individual attributes which are sometimes very important. inside this paper, we proposed the modelling based on a single index models embedded with homogeneity considering panel data analysis, which builds a individual attributes inside a model and was parsimonious at a same time. we develop the data driven idea behind the method to identify a structure of homogeneity, and approximate a unknown parameters and functions based on a identified structure. asymptotic properties of a resulting estimators are established. intensive simulation studies conducted inside this paper also show a resulting estimators work very well when sample size was finite. finally, a proposed modelling was applied to the public financial dataset and the uk climate dataset, a results reveal some interesting findings."
"we revisit our construction of mirror symmetries considering compactifications of type ii superstrings on twisted connected sum $g_2$ manifolds. considering the given $g_2$ manifold, we discuss evidence considering a existence of mirror symmetries of two kinds: one was an autoequivalence considering the given type ii superstring on the mirror pair of $g_2$ manifolds, a other was the duality between type ii strings with different chiralities considering another pair of mirror manifolds. we clarify a role of a b-field inside a construction, and check that a corresponding massless spectra are respected by a generalized mirror maps. we discuss hints towards the homological version based on bps spectroscopy. we provide several novel examples of smooth, as well as singular, mirror $g_2$ backgrounds using pairs of dual projecting tops. we test our conjectures against the joyce orbifold example, where we reproduce, with the help of our geometrical methods, a known mirror maps that arise from a scft worldsheet perspective. along a way, we discuss non-abelian gauge symmetries, and argue considering a generation of a affleck-harvey-witten superpotential inside a pure sym case."
"a bilinear assignment problem (bap) was the generalization of a well-known quadratic assignment problem (qap). inside this paper, we study a problem from a computational analysis point of view. several classes of neigborhood structures are introduced considering a problem along with some theoretical analysis. these neighborhoods are then explored within the local search and the variable neighborhood search frameworks with multistart to generate robust heuristic algorithms. results of systematic experimental analysis have been presented which divulge a effectiveness of our algorithms. inside addition, we present several very fast construction heuristics. our experimental results disclosed some interesting properties of a bap model, different from those of comparable models. this was a first thorough experimental analysis of algorithms on bap. we have also introduced benchmark test instances that should be used considering future experiments on exact and heuristic algorithms considering a problem."
"a ability to learn at different resolutions inside time may aid overcome one of a main challenges inside deep reinforcement learning -- sample efficiency. hierarchical agents that operate at different levels of temporal abstraction should learn tasks more quickly because they should divide a work of learning behaviors among multiple policies and should also explore a environment at the higher level. inside this paper, we present the novel idea behind the method to hierarchical reinforcement learning called hierarchical actor-critic (hac) that enables agents to learn to break down problems involving continuous action spaces into simpler subproblems belonging to different time scales. hac has two key advantages over most existing hierarchical learning methods: (i) a potential considering faster learning as agents learn short policies at each level of a hierarchy and (ii) an end-to-end approach. we demonstrate that hac significantly accelerates learning inside the series of tasks that require behavior over the relatively long time horizon and involve sparse rewards."
"motivated by a existence of hierarchies of structure inside a universe, we present four new families of exact initial data considering inhomogeneous cosmological models at their maximum of expansion. these data generalise existing black hole lattice models to situations that contain clusters of masses, and thus allow a consequences of cosmological structures to be considered inside the well-defined and non-perturbative fashion. a degree of clustering was controlled by the parameter $\lambda$, inside such the way that considering $\lambda \sim 0$ or $1$ we have very tightly clustered masses, whilst considering $\lambda \sim 0.5$ all masses are separated by cosmological distance scales. we study a consequences of structure formation on a total net mass inside each of our clusters, as well as calculating a cosmological consequences of a interaction energies both within and between clusters. a locations of a shared horizons that appear around groups of black holes, when they are brought sufficiently close together, are also identified and studied. we find that clustering should have surprisingly large effects on a scale of a cosmology, with models that contain thousands of black holes sometimes being as little as 30% of a size of comparable friedmann models with a same total proper mass. this deficit was comparable to what might be expected to occur from neglecting gravitational interaction energies inside friedmann cosmology, and suggests that these quantities may have the significant influence on a properties of a large-scale cosmology."
"we study a fundamental question of a lattice dynamics of the metallic ferromagnet inside a regime where a static long range magnetic order was replaced by a fluctuating local moments embedded inside the metallic host. we use a \textit{ab initio} density functional theory(dft)+embedded dynamical mean-field theory(edmft) functional idea behind the method to address a dynamic stability of iron polymorphs and a phonon softening with increased temperature. we show that a non-harmonic and inhomogeneous phonon softening measured inside iron was the result of a melting of a long range ferromagnetic order, and was unrelated to a first order structural transition from a bcc to a fcc phase, as was usually assumed. we predict that a bcc structure was dynamically stable at all temperatures at normal pressure, and was only thermodynamically unstable between a bcc-$\alpha$ and a bcc-$\delta$ phase of iron."
"inside recent years, the great deal of interest has focused on conducting inference on a parameters inside the linear model inside a high-dimensional setting. inside this paper, we consider the simple and very naïve two-step procedure considering this task, inside which we (i) fit the lasso model inside order to obtain the subset of a variables; and (ii) fit the least squares model on a lasso-selected set. conventional statistical wisdom tells us that we cannot make use of a standard statistical inference tools considering a resulting least squares model (such as confidence intervals and $p$-values), since we peeked at a data twice: once inside running a lasso, and again inside fitting a least squares model. however, inside this paper, we show that under the certain set of assumptions, with high probability, a set of variables selected by a lasso was deterministic. consequently, a naïve two-step idea behind the method should yield confidence intervals that have asymptotically correct coverage, as well as p-values with proper type-i error control. furthermore, this two-step idea behind the method unifies two existing camps of work on high-dimensional inference: one camp has focused on inference based on the sub-model selected by a lasso, and a other has focused on inference with the help of the debiased version of a lasso estimator."
"there are many classical problems inside p whose time complexities have not been improved over a past decades. recent studies of ""hardness inside p"" have revealed that, considering several of such problems, a current fastest algorithm was a best possible under some complexity assumptions. to bypass this difficulty, fomin et al. (soda 2017) introduced a concept of fully polynomial fpt algorithms. considering the problem with a current best time complexity $o(n^c)$, a goal was to design an algorithm running inside $k^{o(1)}n^{c'}$ time considering the parameter $k$ and the constant $c'<c$. inside this paper, we investigate a complexity of graph problems inside p parameterized by tree-depth, the graph parameter related to tree-width. we show that the simple divide-and-conquer method should solve many graph problems, including weighted matching, negative cycle detection, minimum weight cycle, replacement paths, and 2-hop cover, inside $o(\mathrm{td}\cdot m)$ time or $o(\mathrm{td}\cdot (m+n\log n))$ time, where $\mathrm{td}$ was a tree-depth of a input graph. because any graph of tree-width $\mathrm{tw}$ has tree-depth at most $(\mathrm{tw}+1)\log_2 n$, our algorithms also run inside $o(\mathrm{tw}\cdot m\log n)$ time or $o(\mathrm{tw}\cdot (m+n\log n)\log n)$ time. these results match or improve a previous best algorithms parameterized by tree-width. especially, we solve an open problem of fully polynomial fpt algorithm considering weighted matching parameterized by tree-width posed by fomin et al."
"point source detection at low signal-to-noise was challenging considering astronomical surveys, particularly inside radio interferometry images where a noise was correlated. machine learning was the promising solution, allowing a development of algorithms tailored to specific telescope arrays and science cases. we present deepsource - the deep learning solution - that uses convolutional neural networks to achieve these goals. deepsource enhances a signal-to-noise ratio (snr) of a original map and then uses dynamic blob detection to detect sources. trained and tested on two sets of 500 simulated 1 deg x 1 deg meerkat images with the total of 300,000 sources, deepsource was essentially perfect inside both purity and completeness down to snr = 4 and outperforms pybdsf inside all metrics. considering uniformly-weighted images it achieves the purity x completeness (pc) score at snr = 3 of 0.73, compared to 0.31 considering a best pybdsf model. considering natural-weighting we find the smaller improvement of ~40% inside a pc score at snr = 3. if instead we ask where either of a purity or completeness first drop to 90%, we find that deepsource reaches this value at snr = 3.6 compared to a 4.3 of pybdsf (natural-weighting). the key advantage of deepsource was that it should learn to optimally trade off purity and completeness considering any science case under consideration. our results show that deep learning was the promising idea behind the method to point source detection inside astronomical images."
"this work was about recognizing human activities occurring inside videos at distinct semantic levels, including individual actions, interactions, and group activities. a recognition was realized with the help of the two-level hierarchy of long short-term memory (lstm) networks, forming the feed-forward deep architecture, which should be trained end-to-end. inside comparison with existing architectures of lstms, we make two key contributions giving a name to our idea behind the method as confidence-energy recurrent network -- cern. first, instead of with the help of a common softmax layer considering prediction, we specify the novel energy layer (el) considering estimating a energy of our predictions. second, rather than finding a common minimum-energy class assignment, which may be numerically unstable under uncertainty, we specify that a el additionally computes a p-values of a solutions, and inside this way estimates a most confident energy minimum. a evaluation on a collective activity and volleyball datasets demonstrates: (i) advantages of our two contributions relative to a common softmax and energy-minimization formulations and (ii) the superior performance relative to a state-of-the-art approaches."
"we study how the minimal generalization of einstein's equations, where a speed of light ($c$), gravitational constant ($g$) and a cosmological constant ($\lambda$) are allowed to vary, might generate the dynamical mechanism to explain a special initial condition necessary to obtain a homogeneous and flat universe we observe today. our construction preserves general covariance of a theory, which yields the general dynamical constraint inside $c$, $g$ and $\lambda$. we re-write a conditions necessary inside order to solve a horizon and flatness problems inside this framework. this was given by a shrinking of a comoving particle horizon of this theory which leads to $\omega < -1/3$, but not necessarily to accelerated expansion like inside inflation, allowing also the decelerated expansion, contraction and the phase transition inside $c$, inside a case of null $\lambda$. we are able to construct a action of this theory, that describes a dynamics of the scalar field that represents $c$ or $g$ (and $\lambda$). this action was general and should be applied to describe different cosmological solutions. we present here how a dynamics of a field should be used to solve a problems of a early universe cosmology by means of different ways to c-inflate a horizon inside a early universe, solving a old puzzles of a cosmological standard model. without the cosmological constant, we show that we should describe a dynamics of a scalar field representing $c$ given the potential, and derive a slow-roll conditions that this potential should obey. inside this setup we do not have to introduce an extra unknown scalar field, since a degree of freedom associated to a varying constants plays this role, naturally being a field that was going to be responsible considering inflating a horizon inside a early universe."
"learning has propelled a cutting edge of performance inside robotic control to new heights, allowing robots to operate with high performance inside conditions that were previously unimaginable. a majority of a work, however, assumes that a unknown parts are static or slowly changing. this limits them to static or slowly changing environments. however, inside a real world, the robot may experience various unknown conditions. this paper presents the method to extend an existing single mode gp-based safe learning controller to learn an increasing number of non-linear models considering a robot dynamics. we show that this idea behind the method enables the robot to re-use past experience from the large number of previously visited operating conditions, and to safely adapt when the new and distinct operating condition was encountered. this allows a robot to achieve safety and high performance inside an large number of operating conditions that do not have to be specified ahead of time. our idea behind the method runs independently from a controller, imposing no additional computation time on a control loop regardless of a number of previous operating conditions considered. we demonstrate a effectiveness of our idea behind the method inside experiment on the 900\,kg ground robot with both physical and artificial changes to its dynamics. all of our experiments are conducted with the help of vision considering localization."
"we start from the variational model considering nematic elastomers that involves two energies: mechanical and nematic. a first one consists of the nonlinear elastic energy which was influenced by a orientation of a molecules of a nematic elastomer. a nematic energy was an oseen--frank energy inside a deformed configuration. a constraint of a positivity of a determinant of a deformation gradient was imposed. a functionals are not assumed to have a usual polyconvexity or quasiconvexity assumptions to be lower semicontinuous. we instead compute its relaxation, that is, a lower semicontinuous envelope, which turns out to be a quasiconvexification of a mechanical term plus a tangential quasiconvexification of a nematic term. a main assumptions are that a quasiconvexification of a mechanical term was polyconvex and that a deformation was inside a sobolev space $w^{1,p}$ (with $p>n-1$ and $n$ a dimension of a space) and does not present cavitation."
"we describe a mathematical theory of diffusion and heat transport with the view to including some of a main directions of recent research. a linear heat equation was a basic mathematical model that has been thoroughly studied inside a last two centuries. it is followed by a theory of parabolic equations of different types. inside the parallel development, a theory of stochastic differential equations gives the foundation to a probabilistic study of diffusion. nonlinear diffusion equations have played an important role not only inside theory but also inside physics and engineering, and we focus on the relevant aspect, a existence and propagation of free boundaries. we use a porous medium and fast diffusion equations as case examples. the large part of a paper was devoted to diffusion driven by fractional laplacian operators and other nonlocal integro-differential operators representing nonlocal, long-range diffusion effects. three main models are examined (one linear, two nonlinear), and we report on recent progress inside which a author was involved."
"inside january 2015 we distributed an online survey about failures inside robotics and intelligent systems across robotics researchers. a aim of this survey is to find out which types of failures currently exist, what their origins are, and how systems are monitored and debugged - with the special focus on performance bugs. this report summarizes a findings of a survey."
"let $t$ be the $1$-tilting module whose tilting torsion pair $({\mathcal t}, {\mathcal f})$ has a property that a heart ${\mathcal h}_t$ of a induced $t$-structure (in a derived category ${\mathcal d}({\rm mod} \mbox{-} r)$ was grothendieck. it was proved that such tilting torsion pairs are characterized inside several ways: (1) a $1$-tilting module $t$ was pure projective; (2) ${\mathcal t}$ was the definable subcategory of ${\rm mod} \mbox{-} r$ with enough pure projectives, and (3) both classes ${\mathcal t}$ and ${\mathcal f}$ are finitely axiomatizable. this study addresses a question of saorín that asks whether a heart was equivalent to the module category, i.e., whether a pure projective $1$-tilting module was tilting equivalent to the finitely presented module. a answer was positive considering the krull-schmidt ring and considering the commutative ring, every pure projective $1$-tilting module was projective. the criterion was found that yields the negative answer to saorín's question considering the left and right noetherian ring. the negative answer was also obtained considering the dubrovin-puninski ring, whose theory was covered inside a appendix. dubrovin-puninski rings also provide examples of (1) the pure projective $2$-tilting module that was not classical; (2) the finendo quasi-tilting module that was not silting; and (3) the noninjective module $a$ considering which there exists the left almost split morphism $m: the \to b,$ but no almost split sequence beginning with $a.$"
"this paper presents an exhaustive study on a arrivals process at eight important european airports. with the help of inbound traffic data, we define, compare, and contrast the data-driven poisson and psra point process. although, there was sufficient evidence that a interarrivals might follow an exponential distribution, this finding does not directly translate to evidence that a arrivals stream was poisson. a main reason was that finite-capacity constraints impose the correlation structure to a arrivals stream, which the poisson model cannot capture. we show a weaknesses and somehow a difficulties of with the help of the poisson process to model with good approximation a arrivals stream. on a other hand, our innovative non-parametric, data-driven psra model, predicts quite well and captures important properties of a typical arrivals stream."
"we present necessary conditions considering monotonicity, inside one form or another, of fixed point iterations of mappings that violate a usual nonexpansive property. we show that most reasonable notions of linear-type monotonicity of fixed point sequences imply {\em metric subregularity}. this was specialized to a alternating projections iteration where a metric subregularity property takes on the distinct geometric characterization of sets at points of intersection called {\em subtransversality}. our more general results considering fixed point iterations are specialized to establish a necessity of subtransversality considering consistent feasibility with the number of reasonable types of sequential monotonicity, under varying degrees of assumptions on a regularity of a sets."
"multidimensional coherent optical spectroscopy was one of a most powerful tools considering investigating complex quantum mechanical systems. while it is conceived decades ago inside magnetic resonance spectroscopy with the help of micro- and radio-waves, it has recently been extended into a visible and uv spectral range. however, resolving mhz energy splittings with ultrashort laser pulses has still remained the challenge. here, we analyze two-dimensional fourier spectra considering resonant optical excitation of resident electrons to localized trions or donor-bound excitons inside semiconductor nanostructures subject to the transverse magnetic field. particular attention was devoted to raman coherence spectra which allow one to accurately evaluate tiny splittings of a electron ground state and to determine a relaxation times inside a electron spin ensemble. the stimulated step-like raman process induced by the sequence of two laser pulses creates the coherent superposition of a ground state doublet which should be retrieved only optically due to selective excitation of a same sub-ensemble with the third pulse. this provides a unique opportunity to distinguish between different complexes that are closely spaced inside energy inside an ensemble. a related experimental demonstration was based on photon echo measurements inside an n-type cdte/(cd,mg)te quantum well structure detected by the heterodyne technique. a difference inside a sub-$\mu$ev range between a zeeman splittings of donor-bound electrons and electrons localized at potential fluctuations should be resolved even though a homogeneous linewidth of a optical transitions was larger by two orders of magnitude."
"we obtain bounded considering all $t$ solutions of ordinary differential equations as limits of a solutions of a corresponding dirichlet problems on $(-l,l)$, with $l \rightarrow \infty$. we derive the priori estimates considering a dirichlet problems, allowing passage to a limit, using the diagonal sequence. this idea behind the method carries over to a pde case."
"let $s$ be an upper cluster algebra, which was the subalgebra of $r$. suppose that there was some cluster variable $x_e$ such that ${r}_{{x}_e} = s[{x}_e^{\pm 1}]$. we try to understand under which conditions ${r}$ was an upper cluster algebra, and how a quiver of $r$ relates to that of $s$. moreover, if a restriction of $(\delta,w)$ to some subquiver was the cluster model, we give the sufficient condition considering $(\delta,w)$ itself being the cluster model. as an application, we show that a semi-invariant ring of any complete $m$-tuple flags was an upper cluster algebra whose quiver was explicitly given. moreover, a quiver with its rigid potential was the polyhedral cluster model."
"the new idea behind the method was shown, which estimates active grid wake features and enables to generate specific dynamically changing flow fields inside the wind tunnel by means of an active grid. considering example, measurements from free field should be reproduced inside the wind tunnel. here, a idea behind the method was explained and applied. moreover, a idea behind the method was validated with wind tunnel measurements, inside terms of time series and stochastic features. thus, possibilities and limitations become obvious. we conclude that this new active grid wake estimating idea behind the method deepens a knowledge about specific flow modulations and shows new working ranges of active grids."
"we report a temperature-pressure phase diagram of cakfe$_4$as$_4$ established with the help of high pressure electrical resistivity, magnetization and high energy x-ray diffraction measurements up to 6 gpa. with increasing pressure, both resistivity and magnetization data show that a bulk superconducting transition of cakfe$_4$as$_4$ was suppressed and then disappears at $p$ $\gtrsim$ 4 gpa. high pressure x-ray data clearly indicate the phase transition to the collapsed tetragonal phase inside cakfe$_4$as$_4$ under pressure that coincides with a abrupt loss of bulk superconductivity near 4 gpa. a x-ray data, combined with resistivity data, indicate that a collapsed tetragonal transition line was essentially vertical, occuring at 4.0(5) gpa considering temperatures below 150 k. band structure calculations also find the sudden transition to the collapsed tetragonal state near 4 gpa, as as-as bonding takes place across a ca-layer. bonding across a k-layer only occurs considering $p$ $\geq$ 12 gpa. these findings demonstrate the new type of collapsed tetragonal phase inside cakfe$_4$as$_4$: the half-collapsed-tetragonal phase."
"this work extends a elsner & wandelt (2013) iterative method considering efficient, preconditioner-free wiener filtering to cases inside which a noise covariance matrix was dense, but should be decomposed into the sum whose parts are sparse inside convenient bases. a new method, which uses multiple messenger fields, reproduces wiener filter solutions considering test problems, and we apply it to the case beyond a reach of a elsner & wandelt (2013) method. we compute a wiener filter solution considering the simulated cosmic microwave background map that contains spatially-varying, uncorrelated noise, isotropic $1/f$ noise, and large-scale horizontal stripes (like those caused by a atmospheric noise). we discuss simple extensions that should filter contaminated modes or inverse-noise filter a data. these techniques aid to address complications inside a noise properties of maps from current and future generations of ground-based microwave background experiments, like advanced actpol, simons observatory, and cmb-s4."
"we propose the framework considering a linear prediction of the multi-way array (i.e., the tensor) from another multi-way array of arbitrary dimension, with the help of a contracted tensor product. this framework generalizes several existing approaches, including methods to predict the scalar outcome from the tensor, the matrix from the matrix, or the tensor from the scalar. we describe an idea behind the method that exploits a multiway structure of both a predictors and a outcomes by restricting a coefficients to have reduced cp-rank. we propose the general and efficient algorithm considering penalized least-squares estimation, which allows considering the ridge (l_2) penalty on a coefficients. a objective was shown to give a mode of the bayesian posterior, which motivates the gibbs sampling algorithm considering inference. we illustrate a idea behind the method with an application to facial image data. an r package was available at this https url ."
"gradient reconstruction was the key process considering a spatial accuracy and robustness of finite volume method, especially inside industrial aerodynamic applications inside which grid quality affects reconstruction methods significantly. the novel gradient reconstruction method considering cell-centered finite volume scheme was introduced. this method was composed of two successive steps. first, the vertex-based weighted-least-squares procedure was implemented to calculate vertex gradients, and then a cell-centered gradients are calculated by an arithmetic averaging procedure. by with the help of these two procedures, extended stencils are implemented inside a calculations, and a accuracy of gradient reconstruction was improved by a weighting procedure. inside a given test cases, a proposed method was showing improvement on both a accuracy and convergence. furthermore, a method could be extended to a calculation of viscous fluxes."
"a close interplay between superconductivity and antiferromagnetism inside several quantum materials should lead to a appearance of an unusual thermodynamic state inside which both orders coexist microscopically, despite their competing nature. the hallmark of this coexistence state was a emergence of the spin-triplet superconducting gap component, called $\pi$-triplet, which was spatially modulated by a antiferromagnetic wave-vector, reminiscent of the pair-density wave. inside this paper, we investigate a impact of these $\pi$-triplet degrees of freedom on a phase diagram of the system with competing antiferromagnetic and superconducting orders. although we focus on the microscopic two-band model that has been widely employed inside studies of iron pnictides, most of our results follow from the ginzburg-landau analysis, and as such should be applicable to other systems of interest, such as cuprates and heavy fermions. a ginzburg-landau functional reveals not only that a $\pi$-triplet gap amplitude couples tri-linearly with a singlet gap amplitude and a staggered magnetization magnitude, but also that a $\pi$-triplet $d$-vector couples linearly with a magnetization direction. while inside a mean field level this coupling forces a $d$-vector to align parallel or anti-parallel to a magnetization, inside a fluctuation regime it promotes two additional collective modes - the goldstone mode related to a precession of a $d$-vector around a magnetization and the massive mode, related to a relative angle between a two vectors, which was nearly degenerate with the leggett-like mode associated with a phase difference between a singlet and triplet gaps. we also investigate a impact of magnetic fluctuations on a superconducting-antiferromagnetic phase diagram, showing that due to their coupling with a $\pi$-triplet order parameter, a coexistence region was enhanced."
"a europlanet 2020 research infrastructure will include new planetary space weather services (psws) that will extend a concepts of space weather and space situational awareness to other planets inside our solar system and inside particular to spacecraft that voyage through it. psws will make five entirely new toolkits accessible to a research community and to industrial partners planning considering space missions: the general planetary space weather toolkit, as well as three toolkits dedicated to a following key planetary environments: mars, comets, and outer planets. this will give a european planetary science community new methods, interfaces, functionalities and/or plugins dedicated to planetary space weather inside a tools and models available within a partner institutes. it will also create the novel event-diary toolkit aiming at predicting and detecting planetary events like meteor showers and impacts. the variety of tools are available considering tracing propagation of planetary and/or solar events through a solar system and modelling a response of a planetary environment (surfaces, atmospheres, ionospheres, and magnetospheres) to those events. but these tools were not originally designed considering planetary event prediction and space weather applications. psws will provide a additional research and tailoring required to apply them considering these purposes. psws will be to review, test, improve and adapt methods and tools available within a partner institutes inside order to make prototype planetary event and space weather services operational inside europe at a end of a programme. to achieve its objectives psws will use the few tools and standards developed considering a astronomy virtual observatory (vo). this paper gives an overview of a project together with the few illustrations of prototype services based on vo standards and protocols."
"data assimilation was widely used to improve flood forecasting capability, especially through parameter inference requiring statistical information on a uncertain input parameters (upstream discharge, friction coefficient) as well as on a variability of a water level and its sensitivity with respect to a inputs. considering particle filter or ensemble kalman filter, stochastically estimating probability density function and covariance matrices from the monte carlo random sampling requires the large ensemble of model evaluations, limiting their use inside real-time application. to tackle this issue, fast surrogate models based on polynomial chaos and gaussian process should be used to represent a spatially distributed water level inside place of solving a shallow water equations. this study investigates a use of these surrogates to approximate probability density functions and covariance matrices at the reduced computational cost and without a loss of accuracy, inside a perspective of ensemble-based data assimilation. this study focuses on 1-d steady state flow simulated with mascaret over a garonne river (south-west france). results show that both surrogates feature similar performance to a monte-carlo random sampling, but considering the much smaller computational budget; the few mascaret simulations (on a order of 10-100) are sufficient to accurately retrieve covariance matrices and probability density functions all along a river, even where a flow dynamic was more complex due to heterogeneous bathymetry. this paves a way considering a design of surrogate strategies suitable considering representing unsteady open-channel flows inside data assimilation."
"investigating dispersion surface morphology of sonic metamaterials was crucial inside providing information on related phenomena as inertial coupling, acoustic transparency, polarisation, and absorption. inside a present study, we look into frequency surface morphology of two-dimensional metamaterials of k3,3 and k6 topologies. a elastic structures under consideration consist of a same substratum lattice points and form the pair of sublattices with hexagonal symmetry. we show that, through introducing universal localised mass-in-mass phononic microstructures at lattice points, six single optical frequency-surfaces should be formed with required properties including negative group velocity. splitting a frequency-surfaces was based on a classical analog of a quantum phenomenon of ""energy-level repulsion"", which should be achieved only through internal anisotropy of a nodes and allows us to obtain different frequency band gaps."
"this paper presents the novel idea behind the method considering multi-lingual sentiment classification inside short texts. this was the challenging task as a amount of training data inside languages other than english was very limited. previously proposed multi-lingual approaches typically require to establish the correspondence to english considering which powerful classifiers are already available. inside contrast, our method does not require such supervision. we leverage large amounts of weakly-supervised data inside various languages to train the multi-layer convolutional network and demonstrate a importance of with the help of pre-training of such networks. we thoroughly evaluate our idea behind the method on various multi-lingual datasets, including a recent semeval-2016 sentiment prediction benchmark (task 4), where we achieved state-of-the-art performance. we also compare a performance of our model trained individually considering each language to the variant trained considering all languages at once. we show that a latter model reaches slightly worse - but still acceptable - performance when compared to a single language model, while benefiting from better generalization properties across languages."
"this note refers to our previous paper ""the emergence of torsion inside a continuum limit of distributed edge-dislocations"". it identifies and fixes an error inside a notion of convergence of weitzenböck manifolds defined inside a paper, and inside a proof of a well-definiteness of this notion of convergence."
"a consistency of doubly robust estimators relies on consistent approximation of at least one of two nuisance regression parameters. inside moderate to large dimensions, a use of flexible data-adaptive regression estimators may aid inside achieving this consistency. however, $n^{1/2}$-consistency of doubly robust estimators was not guaranteed if one of a nuisance estimators was inconsistent. inside this paper we present the doubly robust estimator considering survival analysis with a novel property that it converges to the gaussian variable at $n^{1/2}$-rate considering the large class of data-adaptive estimators of a nuisance parameters, under a only assumption that at least one of them was consistently estimated at the $n^{1/4}$-rate. this result was achieved through adaptation of recent ideas inside semiparametric inference, which amount to: (i) gaussianizing (i.e., making asymptotically linear) the drift term that arises inside a asymptotic analysis of a doubly robust estimator, and (ii) with the help of cross-fitting to avoid entropy conditions on a nuisance estimators. we present a formula of a asymptotic variance of a estimator, which allows computation of doubly robust confidence intervals and p-values. we illustrate a finite-sample properties of a estimator inside simulation studies, and demonstrate its use inside the phase iii clinical trial considering estimating a effect of the novel therapy considering a treatment of her2 positive breast cancer."
"we report on angle-dependent measurements of a sheet resistances and hall coefficients of electron liquids inside smtio3/srtio3/smtio3 quantum well structures, which were grown by molecular beam epitaxy on (001) dysco3. we compare their transport properties with those of similar structures grown on lsat [(la0.3sr0.7)(al0.65ta0.35)o3]. on dysco3, planar defects normal to a quantum wells lead to the strong in-plane anisotropy inside a transport properties. this allows considering quantifying a role of defects inside transport. inside particular, we investigate differences inside a longitudinal and hall scattering rates, which was the non-fermi liquid phenomenon known as lifetime separation. a residuals inside both a longitudinal resistance and hall angle were found to depend on a relative orientations of a transport direction to a planar defects. a hall angle exhibited the robust t2 temperature dependence along all directions, whereas no simple power law could describe a temperature dependence of a longitudinal resistances. remarkably, a degree of a carrier lifetime separation, as manifested inside a distinctly different temperature dependences and diverging residuals near the critical quantum well thickness, is completely insensitive to disorder. a results allow considering the clear distinction between disorder-induced contributions to a transport and intrinsic, non-fermi liquid phenomena, which includes a lifetime separation."
"fractional-order dynamical systems are used to describe processes that exhibit long-term memory with power-law dependence. notable examples include complex neurophysiological signals such as electroencephalogram (eeg) and blood-oxygen-level dependent (bold) signals. when analyzing different neurophysiological signals and other signals with different origin (for example, biological systems), we often find a presence of artifacts, that is, recorded activity that was due to external causes and does not have its origins inside a system of interest. inside this paper, we consider a problem of estimating a states of the discrete-time fractional-order dynamical system when there are artifacts present inside some of a sensor measurements. specifically, we provide necessary and sufficient conditions that ensure we should retrieve a system states even inside a presence of artifacts. we provide the state approximation algorithm that should approximate a states of a system inside a presence of artifacts. finally, we present illustrative examples of our main results with the help of real eeg data."
"we investigate a effects of a in-plane biaxial strain and charge doping on a charge density wave (cdw) order of monolayer $1t$-tise$_2$ by with the help of a first-principles calculations. our results show that a tensile strain should significantly enhance a cdw order, while both compressive strain and charge doping (electrons and holes) suppress a cdw instability. a tensile strain may provide an effective method considering obtaining higher cdw transition temperature on a basis of monolayer $1t$-tise$_2$. we also discuss a potential superconductivity inside charge-doped monolayer $1t$-tise$_2$. controllable electronic phase transition from cdw state to metallic state or even superconducting state should be realized inside monolayer $1t$-tise$_2$, which makes $1t$-tise$_2$ possess the promising application inside controllable switching electronic devices based on cdw."
"inside recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. inside this work, we ask how to imagine goal-directed visual plans -- the plausible sequence of observations that transition the dynamical system from its current configuration to the desired goal state, which should later be used as the reference trajectory considering control. we focus on systems with high-dimensional observations, such as images, and propose an idea behind the method that naturally combines representation learning and planning. our framework learns the generative model of sequential observations, where a generative process was induced by the transition inside the low-dimensional planning model, and an additional noise. by maximizing a mutual information between a generated observations and a transition inside a planning model, we obtain the low-dimensional representation that best explains a causal nature of a data. we structure a planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. finally, to generate the visual plan, we project a current and goal observations onto their respective states inside a planning model, plan the trajectory, and then use a generative model to transform a trajectory to the sequence of observations. we demonstrate our method on imagining plausible visual plans of rope manipulation."
"we present the new idea behind the method to learn compressible representations inside deep architectures with an end-to-end training strategy. our method was based on the soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. we showcase this method considering two challenging applications: image compression and neural network compression. while these tasks have typically been approached with different methods, our soft-to-hard quantization idea behind the method gives results competitive with a state-of-the-art considering both."
"binomial random intersection graphs should be used as parsimonious statistical models of large and sparse networks, with one parameter considering a average degree and another considering transitivity, a tendency of neighbours of the node to be connected. this paper discusses a approximation of these parameters from the single observed instance of a graph, with the help of moment estimators based on observed degrees and frequencies of 2-stars and triangles. a observed data set was assumed to be the subgraph induced by the set of $n_0$ nodes sampled from a full set of $n$ nodes. we prove a consistency of a proposed estimators by showing that a relative approximation error was small with high probability considering $n_0 \gg n^{2/3} \gg 1$. as the byproduct, our analysis confirms that a empirical transitivity coefficient of a graph was with high probability close to a theoretical clustering coefficient of a model."
"inside this paper, we explore a utilization of natural language to drive transfer considering reinforcement learning (rl). despite a wide-spread application of deep rl techniques, learning generalized policy representations that work across domains remains the challenging problem. we demonstrate that textual descriptions of environments provide the compact intermediate channel to facilitate effective policy transfer. specifically, by learning to ground a meaning of text to a dynamics of a environment such as transitions and rewards, an autonomous agent should effectively bootstrap policy learning on the new domain given its description. we employ the model-based rl idea behind the method consisting of the differentiable planning module, the model-free component and the factorized state representation to effectively use entity descriptions. our model outperforms prior work on both transfer and multi-task scenarios inside the variety of different environments. considering instance, we achieve up to 14% and 11.5% absolute improvement over previously existing models inside terms of average and initial rewards, respectively."
"let v be an harish-chandra discrete series representation of the real semi-simple lie group g' and let g be the semi-simple subgroup of g'. inside this paper, we give the geometric expression of a g-multiplicities inside v when a representation v was supposed to be g-admissible."
"this paper introduces the novel algorithm considering transductive inference inside higher-order mrfs, where a unary energies are parameterized by the variable classifier. a considered task was posed as the joint optimization problem inside a continuous classifier parameters and a discrete label variables. inside contrast to prior approaches such as convex relaxations, we propose an advantageous decoupling of a objective function into discrete and continuous subproblems and the novel, efficient optimization method related to admm. this idea behind the method preserves integrality of a discrete label variables and guarantees global convergence to the critical point. we demonstrate a advantages of our idea behind the method inside several experiments including video object segmentation on a davis data set and interactive image segmentation."
"we consider the class of models inside which thermal dark matter was lighter than an mev. if dark matter thermalizes with a standard model below a temperature of neutrino-photon decoupling, equilibration and freeze-out cools and heats a standard model bath comparably, alleviating constraints from measurements of a effective number of neutrino species. we demonstrate this mechanism inside the model consisting of fermionic dark matter coupled to the light scalar mediator. thermal dark matter should be as light as the few kev, while remaining compatible with existing cosmological and astrophysical observations. this framework motivates new experiments inside a direct search considering sub-mev thermal dark matter and light force carriers."
"srruo$_3$ (sro) films are known to exhibit insulating behavior as their thickness approaches four unit cells. we employ electron energy$-$loss (eel) spectroscopy to probe a spatially resolved electronic structures of both insulating and conducting sro to correlate them with a metal$-$insulator transition (mit). importantly, a central layer of a ultrathin insulating film exhibits distinct features from a metallic sro. moreover, eel near edge spectra adjacent to a srtio$_3$ (sto) substrate or to a capping layer are remarkably similar to those of sto. a site$-$projected density of states based on density functional theory (dft) partially reflects a characteristics of a spectra of these layers. these results may provide important information on a possible influence of sto on a electronic states of ultrathin sro."
"inside this paper, we provide the complete classification considering all a isometric cohomogeneity one actions on unit spheres. with the help of this theory, we should very easily classify all a isometric cohomogeneity one actions on a riemannian symmetric spaces $\mathbb{c}\mathrm{p}^{m-1}$ and $\mathbb{h}\mathrm{p}^{m-1}$."
"existing concepts of reversible superconducting circuits as well as demonstrated adiabatic circuits require three-phase bias/clock signals generated by room temperature sources. the while ago, we suggested that the multi-phase bias/clock could be provided by the local josephson junction-based generator. a generator looks like the long annular josephson junction, only composed of discreet elements - junctions and inductors, and closed into the ring using the flux pump to inject any required number of vortices into a ring. the steady motion of a vortices forced by the uniformly distributed dc bias current applied to a ring was accompanied by the nearly harmonic ac currents flowing using a josephson junctions (jjs) connected inside series with small inductors. these ac currents serve as multi-phase bias/clock considering nsquid-based circuitry. to verify this concept and compare a dissipated energy with kbtln2 threshold, we developed the ring composed of 256 unshunted jjs with 20 {\mu}a target critical current, ic. we investigated a behavior of a ring oscillator at each vortex count from 0 to 256. a measured critical current of a ring with vortices is about 0.1 {\mu}a per one jj, which should be explained by unavoidable nonuniformity of a ring components and a influence of fluxes frozen near a ring. a corresponding energy dissipation, about 10kbt per passage of one vortex through one jj, should be reduced further considering prospective experiments with reversible circuits. however, obtained i-v characteristics could be of interest considering scientists working with long josephson junctions. superiority of a fabrication process used inside this work was demonstrated by a obtained about 200 times reduction of ic of a ring with vortices with respect to the single comprising jj, much larger than inside any previously described case."
"deep generative models are reported to be useful inside broad applications including image generation. repeated inference between data space and latent space inside these models should denoise cluttered images and improve a quality of inferred results. however, previous studies only qualitatively evaluated image outputs inside data space, and a mechanism behind a inference has not been investigated. a purpose of a current study was to numerically analyze changes inside activity patterns of neurons inside a latent space of the deep generative model called the ""variational auto-encoder"" (vae). what kinds of inference dynamics a vae demonstrates when noise was added to a input data are identified. a vae embeds the dataset with clear cluster structures inside a latent space and a center of each cluster of multiple correlated data points (memories) was referred as a concept. our study demonstrated that transient dynamics of inference first approaches the concept, and then moves close to the memory. moreover, a vae revealed that a inference dynamics approaches the more abstract concept to a extent that a uncertainty of input data increases due to noise. it is demonstrated that by increasing a number of a latent variables, a trend of a inference dynamics to idea behind the method the concept should be enhanced, and a generalization ability of a vae should be improved."
"let $q$ be the nondegenerate quadratic form on the vector space $v$ of even dimension $n$ over the number field $f$. using a circle method or automorphic methods one should give good estimates considering smoothed sums over a number of zeros of a quadratic form whose coordinates are of size at most $x$ (properly interpreted). considering example, when $f=\mathbb{q}$ and $\dim v>4$ heath-brown has given an asymptotic of a form \begin{align} \label{hb:esti} c_1x^{n-2}+o_{q,\varepsilon,f}(x^{n/2+\varepsilon}) \end{align} considering any $\varepsilon>0$. here $c_1 \in \mathbb{c}$ and $f \in \mathcal{s}(v(\mathbb{r}))$ was the smoothing function. we refine heath-brown's work to give an asymptotic of a form $$ c_1x^{n-2}+c_2x^{n/2}+o_{q,\varepsilon,f}(x^{n/2+\varepsilon-1}) $$ over any number field. here $c_2 \in \mathbb{c}$. interestingly a secondary term $c_2$ was a sum of the rapidly decreasing function on $v(\mathbb{r})$ over a zeros of $q^{\vee}$, a form whose matrix was inverse to a matrix of $q$. we also prove analogous results inside a boundary case $n=4$, generalizing and refining heath-brown's work inside a case $f=\mathbb{q}$."
"inside parallel computing, the valid graph coloring yields the lock-free processing of a colored tasks, data points, etc., without expensive synchronization mechanisms. however, coloring was not free and a overhead should be significant. inside particular, considering a bipartite-graph partial coloring (bgpc) and distance-2 graph coloring (d2gc) problems, which have various use-cases within a scientific computing and numerical optimization domains, a coloring overhead should be inside a order of minutes with the single thread considering many real-life graphs. inside this work, we propose parallel algorithms considering bipartite-graph partial coloring on shared-memory architectures. compared to a existing shared-memory bgpc algorithms, a proposed ones employ greedier and more optimistic techniques that yield the better parallel coloring performance. inside particular, on 16 cores, a proposed algorithms perform more than 4x faster than their counterparts inside a colpack library which is, to a best of our knowledge, a only publicly-available coloring library considering multicore architectures. inside addition to bgpc, a proposed techniques are employed to devise parallel distance-2 graph coloring algorithms and similar performance improvements have been observed. finally, we propose two costless balancing heuristics considering bgpc that should reduce a skewness and imbalance on a cardinality of color sets (almost) considering free. a heuristics should also be used considering a d2gc problem and inside general, they will probably yield the better color-based parallelization performance especially on many-core architectures."
"with the help of a landau ginzburg devonshire theory and scalar approximation, we derived analytical expressions considering a singular points (zeros, complex ranges) of a acoustic phonon mode (a mode) frequency inside dependence on a wave vector k and examined a conditions of a soft the modes appearance inside the ferroelectric depending on a magnitude of a flexoelectric coefficient f and temperature t. we predict that if a magnitude of a flexocoefficient f was equal to a temperature-dependent critical value fcr(t) at a temperature t=t_ic, a the mode frequency tends to zero at k=kr_0 and a spontaneous polarization becomes spatially modulated inside the temperature range t<t_ic.the comparison of calculated physical properties with measured ones are performed considering some ferroelectrics with smp phases.in particular, temperature dependence of a calculated direct and inverse static dielectric susceptibility was inside an agreement with experimental data inside sn2p2(sexs1-x)6 that gives us additional background to predict flexo-coupling induced soft acoustic amplitudon-type mode inside a smp phase.the available experimental data on neutron scattering inside organic incommensurate ferroelectric (ch3)3nch2coo*cacl2*2h2o are inside the semi-quantitative agreement with our theoretical results. to quantify a theory, it was necessary to measure a frequency dependence of a the mode inside the uniaxial ferroelectric with the spatially modulated phase inside a temperature interval near its occurrence."
"inside this paper, we study conformally flat hypersurfaces of dimension $n(\geq 4)$ inside $\mathbb{s}^{n+1}$ with the help of a framework of möbius geometry. first, we classify and explicitly express a conformally flat hypersurfaces of dimension $n(\geq 4)$ with constant möbius scalar curvature under a möbius transformation group of $\mathbb{s}^{n+1}$. second, we prove that if a conformally flat hypersurface with constant möbius scalar curvature $r$ was compact, then $$r=(n-1)(n-2)r^2, ~~0<r<1,$$ and a compact conformally flat hypersurface was möbius equivalent to a torus $$\mathbb{ s}^1(\sqrt{1-r^2})\times \mathbb{s}^{n-1}(r)\hookrightarrow \mathbb{s}^{n+1}.$$"
"we develop efficient algorithms considering estimating low-degree moments of unknown distributions inside a presence of adversarial outliers. a guarantees of our algorithms improve inside many cases significantly over a best previous ones, obtained inside recent works of diakonikolas et al, lai et al, and charikar et al. we also show that a guarantees of our algorithms match information-theoretic lower-bounds considering a class of distributions we consider. these improved guarantees allow us to give improved algorithms considering independent component analysis and learning mixtures of gaussians inside a presence of outliers. our algorithms are based on the standard sum-of-squares relaxation of a following conceptually-simple optimization problem: among all distributions whose moments are bounded inside a same way as considering a unknown distribution, find a one that was closest inside statistical distance to a empirical distribution of a adversarially-corrupted sample."
"forward-looking ground-penetrating radar (flgpr) has recently been investigated as the remote sensing modality considering buried target detection (e.g., landmines). inside this context, raw flgpr data was beamformed into images and then computerized algorithms are applied to automatically detect subsurface buried targets. most existing algorithms are supervised, meaning they are trained to discriminate between labeled target and non-target imagery, usually based on features extracted from a imagery. the large number of features have been proposed considering this purpose, however thus far it was unclear which are a most effective. a first goal of this work was to provide the comprehensive comparison of detection performance with the help of existing features on the large collection of flgpr data. fusion of a decisions resulting from processing each feature was also considered. a second goal of this work was to investigate two modern feature learning approaches from a object recognition literature: a bag-of-visual-words and a fisher vector considering flgpr processing. a results indicate that a new feature learning approaches outperform existing methods. results also show that fusion between existing features and new features yields little additional performance improvements."
we provide $l^p$-versus $l^\infty$-bounds considering eigenfunctions on the real spherical space $z$ of wavefront type. it was shown that these bounds imply the non-trivial error term approximate considering lattice counting on $z$. a paper also serves as an introduction to geometric counting on spaces of a mentioned type. section 7 on higher rank was new and extends a result from v1 to higher rank. final version. to appear inside acta math. sinica.
"inside a typical framework considering boolean games (bg) each player should change a truth value of some propositional atoms, while attempting to make her goal true. inside standard bg goals are propositional formulas, whereas inside iterated bg goals are formulas of linear temporal logic. both notions of bg are characterised by a fact that agents have exclusive control over their set of atoms, meaning that no two agents should control a same atom. inside a present contribution we drop a exclusivity assumption and explore structures where an atom should be controlled by multiple agents. we introduce concurrent game structures with shared propositional control (cgs-spc) and show that they ac- count considering several classes of repeated games, including iterated boolean games, influence games, and aggregation games. our main result shows that, as far as verification was concerned, cgs-spc should be reduced to concurrent game structures with exclusive control. this result provides the polynomial reduction considering a model checking problem of specifications inside alternating-time temporal logic on cgs-spc."
"a analysis of manifold-valued data requires efficient tools from riemannian geometry to cope with a computational complexity at stake. this complexity arises from a always-increasing dimension of a data, and a absence of closed-form expressions to basic operations such as a riemannian logarithm. inside this paper, we adapt the generic numerical scheme recently introduced considering computing parallel transport along geodesics inside the riemannian manifold to finite-dimensional manifolds of diffeomorphisms. we provide the qualitative and quantitative analysis of its behavior on high-dimensional manifolds, and investigate an application with a prediction of brain structures progression."
"aims: inside order to test a nature of an (accretion) disk inside a vicinity of cepheus the hw2, we measured a three-dimensional velocity field of a ch3oh maser spots, which are projected within 1000au of a hw2 object, with an accuracy of a order of 0.1km/s. methods: we made use of a european vlbi network (evn) to image a 6.7ghz ch3oh maser emission towards cepheus the hw2 with 4.5 milli-arcsecond resolution (3au). we observed at three epochs spaced by one year between 2013 and 2015. during a last epoch, on mid-march 2015, we benefited from a new deployed sardinia radio telescope. results: we show that a ch3oh velocity vectors lie on the preferential plane considering a gas motion with only small deviations of 12+/-9 degrees away from a plane. this plane was oriented at the position angle of 134 degrees east of north, and inclined by 26 degrees with a line-of-sight, closely matching a orientation of a disk-like structure previously reported by patel et al.(2005). knowing a orientation of a equatorial plane, we should reconstruct the face-on view of a ch3oh gas kinematics onto a plane. ch3oh maser emission was detected within the radius of 900au from hw2, and down to the radius of about 300au, a latter coincident with a extent of a dust emission at 0.9mm. a velocity field was dominated by an infall component of about 2km/s down to the radius of 300au, where the rotational component of 4km/s becomes dominant. we discuss a nature of this velocity field and a implications considering a enclosed mass. conclusions: these findings bring direct support to a interpretation that a high-density gas and dust emission, surrounding cepheus the hw2, trace an accretion disk."
"model-based compression was an effective, facilitating, and expanded model of neural network models with limited computing and low power. however, conventional models of compression techniques utilize crafted features [2,3,12] and explore specialized areas considering exploration and design of large spaces inside terms of size, speed, and accuracy, which usually have returns less and time was up. this paper will effectively analyze deep auto compression (adc) and reinforcement learning strength inside an effective sample and space design, and improve a compression quality of a model. a results of compression of a advanced model are obtained without any human effort and inside the completely automated way. with the 4- fold reduction inside flop, a accuracy of 2.8% was higher than a manual compression model considering vgg-16 inside imagenet."
"we report giant resistive switching of an order of 104, long-time charge retention characteristics up to 104 s, non-overlapping set and reset voltages, ohmic inside low resistance state (lrs) and space charge limited current (sclc) mechanism inside high resistance state (hrs) properties inside polycrystalline perovskite cobalt titanate (cotio3 ~ cto) thin films. impedance spectroscopy study is carried out considering both lrs and hrs states which illustrates that only bulk resistance changes after resistance switching, however, there was the small change (<10% which was inside pf range) inside a bulk capacitance value inside both states. these results suggest that inside lrs state current filaments break a capacitor inside many small capacitors inside the parallel configuration which inside turn provides a same capacitance inside both states even there is 90 degree changes inside phase-angle and an order of change inside a tangent loss."
"we consider a problem of bandit optimization, inspired by stochastic optimization and online learning problems with bandit feedback. inside this problem, a objective was to minimize the global loss function of all a actions, not necessarily the cumulative loss. this framework allows us to study the very general class of problems, with applications inside statistics, machine learning, and other fields. to solve this problem, we analyze a upper-confidence frank-wolfe algorithm, inspired by techniques considering bandits and convex optimization. we give theoretical guarantees considering a performance of this algorithm over various classes of functions, and discuss a optimality of these results."
the few notes on a use of machine learning inside medicine and a related unintended consequences.
"high speed navigation through unknown environments was the challenging problem inside robotics. it requires fast computation and tight integration of all a subsystems on a robot such that a latency inside a perception-action loop was as small as possible. aerial robots add the limitation of payload capacity, which restricts a amount of computation that should be carried onboard. this requires efficient algorithms considering each component inside a navigation system. inside this paper, we describe our quadrotor system which was able to smoothly navigate through mixed indoor and outdoor environments and was able to fly at speeds of more than 18 m/s. we provide an overview of our system and details about a specific component technologies that enable a high speed navigation capability of our platform. we demonstrate a robustness of our system through high speed autonomous flights and navigation through the variety of obstacle rich environments."
"inside this paper we study a behavior of a fractions of the factorial design under permutations of a factor levels. we focus on a notion of regular fraction and we introduce methods to check whether the given symmetric orthogonal array should or should not be transformed into the regular fraction by means of suitable permutations of a factor levels. a proposed techniques take advantage of a complex coding of a factor levels and of some tools from polynomial algebra. several examples are described, mainly involving factors with five levels."
the functional analytic idea behind the method to obtaining self-improving properties of solutions to linear non-local elliptic equations was presented. it yields conceptually simple and very short proofs of some previous results due to kuusi-mingione-sire and bass-ren. its flexibility was demonstrated by new applications to non-autonomous parabolic equations with non-local elliptic part and questions related to maximal regularity
"fluid dynamics accompanies with a entropy production thus increases a local temperature, which plays an important role inside charged systems such as a ion channel inside biological environment and electrodiffusion inside capacitors/batteries. inside this article, we propose the general framework to derive a transport equations with heat flow through a energetic variational approach. according to thermodynamic first law, a total energy was conserved and we should then use a least action principle to derive a conservative forces. from thermodynamic second law, a entropy increases and a dissipative forces should be computed with a maximum dissipation principle. combining these two laws, we then conclude with a force balance equations and the temperature equation. to emphasis, our method provide the self consistent procedure to obtain dynamical equations satisfying proper energy laws and it not only works considering a charge systems but also considering general systems."
"this article presents the novel idea behind the method to approximate semantic entity similarity with the help of entity features available as linked data. a key idea was to exploit ranked lists of features, extracted from linked data sources, as the representation of a entities to be compared. a similarity between two entities was then estimated by comparing their ranked lists of features. a article describes experiments with museum data from dbpedia, with datasets from the lod catalog, and with computer science conferences from a dblp repository. a experiments demonstrate that entity similarity, computed with the help of ranked lists of features, achieves better accuracy than state-of-the-art measures."
"successful multimodal search and retrieval requires a automatic understanding of semantic cross-modal relations, which, however, was still an open research problem. previous work has suggested a metrics cross-modal mutual information and semantic correlation to model and predict cross-modal semantic relations of image and text. inside this paper, we present an idea behind the method to predict a (cross-modal) relative abstractness level of the given image-text pair, that was whether a image was an abstraction of a text or vice versa. considering this purpose, we introduce the new metric that captures this specific relationship between image and text at a abstractness level (abs). we present the deep learning idea behind the method to predict this metric, which relies on an autoencoder architecture that allows us to significantly reduce a required amount of labeled training data. the comprehensive set of publicly available scientific documents has been gathered. experimental results on the challenging test set demonstrate a feasibility of a approach."
"there was the pressing need to build an architecture that could subsume these networks under the unified framework that achieves both higher performance and less overhead. to this end, two fundamental issues are yet to be addressed. a first one was how to implement a back propagation when neuronal activations are discrete. a second one was how to remove a full-precision hidden weights inside a training phase to break a bottlenecks of memory/computation consumption. to address a first issue, we present the multi-step neuronal activation discretization method and the derivative approximation technique that enable a implementing a back propagation algorithm on discrete dnns. while considering a second issue, we propose the discrete state transition (dst) methodology to constrain a weights inside the discrete space without saving a hidden weights. through this way, we build the unified framework that subsumes a binary or ternary networks as its special cases, and under which the heuristic algorithm was provided at a website this https url. more particularly, we find that when both a weights and activations become ternary values, a dnns should be reduced to sparse binary networks, termed as gated xnor networks (gxnor-nets) since only a event of non-zero weight and non-zero activation enables a control gate to start a xnor logic operations inside a original binary networks. this promises a event-driven hardware design considering efficient mobile intelligence. we achieve advanced performance compared with state-of-the-art algorithms. furthermore, a computational sparsity and a number of states inside a discrete space should be flexibly modified to make it suitable considering various hardware platforms."
"bayesian neural networks (bnns) allow us to reason about uncertainty inside the principled way. stochastic gradient langevin dynamics (sgld) enables efficient bnn learning by drawing samples from a bnn posterior with the help of mini-batches. however, sgld and its extensions require storage of many copies of a model parameters, the potentially prohibitive cost, especially considering large neural networks. we propose the framework, adversarial posterior distillation, to distill a sgld samples with the help of the generative adversarial network (gan). at test-time, samples are generated by a gan. we show that this distillation framework incurs no loss inside performance on recent bnn applications including anomaly detection, active learning, and defense against adversarial attacks. by construction, our framework not only distills a bayesian predictive distribution, but a posterior itself. this allows one to compute quantities such as a approximate model variance, which was useful inside downstream tasks. to our knowledge, these are a first results applying mcmc-based bnns to a aforementioned downstream applications."
"considering fifty years astronomers have been searching considering pulsar signals inside observational data. throughout this time a process of choosing detections worthy of investigation, so called candidate selection, has been effective, yielding thousands of pulsar discoveries. yet inside recent years technological advances have permitted a proliferation of pulsar-like candidates, straining our candidate selection capabilities, and ultimately reducing selection accuracy. to overcome such problems, we now apply intelligent machine learning tools. whilst these have achieved success, candidate volumes continue to increase, and our methods have to evolve to keep pace with a change. this talk considers how to meet this challenge as the community."
"loss to followup was the significant issue inside healthcare and has serious consequences considering the study's validity and cost. methods available at present considering recovering loss to followup information are restricted by their expressive capabilities and struggle to model highly non-linear relations and complex interactions. inside this paper we propose the model based on overcomplete denoising autoencoders to recover loss to followup information. designed to work with high volume data, results on various simulated and real life datasets show our model was appropriate under varying dataset and loss to followup conditions and outperforms a state-of-the-art methods by the wide margin ($\ge 20\%$ inside some scenarios) while preserving a dataset utility considering final analysis."
"inside a context of machine learning, disparate impact refers to the form of systematic discrimination whereby a output distribution of the model depends on a value of the sensitive attribute (e.g., race or gender). inside this paper, we propose an information-theoretic framework to analyze a disparate impact of the binary classification model. we view a model as the fixed channel, and quantify disparate impact as a divergence inside output distributions over two groups. our aim was to find the correction function that should perturb a input distributions of each group to align their output distributions. we present an optimization problem that should be solved to obtain the correction function that will make a output distributions statistically indistinguishable. we derive closed-form expressions to efficiently compute a correction function, and demonstrate a benefits of our framework on the recidivism prediction problem based on a propublica compas dataset."
"inside this thesis we present research into linear perturbations inside lemaitre-tolman-bondi (ltb) and assisted coupled quintessence (acq) cosmologies. first we give the brief overview of a standard model of cosmology. we then introduce cosmological perturbation theory (cpt) at linear order considering the flat friedmann-robertson-walker (frw) cosmology. next we study linear perturbations to the lemaitre-tolman-bondi (ltb) background spacetime. studying a transformation behaviour of a perturbations under gauge transformations, we construct gauge invariant quantities inside ltb. we show, with the help of a perturbed energy conservation equation, that there was the conserved quantity inside ltb which was conserved on all scales. we then briefly extend our discussion to a lemaitre spacetime, and construct gauge-invariant perturbations inside this extension of ltb spacetime. we also study a behaviour of linear perturbations inside assisted coupled quintessence models inside the frw background. we provide a full set of governing equations considering this class of models, and solve a system numerically. a code written considering this purpose was then used to evolve growth functions considering various models and parameter values, and we compare these both to a standard $\lambda$cdm model and to current and future observational bounds. we also examine a applicability of a ""small scale approximation"", often used to calculate growth functions inside quintessence models, inside light of upcoming experiments such as ska and euclid. we find a results of a full equations deviates from a approximation by more than a experimental uncertainty considering these future surveys. a construction of a numerical code, pyessence, written inside python to solve a system of background and perturbed evolution equations considering assisted coupled quintessence, was also discussed."
"deep neural networks (dnns) achieve excellent performance on standard classification tasks. however, under image quality distortions such as blur and noise, classification accuracy becomes poor. inside this work, we compare a performance of dnns with human subjects on distorted images. we show that, although dnns perform better than or on par with humans on good quality images, dnn performance was still much lower than human performance on distorted images. we additionally find that there was little correlation inside errors between dnns and human subjects. this could be an indication that a internal representation of images are different between dnns and a human visual system. these comparisons with human performance could be used to guide future development of more robust dnns."
"a lasso and elastic net linear regression models impose the double-exponential prior distribution on a model parameters to achieve regression shrinkage and variable selection, allowing a inference of robust models from large data sets. however, there has been limited success inside deriving estimates considering a full posterior distribution of regression coefficients inside these models, due to the need to evaluate analytically intractable partition function integrals. here, a fourier transform was used to express these integrals as complex-valued oscillatory integrals over ""regression frequencies"". this results inside an analytic expansion and stationary phase approximation considering a partition functions of a bayesian lasso and elastic net, where a non-differentiability of a double-exponential prior has so far eluded such an approach. use of this approximation leads to highly accurate numerical estimates considering a expectation values and marginal posterior distributions of a regression coefficients, and allows considering bayesian inference of much higher dimensional models than previously possible."
"inside this paper, an unsupervised steganalysis method that combines artificial training setsand supervised classification was proposed. we provide the formal framework considering unsupervisedclassification of stego and cover images inside a typical situation of targeted steganalysis (i.e.,for the known algorithm and approximate embedding bit rate). we also present the completeset of experiments with the help of 1) eight different image databases, 2) image features based on richmodels, and 3) three different embedding algorithms: least significant bit (lsb) matching,highly undetectable steganography (hugo) and wavelet obtained weights (wow). weshow that a experimental results outperform previous methods based on rich models inthe majority of a tested cases. at a same time, a proposed idea behind the method bypasses theproblem of cover source mismatch -when a embedding algorithm and bit rate are known-, since it removes a need of the training database when we have the large enough testing set.furthermore, we provide the generic proof of a proposed framework inside a machine learningcontext. hence, a results of this paper could be extended to other classification problemssimilar to steganalysis."
"we investigate a effectiveness of generative adversarial networks (gans) considering speech enhancement, inside a context of improving noise robustness of automatic speech recognition (asr) systems. prior work demonstrates that gans should effectively suppress additive noise inside raw waveform speech signals, improving perceptual quality metrics; however this technique is not justified inside a context of asr. inside this work, we conduct the detailed study to measure a effectiveness of gans inside enhancing speech contaminated by both additive and reverberant noise. motivated by recent advances inside image processing, we propose operating gans on log-mel filterbank spectra instead of waveforms, which requires less computation and was more robust to reverberant noise. while gan enhancement improves a performance of the clean-trained asr system on noisy speech, it falls short of a performance achieved by conventional multi-style training (mtr). by appending a gan-enhanced features to a noisy inputs and retraining, we achieve the 7% wer improvement relative to a mtr system."
"we present the deep convolutional decoder architecture that should generate volumetric 3d outputs inside the compute- and memory-efficient manner by with the help of an octree representation. a network learns to predict both a structure of a octree, and a occupancy values of individual cells. this makes it the particularly valuable technique considering generating 3d shapes. inside contrast to standard decoders acting on regular voxel grids, a architecture does not have cubic complexity. this allows representing much higher resolution outputs with the limited memory budget. we demonstrate this inside several application domains, including 3d convolutional autoencoders, generation of objects and whole scenes from high-level representations, and shape from the single image."
"inside about four years, a national aeronautics and space administration (nasa) will launch the small explorer mission named a imaging x-ray polarimetry explorer (ixpe). ixpe was the satellite dedicated to a observation of x-ray polarization from bright astronomical sources inside a 2-8 kev energy range. with the help of gas pixel detectors (gpd), a mission will allow considering a first time to acquire x-ray polarimetric imaging and spectroscopy of about the hundred of sources during its first two years of operation. among them are a most powerful sources of light inside a universe: active galactic nuclei (agn). inside this proceedings, we summarize a scientific exploration we aim to achieve inside a field of agn with the help of ixpe, describing a main discoveries that this new generation of x-ray polarimeters will be able to make. among these discoveries, we expect to detect indisputable signatures of strong gravity, quantifying a amount and importance of scattering on distant cold material onto a iron k_alpha line observed at 6.4 kev. ixpe will also be able to probe a morphology of parsec-scale agn regions, a magnetic field strength and direction inside quasar jets, and, among a most important results, deliver an independent measurement of a spin of black holes."
"we theoretically study the kitaev chain with the quasiperiodic potential, where a quasiperiodicity was introduced by the fibonacci sequence. based on an analysis of a majorana zero-energy mode, we find a critical $p$-wave superconducting pairing potential separating the topological phase and the non-topological phase. a topological phase diagram with respect to fibonacci potentials follow the self-similar fractal structure characterized by a box-counting dimension, which was an example of a interplay of fractal and topology like a hofstadter's butterfly inside quantum hall insulators."
"uniformly most powerful bayesian tests (umpbts) are an objective class of bayesian hypothesis tests that should be considered a bayesian counterpart of classical uniformly most powerful tests. unfortunately, umpbts have only been exposed considering application inside one parameter exponential family models. a purpose of this article was to describe methodology considering deriving umpbts considering the larger class of tests. specifically, we introduce sufficient conditions considering a existence of umpbts and propose the unified idea behind the method considering their derivation. an important application of our methodology was a extension of umpbts to testing whether a non-centrality parameter of the chi-squared distribution was zero. a resulting tests have broad applicability, providing default alternative hypotheses to compute bayes factors in, considering example, pearson's chi-squared test considering goodness-of-fit, tests of independence inside contingency tables, and likelihood ratio, score and wald tests. we close with the brief comparison of our methodology to a karlin-rubin theorem."
"inside this work we exploit agglomeration based $h$-multigrid preconditioners to speed-up a iterative solution of discontinuous galerkin discretizations of a stokes and navier-stokes equations. as the distinctive feature $h$-coarsened mesh sequences are generated by recursive agglomeration of the fine grid, admitting arbitrarily unstructured grids of complex domains, and agglomeration based discontinuous galerkin discretizations are employed to deal with agglomerated elements of coarse levels. both a expense of building coarse grid operators and a performance of a resulting multigrid iteration are investigated. considering a sake of efficiency coarse grid operators are inherited through element-by-element $l^2$ projections, avoiding a cost of numerical integration over agglomerated elements. specific care was devoted to a projection of viscous terms discretized by means of a br2 dg method. we demonstrate that enforcing a correct amount of stabilization on coarse grids levels was mandatory considering achieving uniform convergence with respect to a number of levels. a numerical solution of steady and unsteady, linear and non-linear problems was considered tackling challenging 2d test cases and 3d real life computations on parallel architectures. significant execution time gains are documented."
we propose the method to generate multiple diverse and valid human pose hypotheses inside 3d all consistent with a 2d detection of joints inside the monocular rgb image. we use the novel generative model uniform (unbiased) inside a space of anatomically plausible 3d poses. our model was compositional (produces the pose by combining parts) and since it was restricted only by anatomical constraints it should generalize to every plausible human 3d pose. removing a model bias intrinsically helps to generate more diverse 3d pose hypotheses. we argue that generating multiple pose hypotheses was more reasonable than generating only the single 3d pose based on a 2d joint detection given a depth ambiguity and a uncertainty due to occlusion and imperfect 2d joint detection. we hope that a idea of generating multiple consistent pose hypotheses should give rise to the new line of future work that has not received much attention inside a literature. we used a human3.6m dataset considering empirical evaluation.
"we study boundary conditions of topological sigma models with a goal of generalizing a concepts of anomalous symmetry and symmetry protected topological order. we find the version of 't hooft's anomaly matching conditions on a renormalization group flow of boundaries of invertible topological sigma models and discuss several examples of anomalous boundary theories. we also comment on bulk topological transitions inside dynamical sigma models and argue that one can, with care, use topological data to draw sigma model phase diagrams."
"deep learning algorithms offer the powerful means to automatically analyze a content of medical images. however, many biological samples of interest are primarily transparent to visible light and contain features that are difficult to resolve with the standard optical microscope. here, we use the convolutional neural network (cnn) not only to classify images, but also to optimize a physical layout of a imaging device itself. we increase a classification accuracy of the microscope's recorded images by merging an optical model of image formation into a pipeline of the cnn. a resulting network simultaneously determines an ideal illumination arrangement to highlight important sample features during image acquisition, along with the set of convolutional weights to classify a detected images post-capture. we demonstrate our joint optimization technique with an experimental microscope configuration that automatically identifies malaria-infected cells with 5-10% higher accuracy than standard and alternative microscope lighting designs."
"this paper was devoted to a 3-dimensional relative differential geometry of surfaces. inside a euclidean space $\r{e} ^3 $ we consider the surface $\varphi %\colon \vect{x} = \vect{x}(u^1,u^2) $ with position vector field $\vect{x}$, which was relatively normalized by the relative normalization $\vect{y}% (u^1,u^2) $. the surface $\varphi^*% \colon \vect{x}^* = \vect{x}^*(u^1,u^2) $ with position vector field $\vect{x}^* = \vect{x} + \mu \, \vect{y}$, where $\mu$ was the real constant, was called the relatively parallel surface to $\varphi$. then $\vect{y}$ was also the relative normalization of $\varphi^*$. a aim of this paper was to formulate and prove a relative analogues of two well known theorems of o.~bonnet which concern a parallel surfaces (see~\cite{ob1853})."
"a x-ray transform on a periodic slab $[0,1]\times\mathbb t^n$, $n\geq0$, has the non-trivial kernel due to a symmetry of a manifold and presence of trapped geodesics. considering tensor fields gauge freedom increases a kernel further, and a x-ray transform was not solenoidally injective unless $n=0$. we characterize a kernel of a geodesic x-ray transform considering $l^2$-regular $m$-tensors considering any $m\geq0$. a characterization extends to more general manifolds, twisted slabs, including a möbius strip as a simplest example."
"we analyze a behavior of approximate bayesian computation (abc) when a model generating a simulated data differs from a actual data generating process; i.e., when a data simulator inside abc was misspecified. we demonstrate both theoretically and inside simple, but practically relevant, examples that when a model was misspecified different versions of abc should lead to substantially different results. our theoretical results demonstrate that under regularity conditions the version of a accept/reject abc idea behind the method concentrates posterior mass on an appropriately defined pseudo-true parameter value. however, under model misspecification a abc posterior does not yield credible sets with valid frequentist coverage and has non-standard asymptotic behavior. we also examine a theoretical behavior of a popular linear regression adjustment to abc under model misspecification and demonstrate that this idea behind the method concentrates posterior mass on the completely different pseudo-true value than that obtained by a accept/reject idea behind the method to abc. with the help of our theoretical results, we suggest two approaches to diagnose model misspecification inside abc. all theoretical results and diagnostics are illustrated inside the simple running example."
"a validity of a widely used linear mixing approximation considering a equations of state (eos) of planetary ices was investigated at pressure-temperature conditions typical considering a interior of uranus and neptune. a basis of this study are ab initio data ranging up to 1000 gpa and 20 000 k calculated using density functional theory molecular dynamics simulations. inside particular, we calculate the new eos considering methane and eos data considering a 1:1 binary mixtures of methane, ammonia, and water, as well as their 2:1:4 ternary mixture. additionally, a self-diffusion coefficients inside a ternary mixture are calculated along three different uranus interior profiles and compared to a values of a pure compounds. we find that deviations of a linear mixing approximation from a results of a real mixture are generally small; considering a thermal eos they amount to 4% or less. a diffusion coefficients inside a mixture agree with those of a pure compounds within 20% or better. finally, the new adiabatic model of uranus with an inner layer of almost pure ices was developed. a model was consistent with a gravity field data and results inside the rather cold interior ($\mathrm{t_{core}} \mathtt{\sim}$ 4000 k)."
"a purpose of this paper was to explore the resolution considering a faint young sun paradox that has been mostly rejected by a community, namely a possibility of the somewhat more massive young sun with the large mass loss rate sustained considering two to three billion years. this would make a young sun bright enough to keep both a terrestrial and martian oceans from freezing, and thus resolve a paradox. it was found that the large and sustained mass loss was consistent with a well observed spin-down rate of sun-like stars, and indeed may be required considering it. it was concluded that the more massive young sun must be considered the plausible hypothesis."
"a topological hall effect (the), as one of a primary manifestations of non-trivial topology of chiral skyrmions, was traditionally used to detect a emergence of skyrmion lattices with locally ferromagnetic order. inside this work we demonstrate that a appearance of non-trivial two-dimensional chiral textures with locally {\it anti}-ferromagnetic order should be detected through a spin version of a a $-$ a topological spin hall effect (tshe). utilizing a semiclassical formalism, here used to combine chiral antiferromagnetic textures with the density functional theory description of a collinear, degenerate electronic structure, we follow a real-space real-time evolution of electronic su(2) wavepackets inside an external electric field to demonstrate a emergence of sizeable transverse pure spin current inside synthetic antiferromagnets of a fe/cu/fe trilayer type. we further unravel a extreme sensitivity of a tshe to a details of a electronic structure, suggesting that a magnitude and sign of a tshe inside transition-metal synthetic antiferromagnets should be engineered by tuning such parameters as a thickness or band filling. besides being an important step inside our understanding of a topological properties of ever more complex skyrmionic systems, our results bear great potential inside stimulating a discovery of antiferromagnetic skyrmions."
"inside this paper, we consider a generalized linear models (glm) with heavy-tailed features and corruptions. besides clipping a response, we propose to shrink a feature vector by its $\ell_4$-norm under a low dimensional regime and clip each entry of a feature vector inside a high-dimensional regime. under bounded fourth moment assumptions, we show that a mle based on shrunk or clipped data enjoys nearly a minimax optimal rate with exponential deviation bound. simulations demonstrate significant improvement inside statistical performance by feature shrinkage and clipping inside linear regression with heavy-tailed noise and logistic regression with noisy labels. we also apply shrinkage to deep features of mnist images and find that classifiers trained by shrunk deep features are fairly robust to noisy labels: it achieves $0.9\%$ testing error inside a presence of $40\%$ mislabeled data."
"estimating a time lag between two hydrogeologic time series (e.g. precipitation and water levels inside an aquifer) was of significance considering the hydrogeologist-modeler. inside this paper, we present the method to quantify such lags by adapting a visibility graph algorithm, which converts time series into the mathematical graph. we present simulation results to assess a performance of a method. we also illustrate a utility of our idea behind the method with the help of the real world hydrogeologic dataset."
"knowledge matters: importance of prior information considering optimization [7], by gulcehre et. al., sought to establish a limits of current black-box, deep learning techniques by posing problems which are difficult to learn without engineering knowledge into a model or training procedure. inside our work, we completely solve a previous knowledge matters problem with the help of the generic model, pose the more difficult and scalable problem, all-pairs, and advance this new problem by introducing the new learned, spatially-varying histogram model called typenet which outperforms conventional models on a problem. we present results on all-pairs where our model achieves 100% test accuracy while a best resnet models achieve 79% accuracy. inside addition, our model was more than an order of magnitude smaller than resnet-34. a challenge of solving larger-scale all-pairs problems with high accuracy was presented to a community considering investigation."
"online social networks (osns) attract billions of users to share information and communicate where viral marketing has emerged as the new way to promote a sales of products. an osn provider was often hired by an advertiser to conduct viral marketing campaigns. a osn provider generates revenue from a commission paid by a advertiser which was determined by a spread of its product information. meanwhile, to propagate influence, a activities performed by users such as viewing video ads normally induce diffusion cost to a osn provider. inside this paper, we aim to find the seed set to optimize the new profit metric that combines a benefit of influence spread with a cost of influence propagation considering a osn provider. under many diffusion models, our profit metric was a difference between two submodular functions which was challenging to optimize as it was neither submodular nor monotone. we design the general two-phase framework to select seeds considering profit maximization and develop several bounds to measure a quality of a seed set constructed. experimental results with real osn datasets show that our idea behind the method should achieve high approximation guarantees and significantly outperform a baseline algorithms, including state-of-the-art influence maximization algorithms."
"motivated by some recent works on bps invariants of open strings/knot invariants, we guess there may be the general correspondence between a ooguri-vafa invariants of toric calabi-yau 3-folds and cohomologies of nakajima quiver varieties. inside this short note, we provide the toy model to explain this correspondence. more precisely, we study a topological open string model of $\mathbb{c}^3$ with one aganagic-vafa brane $\mathcal{d}_\tau$, and we show that, when $\tau\leq 0$, its ooguri-vafa invariants are given by a betti numbers of certain quiver variety. moreover, a existence of ooguri-vafa invariants implies an infinite product formula. inside particular, we find that a $\tau=1$ case of such infinite product formula was closely related to a celebrated rogers-ramanujan identities."
"we use transport and neutron scattering to study a electronic phase diagram and spin excitations of nafe$_{1-x}$cu$_x$as single crystals. similar to co- and ni-doped nafeas, the bulk superconducting phase appears near $x\approx2\%$ with a suppression of stripe-type magnetic order inside nafeas. upon further increasing cu concentration a system becomes insulating, culminating inside an antiferromagnetically ordered insulating phase near $x\approx 50\%$. with the help of transport measurements, we demonstrate that a resistivity inside nafe$_{1-x}$cu$_x$as exhibits non-fermi-liquid behavior near $x\approx1.8\%$. our inelastic neutron scattering experiments reveal the single neutron spin resonance mode exhibiting weak dispersion along $c$-axis inside nafe$_{0.98}$cu$_{0.02}$as. a resonance was high inside energy relative to a superconducting transition temperature $t_{\rm c}$ but weak inside intensity, likely resulting from impurity effects. these results are similar to other iron pnictides superconductors despite a superconducting phase inside nafe$_{1-x}$cu$_x$as was continuously connected to an antiferromagnetically ordered insulating phase near $x\approx 50\%$ with significant electronic correlations. therefore, electron correlations was an important ingredient of superconductivity inside nafe$_{1-x}$cu$_x$as and other iron pnictides."
"we report temperature (t) dependence of dc magnetization, electrical resistivity (rho(t)), and heat-capacity of rare-earth (r) compounds, gd3rusn6 and tb3rusn6, which are found to crystallize inside a yb3cosn6-type orthorhombic structure (space group: cmcm). a results establish that there was an onset of antiferromagnetic order near (t_n) 19 and 25 k respectively. inside addition, we find that there was another magnetic transition considering both a cases around 14 and 17 k respectively. inside a case of a gd compound, a spin-scattering contribution to rho was found to increase below 75 k as a material was cooled towards t_n, thereby resulting inside the minimum inside a plot of rho(t) unexpected considering gd based systems. isothermal magnetization at 1.8 k reveals an upward curvature around 50 koe. isothermal magnetoresistance plots show interesting anomalies inside a magnetically ordered state. there are sign reversals inside a plot of isothermal entropy change versus t inside a magnetically ordered state, indicating subtle changes inside a spin reorientation with t. a results reveal that these compounds exhibit interesting magnetic properties."
"consider a gaussian vector model with mean value {\theta}. we study a twin problems of estimating a number |{\theta}|_0 of non-zero components of {\theta} and testing whether |{\theta}|_0 was smaller than some value. considering testing, we establish a minimax separation distances considering this model and introduce the minimax adaptive test. extensions to a case of unknown variance are also discussed. rewriting a approximation of |{\theta}|_0 as the multiple testing problem of all hypotheses {|{\theta}|_0 <= q}, we both derive the new way of assessing a optimality of the sparsity estimator and we exhibit such an optimal procedure. this general idea behind the method provides the roadmap considering estimating a complexity of a signal inside various statistical models."
feedback delay networks (fdns) belong to the general class of recursive filters which are widely used inside sound synthesis and physical modeling applications. we present the numerical technique to compute a modal decomposition of a fdn transfer function. a proposed pole finding algorithm was based on a ehrlich-aberth iteration considering matrix polynomials and has improved computational performance of up to three orders of magnitude compared to the scalar polynomial root finder. we demonstrate how explicit knowledge of a fdn's modal behavior facilitates analysis and improvements considering artificial reverberation. a statistical distribution of mode frequency and residue magnitudes demonstrate that relatively few modes contribute the large portion of impulse response energy.
"we propose the two-step algorithm considering optimal controlled islanding that partitions the power grid into islands of limited volume while optimizing several criteria: high generator coherency in islands, minimum power flow disruption due to teared lines, and minimum load shedding. several spectral clustering strategies are used inside a first step to lower a problem dimension (taking into account coherency and disruption only), and cplex tools considering a mixed-integer quadratic problem are employed inside a second step to choose the balanced partition of a aggregated grid that minimizes the combination of coherency, disruption and load shedding. the greedy heuristic efficiently limits search space by generating starting solution considering exact algorithm. dimension of a second-step problem depends only on a desired number of islands k instead of a dimension of a original grid. a algorithm was tested on standard systems with 118, 2383, and 9241 nodes showing high quality of partitions and competitive computation time."
"inside recent years much effort has been concentrated towards achieving polynomial time lower bounds on algorithms considering solving various well-known problems. the useful technique considering showing such lower bounds was to prove them conditionally based on well-studied hardness assumptions such as 3sum, apsp, seth, etc. this line of research helps to obtain the better understanding of a complexity in p. the related question asks to prove conditional space lower bounds on data structures that are constructed to solve certain algorithmic tasks after an initial preprocessing stage. this question received little attention inside previous research even though it has potential strong impact. inside this paper we address this question and show that surprisingly many of a well-studied hard problems that are known to have conditional polynomial time lower bounds are also hard when concerning space. this hardness was shown as the tradeoff between a space consumed by a data structure and a time needed to answer queries. a tradeoff may be either smooth or admit one or more singularity points. we reveal interesting connections between different space hardness conjectures and present matching upper bounds. we also apply these hardness conjectures to both static and dynamic problems and prove their conditional space hardness. we believe that this novel framework of polynomial space conjectures should play an important role inside expressing polynomial space lower bounds of many important algorithmic problems. moreover, it seems that it should also aid inside achieving the better understanding of a hardness of their corresponding problems inside terms of time."
"one of a most challenging tasks when adopting bayesian networks (bns) was a one of learning their structure from data. this task was complicated by a huge search space of possible solutions, and by a fact that a problem was np-hard. hence, full enumeration of all a possible solutions was not always feasible and approximations are often required. however, to a best of our knowledge, the quantitative analysis of a performance and characteristics of a different heuristics to solve this problem has never been done before. considering this reason, inside this work, we provide the detailed comparison of many different state-of-the-arts methods considering structural learning on simulated data considering both bns with discrete and continuous variables, and with different rates of noise inside a data. inside particular, we investigate a performance of different widespread scores and algorithmic approaches proposed considering a inference and a statistical pitfalls within them."
"sparsity inducing regularization was an important part considering learning over-complete visual representations. despite a popularity of $\ell_1$ regularization, inside this paper, we investigate a usage of non-convex regularizations inside this problem. our contribution consists of three parts. first, we propose a leaky capped norm regularization (lcnr), which allows model weights below the certain threshold to be regularized more strongly as opposed to those above, therefore imposes strong sparsity and only introduces controllable approximation bias. we propose the majorization-minimization algorithm to optimize a joint objective function. second, our study over monocular 3d shape recovery and neural networks with lcnr outperforms $\ell_1$ and other non-convex regularizations, achieving state-of-the-art performance and faster convergence. third, we prove the theoretical global convergence speed on a 3d recovery problem. to a best of our knowledge, this was a first convergence analysis of a 3d recovery problem."
"background: a direct modeling of water networks was not the common practice inside modern epidemiology. while space often serves as the proxy, it should be problematic. there are multiple ways to directly model water networks, but these methods are not straightforward and should be difficult to implement. this study suggests the simple idea behind the method considering modeling water networks and diseases, and applies this method to the dataset of self-reported gastrointestinal conditions from the questionnaire-based population health survey inside central norway. method: our idea behind the method was based on the standard conditional autoregressive (car) model. an inverse matrix is constructed, with nodes weighted based on a distance to neighboring nodes within a networks. this matrix is then fitted as the generic model. to illustrate its possible use, we utilized data taken from the questionnaire-based population health survey, a hunt study, to measure self-reported gastrointestinal complaints. considering hypothesis testing, we used a deviance information criterion (dic) and included variables inside the stepwise manner. results: a full model converged after six hours. we found no relation between a water networks and a health conditions of people whose residences connected to different parts of a network inside a geographical area studied. conclusion: all water network models are simplifications of a real networks. nevertheless, we suggest the valid idea behind the method considering distinguishing between a general spatial effect and a water network with the help of the generic model."
"mass segmentation provides effective morphological features which are important considering mass diagnosis. inside this work, we propose the novel end-to-end network considering mammographic mass segmentation which employs the fully convolutional network (fcn) to model the potential function, followed by the crf to perform structured learning. because a mass distribution varies greatly with pixel position, a fcn was combined with the position priori. further, we employ adversarial training to eliminate over-fitting due to a small sizes of mammogram datasets. multi-scale fcn was employed to improve a segmentation performance. experimental results on two public datasets, inbreast and ddsm-bcrp, demonstrate that our end-to-end network achieves better performance than state-of-the-art approaches. \footnote{this https url}"
"we propose an inlier-based outlier detection method capable of both identifying a outliers and explaining why they are outliers, by identifying a outlier-specific features. specifically, we employ an inlier-based outlier detection criterion, which uses a ratio of inlier and test probability densities as the measure of plausibility of being an outlier. considering estimating a density ratio function, we propose the localized logistic regression algorithm. thanks to a locality of a model, variable selection should be outlier-specific, and will aid interpret why points are outliers inside the high-dimensional space. through synthetic experiments, we show that a proposed algorithm should successfully detect a important features considering outliers. moreover, we show that a proposed algorithm tends to outperform existing algorithms inside benchmark datasets."
"we explore a energy landscape of the simple neural network. inside particular, we expand upon previous work demonstrating that a empirical complexity of fitted neural networks was vastly less than the naive parameter count would suggest and that this implicit regularization was actually beneficial considering generalization from fitted models."
"although great progresses have been made inside automatic speech recognition (asr), significant performance degradation was still observed when recognizing multi-talker mixed speech. inside this paper, we propose and evaluate several architectures to address this problem under a assumption that only the single channel of mixed signal was available. our technique extends permutation invariant training (pit) by introducing a front-end feature separation module with a minimum mean square error (mse) criterion and a back-end recognition module with a minimum cross entropy (ce) criterion. more specifically, during training we compute a average mse or ce over a whole utterance considering each possible utterance-level output-target assignment, pick a one with a minimum mse or ce, and optimize considering that assignment. this strategy elegantly solves a label permutation problem observed inside a deep learning based multi-talker mixed speech separation and recognition systems. a proposed architectures are evaluated and compared on an artificially mixed ami dataset with both two- and three-talker mixed speech. a experimental results indicate that our proposed architectures should cut a word error rate (wer) by 45.0% and 25.0% relatively against a state-of-the-art single-talker speech recognition system across all speakers when their energies are comparable, considering two- and three-talker mixed speech, respectively. to our knowledge, this was a first work on a multi-talker mixed speech recognition on a challenging speaker-independent spontaneous large vocabulary continuous speech task."
"assuming stationarity was unrealistic inside many time series applications. the more realistic alternative was to allow considering piecewise stationarity, where a model was allowed to change at given time points. inside this article, a problem of detecting a change points inside the high-dimensional piecewise vector autoregressive model (var) was considered. reformulated a problem as the high-dimensional variable selection, the penalized least square approximation with the help of total variation lasso penalty was proposed considering approximation of model parameters. it was shown that a developed method over-estimates a number of change points. the backward selection criterion was thus proposed inside conjunction with a penalized least square estimator to tackle this issue. we prove that a proposed two-stage procedure consistently detects a number of change points and their locations. the block coordinate descent algorithm was developed considering efficient computation of model parameters. a performance of a method was illustrated with the help of several simulation scenarios."
"unsupervised rank aggregation on score-based permutations, which was widely used inside many applications, has not been deeply explored yet. this work studies a use of submodular optimization considering rank aggregation on score-based permutations inside an unsupervised way. specifically, we propose an unsupervised idea behind the method based on a lovasz bregman divergence considering setting up linear structured convex and nested structured concave objective functions. inside addition, stochastic optimization methods are applied inside a training process and efficient algorithms considering inference should be guaranteed. a experimental results from information retrieval, combining distributed neural networks, influencers inside social networks, and distributed automatic speech recognition tasks demonstrate a effectiveness of a proposed methods."
"we consider a problem of performing inverse reinforcement learning when a trajectory of a expert was not perfectly observed by a learner. instead, the noisy continuous-time observation of a trajectory was provided to a learner. this problem exhibits wide-ranging applications and a specific application we consider here was a scenario inside which a learner seeks to penetrate the perimeter patrolled by the robot. a learner's field of view was limited due to which it cannot observe a patroller's complete trajectory. instead, we allow a learner to listen to a expert's movement sound, which it should also use to approximate a expert's state and action with the help of an observation model. we treat a expert's state and action as hidden data and present an algorithm based on expectation maximization and maximum entropy principle to solve a non-linear, non-convex problem. related work considers discrete-time observations and an observation model that does not include actions. inside contrast, our technique takes expectations over both state and action of a expert, enabling learning even inside a presence of extreme noise and broader applications."
"the 1978 theorem of kozen states that two graphs on $n$ vertices are isomorphic if and only if there was the clique of size $n$ inside a weak modular product between a two graphs. restricting to bipartite graphs and considering complete bipartite subgraphs (bicliques) therein, we study a combinatorics of a weak modular product. we identify cases where isomorphism was tractable with the help of this approach, which we call isomorphism using biclique enumeration (ivbe). we find that ivbe was polynomial considering bipartite $2k_2$-free graphs and quasi-polynomial considering families of bipartite graphs, where a largest induced matching and largest induced crown graph grows slowly inside $n$, that is, $o(\mathrm{polylog }\, n)$. furthermore, as expected the straightforward corollary of kozen's theorem and lovász's sandwich theorem was if a weak modular product between two graphs was perfect, then checking if a graphs are isomorphic was polynomial inside $n$. however, we show that considering balanced, bipartite graphs this was only true inside the few trivial cases. inside doing so we define the new graph product on bipartite graphs, a very weak modular product. a results pertaining to bicliques inside bipartite graphs proved here may be of independent interest."
"trained recurrent networks are powerful tools considering modeling dynamic neural computations. we present the target-based method considering modifying a full connectivity matrix of the recurrent network to train it to perform tasks involving temporally complex input/output transformations. a method introduces the second network during training to provide suitable ""target"" dynamics useful considering performing a task. because it exploits a full recurrent connectivity, a method produces networks that perform tasks with fewer neurons and greater noise robustness than traditional least-squares (force) approaches. inside addition, we show how introducing additional input signals into a target-generating network, which act as task hints, greatly extends a range of tasks that should be learned and provides control over a complexity and nature of a dynamics of a trained, task-performing network."
"modern scientific studies often require a identification of the subset of relevant explanatory variables, inside a attempt to understand an interesting phenomenon. several statistical methods have been developed to automate this task, but only recently has a framework of model-free knockoffs proposed the general solution that should perform variable selection under rigorous type-i error control, without relying on strong modeling assumptions. inside this paper, we extend a methodology of model-free knockoffs to the rich family of problems where a distribution of a covariates should be described by the hidden markov model (hmm). we develop an exact and efficient algorithm to sample knockoff copies of an hmm. we then argue that combined with a knockoffs selective framework, they provide the natural and powerful tool considering performing principled inference inside genome-wide association studies with guaranteed fdr control. finally, we apply our methodology to several datasets aimed at studying a crohn's disease and several continuous phenotypes, e.g. levels of cholesterol."
"learning a model parameters of the multi-object dynamical system from partial and perturbed observations was the challenging task. despite recent numerical advancements inside learning these parameters, theoretical guarantees are extremely scarce. inside this article, we study a identifiability of these parameters and a consistency of a corresponding maximum likelihood approximate (mle) under assumptions on a different components of a underlying multi-object system. inside order to understand a impact of a various sources of observation noise on a ability to learn a model parameters, we study a asymptotic variance of a mle through a associated fisher information matrix. considering example, we show that specific aspects of a multi-target tracking (mtt) problem such as detection failures and unknown data association lead to the loss of information which was quantified inside special cases of interest."
"let $m$ be the compact complex manifold admitting the kähler structure. the conformally kähler, einstein-maxwell metric (ckem metric considering short) was the hermitian metric $\tilde{g}$ on $m$ with constant scalar curvature such that there was the positive smooth function $f$ with $g = f^2 \tilde{g}$ being the kähler metric and $f$ being the killing hamiltonian potential with respect to $g$. fixing the kähler class, we characterize such killing vector fields whose hamiltonian function $f$ with respect to some kähler metric $g$ inside a fixed kähler class gives the ckem metric $\tilde{g} = f^{-2}g$. a characterization was described inside terms of critical points of certain volume functional. a conceptual idea was similar to a cases of kähler-ricci solitons and sasaki-einstein metrics inside that a derivative of a volume functional gives rise to the natural obstruction to a existence of ckem metrics. however, unlike a kähler-ricci soliton case and sasaki-einstein case, a functional was neither convex nor proper inside general, and often has more than one critical points. a last observation matches well with a ambitoric examples studied earlier by lebrun and apostolov-maschler."
"this paper analyzes consumer choices over lunchtime restaurants with the help of data from the sample of several thousand anonymous mobile phone users inside a san francisco bay area. a data was used to identify users' approximate typical morning location, as well as their choices of lunchtime restaurants. we build the model where restaurants have latent characteristics (whose distribution may depend on restaurant observables, such as star ratings, food category, and price range), each user has preferences considering these latent characteristics, and these preferences are heterogeneous across users. similarly, each item has latent characteristics that describe users' willingness to travel to a restaurant, and each user has individual-specific preferences considering those latent characteristics. thus, both users' willingness to travel and their base utility considering each restaurant vary across user-restaurant pairs. we use the bayesian idea behind the method to estimation. to make a approximation computationally feasible, we rely on variational inference to approximate a posterior distribution, as well as stochastic gradient descent as the computational approach. our model performs better than more standard competing models such as multinomial logit and nested logit models, inside part due to a personalization of a estimates. we analyze how consumers re-allocate their demand after the restaurant closes to nearby restaurants versus more distant restaurants with similar characteristics, and we compare our predictions to actual outcomes. finally, we show how a model should be used to analyze counterfactual questions such as what type of restaurant would attract a most consumers inside the given location."
"the basic result inside a elementary theory of continued fractions says that two real numbers share a same tail inside their continued fraction expansions iff they belong to a same orbit under a projective action of pgl(2,z). this result is first formulated inside serret's cours d'algèbre supérieure, so we'll refer to it as to a serret theorem. notwithstanding a abundance of continued fraction algorithms inside a literature, the uniform treatment of a serret result seems missing. inside this paper we show that there are finitely many possibilities considering a subgroups sigma of pgl(2,z) generated by a branches of a gauss maps inside the large family of algorithms, and that each sigma-equivalence class of reals was partitioned inside finitely many tail-equivalence classes, whose number we bound. our idea behind the method was through a finite-state transducers that relate gauss maps to each other. they constitute opfibrations of a schreier graphs of a groups, and their synchronizability ---which may or may not hold--- assures a a.e. validity of a serret theorem."
"this paper considers unbalanced multiphase distribution systems with generic topology and different load models, and extends a z-bus iterative load-flow algorithm based on the fixed-point interpretation of a ac load-flow equations. explicit conditions considering existence and uniqueness of load-flow solutions are presented. these conditions also guarantee convergence of a load-flow algorithm to a unique solution. a proposed methodology was applicable to generic systems featuring (i) wye connections; (ii) ungrounded delta connections; (iii) the combination of wye-connected and delta-connected sources/loads; and, (iv) the combination of line-to-line and line-to-grounded-neutral devices at a secondary of distribution transformers. further, the sufficient condition considering a non-singularity of a load-flow jacobian was proposed. finally, linear load-flow models are derived, and their approximation accuracy was analyzed. theoretical results are corroborated through experiments on ieee test feeders."
"the fundamental computation considering statistical inference and accurate decision-making was to compute a marginal probabilities or most probable states of task-relevant variables. probabilistic graphical models should efficiently represent a structure of such complex data, but performing these inferences was generally difficult. message-passing algorithms, such as belief propagation, are the natural way to disseminate evidence amongst correlated variables while exploiting a graph structure, but these algorithms should struggle when a conditional dependency graphs contain loops. here we use graph neural networks (gnns) to learn the message-passing algorithm that solves these inference tasks. we first show that a architecture of gnns was well-matched to inference tasks. we then demonstrate a efficacy of this inference idea behind the method by training gnns on the collection of graphical models and showing that they substantially outperform belief propagation on loopy graphs. our message-passing algorithms generalize out of a training set to larger graphs and graphs with different structure."
"gravitational waves (gws) cause a apparent position of distant stars to oscillate with the characteristic pattern on a sky. astrometric measurements (e.g. those made by gaia) therefore provide the new way to search considering gws. a main difficulty facing such the search was a large size of a data set; gaia observes more than one billion stars. inside this letter a problem of searching considering gws from individually resolvable supermassive black hole binaries with the help of astrometry was addressed considering a first time; it was demonstrated how a data set should be compressed by the factor of more than $10^6$, with the loss of sensitivity of less than $1\%$. this technique was successfully used to recover artificially injected gws from mock gaia data. repeated injections are used to calculate a sensitivity of gaia as the function of frequency, and gaia's directional sensitivity variation, or antenna pattern. throughout a letter a complementarity of gaia and pulsar timing searches considering gws was highlighted."
"we present an asymptotic criterion to determine a optimal number of clusters inside k-means. we consider k-means as data compression, and propose to adopt a number of clusters that minimizes a estimated description length after compression. here we report two types of compression ratio based on two ways to quantify a description length of data after compression. this idea behind the method further offers the way to evaluate whether clusters obtained with k-means have the hierarchical structure by examining whether multi-stage compression should further reduce a description length. we applied our criteria to determine a number of clusters to synthetic data and empirical neuroimaging data to observe a behavior of a criteria across different types of data set and suitability of a two types of criteria considering different datasets. we found that our method should offer reasonable clustering results that are useful considering dimension reduction. while our numerical results revealed dependency of our criteria on a various aspects of dataset such as a dimensionality, a description length idea behind the method proposed here provides the useful guidance to determine a number of clusters inside the principled manner when underlying properties of a data are unknown and only inferred from observation of data."
"social network analysis was leveraged inside the variety of applications such as identifying influential entities, detecting communities with special interests, and determining a flow of information and innovations. however, existing approaches considering extracting social networks from unstructured web content do not scale well and are only feasible considering small graphs. inside this paper, we introduce novel methodologies considering query-based search engine mining, enabling efficient extraction of social networks from large amounts of web data. to this end, we use patterns inside phrase queries considering retrieving entity connections, and employ the bootstrapping idea behind the method considering iteratively expanding a pattern set. our experimental evaluation inside different domains demonstrates that our algorithms provide high quality results and allow considering scalable and efficient construction of social graphs."
"we propose an innovative method considering measuring a neutral hydrogen (hi) content of an optically-selected spectroscopic sample of galaxies through cross-correlation with hi intensity mapping measurements. we show that a hi-galaxy cross-power spectrum contains an additive shot noise term which scales with a average hi brightness temperature of a optically-selected galaxies, allowing constraints to be placed on a average hi mass per galaxy. this idea behind the method should approximate a hi content of populations too faint to directly observe through their 21cm emission over the wide range of redshifts. this cross-correlation, as the function of optical luminosity or colour, should be used to derive hi-scaling relations. we demonstrate that this signal will be detectable by cross-correlating upcoming australian ska pathfinder (askap) observations with existing optically-selected samples. we also use semi-analytic simulations to verify that a hi mass should be successfully recovered by our technique inside a range m_hi > 10^8 m_solar, inside the manner independent of a underlying power spectrum shape. we conclude that this method was the powerful tool to study galaxy evolution, which only requires the single intensity mapping dataset to infer complementary hi gas information from existing optical and infra-red observations."
"a aim of this paper was to introduce the new design of experiment method considering a/b tests inside order to balance a covariate information inside all treatment groups. a/b tests (or ""a/b/n tests"") refer to a experiments and a corresponding inference on a treatment effect(s) of the two-level or multi-level controllable experimental factor. a common practice was to use the randomized design and perform hypothesis tests on a estimates. however, such approximation and inference are not always accurate when covariate imbalance exists among a treatment groups. to overcome this issue, we propose the discrepancy-based criterion and show that a design minimizing this criterion significantly improves a accuracy of a treatment effect(s) estimates. a discrepancy-based criterion was model-free and thus makes a approximation of a treatment effect(s) robust to a model assumptions. more importantly, a proposed design was applicable to both continuous and categorical response measurements. we develop two efficient algorithms to construct a designs by optimizing a criterion considering both offline and online a/b tests. through simulation study and the real example, we show that a proposed design idea behind the method achieves good covariate balance and accurate estimation."
"domain generalization aims to apply knowledge gained from multiple labeled source domains to unseen target domains. a main difficulty comes from a dataset bias: training data and test data have different distributions, and a training set contains heterogeneous samples from different distributions. let $x$ denote a features, and $y$ be a class labels. existing domain generalization methods address a dataset bias problem by learning the domain-invariant representation $h(x)$ that has a same marginal distribution $\mathbb{p}(h(x))$ across multiple source domains. a functional relationship encoded inside $\mathbb{p}(y|x)$ was usually assumed to be stable across domains such that $\mathbb{p}(y|h(x))$ was also invariant. however, it was unclear whether this assumption holds inside practical problems. inside this paper, we consider a general situation where both $\mathbb{p}(x)$ and $\mathbb{p}(y|x)$ should change across all domains. we propose to learn the feature representation which has domain-invariant class conditional distributions $\mathbb{p}(h(x)|y)$. with a conditional invariant representation, a invariance of a joint distribution $\mathbb{p}(h(x),y)$ should be guaranteed if a class prior $\mathbb{p}(y)$ does not change across training and test domains. extensive experiments on both synthetic and real data demonstrate a effectiveness of a proposed method."
"we study a real-time and real-space dynamics of charge inside a one-dimensional hubbard model inside a limit of high temperatures. to this end, we prepare pure initial states with sharply peaked density profiles and calculate a time evolution of these nonequilibrium states, by with the help of numerical forward-propagation approaches to chains as long as 20 sites. considering the class of typical states, we find excellent agreement with linear-response theory and unveil a existence of remarkably clean charge diffusion inside a regime of strong particle-particle interactions. moreover, we demonstrate that this diffusive behavior does not depend on certain details of our initial conditions, i.e., it occurs considering five different realizations with random and nonrandom internal degrees of freedom, single and double occupation of a central site, and displacement of spin-up and spin-down particles."
"we propose an approximation method considering a conditional mode when a conditioning variable was high-dimensional. inside a proposed method, we first approximate a conditional density by solving quantile regressions multiple times. we then approximate a conditional mode by finding a maximum of a estimated conditional density. a proposed method has two advantages inside that it was computationally stable because it has no initial parameter dependencies, and it was statistically efficient with the fast convergence rate. synthetic and real-world data experiments demonstrate a better performance of a proposed method compared to other existing ones."
"over half the million individuals are diagnosed with head and neck cancer each year worldwide. radiotherapy was an important curative treatment considering this disease, but it requires manually intensive delineation of radiosensitive organs at risk (oars). this planning process should delay treatment commencement. while auto-segmentation algorithms offer the potentially time-saving solution, a challenges inside defining, quantifying and achieving expert performance remain. adopting the deep learning approach, we demonstrate the 3d u-net architecture that achieves performance similar to experts inside delineating the wide range of head and neck oars. a model is trained on the dataset of 663 deidentified computed tomography (ct) scans acquired inside routine clinical practice and segmented according to consensus oar definitions. we demonstrate its generalisability through application to an independent test set of 24 ct scans available from a cancer imaging archive collected at multiple international sites previously unseen to a model, each segmented by two independent experts and consisting of 21 oars commonly segmented inside clinical practice. with appropriate validation studies and regulatory approvals, this system could improve a effectiveness of radiotherapy pathways."
"inside a field of road safety epidemiology, it was common to use responsibility analyses to assess a effect of the given factor on a risk of being responsible considering an accident, among drivers involved inside an accident only. with the help of a scm framework, we formally showed inside previous works that a causal odds-ratio of the given factor correlated with high speed cannot be unbiasedly estimated through responsibility analyses if inclusion into a dataset depends on a accident severity. a objective of this present work was to present numerical results to give the first quantification of a magnitude of a selection bias induced by responsibility analyses. we denote a binary variables by x a exposure of interest, v a high speed, f a driving fault, r a responsibility of the severe accident, the a severe accident, and w the set of categorical confounders. we illustrate a potential bias by comparing a causal effect of interest of x on r, cor(x,r|w=w), and a estimable odds-ratio available inside responsibility analyses, or(x,r|w=w, a=1). by considering the binary exposure, and by varying the set of parameters, we describe the situation where x could represent alcohol or cannabis intoxication. we confirm that a estimable odds-ratio available inside responsibility analyses was the biased measure of a causal effect when x was correlated with high speed v and v was related to a accident severity a. inside this case, a magnitude of a bias was all a more important that these two relationships are strong. when x was likely to increase a risk to drive fast v, a estimable odds-ratio underestimates a causal effect. when x was likely to decrease a risk to drive fast v, a estimable odds-ratio upperestimates a causal effect. a values of a different causal quantities considered here are from one to five times higher (or lower) than a estimable quantity available inside responsability analyses."
"word embeddings are the popular idea behind the method to unsupervised learning of word relationships that are widely used inside natural language processing. inside this article, we present the new set of embeddings considering medical concepts learned with the help of an extremely large collection of multimodal medical data. leaning on recent theoretical insights, we demonstrate how an insurance claims database of 60 million members, the collection of 20 million clinical notes, and 1.7 million full text biomedical journal articles should be combined to embed concepts into the common space, resulting inside a largest ever set of embeddings considering 108,477 medical concepts. to evaluate our approach, we present the new benchmark methodology based on statistical power specifically designed to test embeddings of medical concepts. our approach, called cui2vec, attains state of a art performance relative to previous methods inside most instances. finally, we provide the downloadable set of pre-trained embeddings considering other researchers to use, as well as an online tool considering interactive exploration of a cui2vec embeddings."
"we study the model where one target variable y was correlated with the vector x:=(x_1,...,x_d) of predictor variables being potential causes of y. we describe the method that infers to what extent a statistical dependences between x and y are due to a influence of x on y and to what extent due to the hidden common cause (confounder) of x and y. a method relies on concentration of measure results considering large dimensions d and an independence assumption stating that, inside a absence of confounding, a vector of regression coefficients describing a influence of each x on y typically has `generic orientation' relative to a eigenspaces of a covariance matrix of x. considering a special case of the scalar confounder we show that confounding typically spoils this generic orientation inside the characteristic way that should be used to quantitatively approximate a amount of confounding."
"density approximation was the classical problem inside statistics and has received considerable attention when both a data has been fully observed and inside a case of partially observed (censored) samples. inside survival analysis or clinical trials, the typical problem encountered inside a data collection stage was that a samples may be censored from a right. a variable of interest could be observed partially due to a presence of the set of events that occur at random and potentially censor a data. consequently, developing the methodology that enables robust approximation of a lifetimes inside such setting was of high interest considering researchers. inside this paper, we propose the non-parametric linear density estimator with the help of empirical wavelet coefficients that are fully data driven. we derive an asymptotically unbiased estimator constructed from a complete sample based on an inductive bias correction procedure. also, we provide upper bounds considering a bias and analyze a large sample behavior of a expected $\mathbb{l}_{2}$ approximation error based on a idea behind the method used by stute (1995), showing that a estimates are asymptotically normal and possess global mean square consistency. inside addition, we evaluate a proposed idea behind the method using the theoretical simulation study with the help of different exemplary baseline distributions with different sample sizes. inside this study, we choose the censoring scheme that produces the censoring proportion of 40\% on average. finally, we apply a proposed estimator to real data-sets previously published, showing that a proposed wavelet estimator provides the robust and useful tool considering a non-parametric approximation of a survival time density function."
"our goal was to synthesize controllers considering robots that provably generalize well to novel environments given the dataset of example environments. a key technical idea behind our idea behind the method was to leverage tools from generalization theory inside machine learning by exploiting the precise analogy (which we present inside a form of the reduction) between robustness of controllers to novel environments and generalization of hypotheses inside supervised learning. inside particular, we utilize a probably approximately correct (pac)-bayes framework, which allows us to obtain upper bounds (that hold with high probability) on a expected cost of (stochastic) controllers across novel environments. we propose control synthesis algorithms that explicitly seek to minimize this upper bound. a corresponding optimization problem should be solved with the help of convex optimization (relative entropy programming inside particular) inside a setting where we are optimizing over the finite control policy space. inside a more general setting of continuously parameterized controllers, we minimize this upper bound with the help of stochastic gradient descent. we present examples of our idea behind the method inside a context of obstacle avoidance control with depth measurements. our simulated examples demonstrate a potential of our idea behind the method to provide strong generalization guarantees on controllers considering robotic systems with continuous state and action spaces, complicated (e.g., nonlinear) dynamics, and rich sensory inputs (e.g., depth measurements)."
performance of data-driven network considering tumor classification varies with stain-style of histopathological images. this article proposes a stain-style transfer (sst) model based on conditional generative adversarial networks (gans) which was to learn not only a certain color distribution but also a corresponding histopathological pattern. our model considers feature-preserving loss inside addition to well-known gan loss. consequently our model does not only transfers initial stain-styles to a desired one but also prevent a degradation of tumor classifier on transferred images. a model was examined with the help of a camelyon16 dataset.
"mendelian randomization (mr) was the popular instrumental variable (iv) approach. the key iv identification condition known as a exclusion restriction requires no direct effect of an iv on a outcome not through a exposure which was unrealistic inside most mr analyses. as the result, possible violation of a exclusion restriction should seldom be ruled out inside such studies. to address this concern, we introduce the new class of iv estimators which are robust to violation of a exclusion restriction under the large collection of data generating mechanisms consistent with parametric models commonly assumed inside a mr literature. our idea behind the method named ""mr g-estimation under no interaction with unmeasured selection"" (mr genius) may be viewed as the modification to robins' g-estimation idea behind the method that was robust to both additive unmeasured confounding and violation of a exclusion restriction assumption. we also establish that approximation with mr genius may also be viewed as the robust generalization of a well-known lewbel estimator considering the triangular system of structural equations with endogeneity. specifically, we show that unlike lewbel estimation, mr genius was under fairly weak conditions also robust to unmeasured confounding of a effects of a genetic ivs, another possible violation of the key iv identification condition. furthermore, while lewbel approximation involves specification of linear models both considering a outcome and a exposure, mr genius generally does not require specification of the structural model considering a direct effect of invalid ivs on a outcome, therefore allowing a latter model to be unrestricted. finally, unlike lewbel estimation, mr genius was shown to equally apply considering binary, discrete or continuous exposure and outcome variables and should be used under prospective sampling, or retrospective sampling such as inside the case-control study."
"inside a submodular welfare maximization (swm) problem, a input consists of the set of $n$ items, each of which must be allocated to one of $m$ agents. each agent $\ell$ has the valuation function $v_\ell$, where $v_\ell(s)$ denotes a welfare obtained by this agent if she receives a set of items $s$. a functions $v_\ell$ are all submodular; as was standard, we assume that they are monotone and $v_\ell(\emptyset) = 0$. a goal was to partition a items into $m$ disjoint subsets $s_1, s_2, \ldots s_m$ inside order to maximize a social welfare, defined as $\sum_{\ell = 1}^m v_\ell(s_\ell)$. inside this paper, we consider a online version of swm. here, items arrive one at the time inside an online manner; when an item arrives, a algorithm must make an irrevocable decision about which agent to assign it to before seeing any subsequent items. this problem was motivated by applications to internet advertising, where user ad impressions must be allocated to advertisers whose value was the submodular function of a set of users / impressions they receive. inside a random order model, a adversary should construct the worst-case set of items and valuations, but does not control a order inside which a items arrive; instead, they are assumed to arrive inside the random order. obtaining the competitive ratio of $1/2 + \omega(1)$ considering a random order model has been an important open problem considering several years. we solve this open problem by demonstrating that a greedy algorithm has the competitive ratio of at least $0.505$ considering a online submodular welfare maximization problem inside a random order model. considering special cases of submodular functions including weighted matching, weighted coverage functions and the broader class of ""second-order supermodular"" functions, we provide the different analysis that gives the competitive ratio of $0.51$."
"deep learning models are vulnerable to adversarial examples, i.e.\ images obtained using deliberate imperceptible perturbations, such that a model misclassifies them with high confidence. however, class confidence by itself was an incomplete picture of uncertainty. we therefore use principled bayesian methods to capture model uncertainty inside prediction considering observing adversarial misclassification. we provide an extensive study with different bayesian neural networks attacked inside both white-box and black-box setups. a behaviour of a networks considering noise, attacks and clean test data was compared. we observe that bayesian neural networks are uncertain inside their predictions considering adversarial perturbations, the behaviour similar to a one observed considering random gaussian perturbations. thus, we conclude that bayesian neural networks should be considered considering detecting adversarial examples."
"multiplex networks are the type of multilayer network inside which entities are connected to each other using multiple types of connections. we propose the method, based on computing pairwise similarities between layers and then doing community detection, considering grouping structurally similar layers inside multiplex networks. we illustrate our idea behind the method with the help of both synthetic and empirical networks, and we are able to find meaningful groups of layers inside both cases. considering example, we find that airlines that are based inside similar geographic locations tend to be grouped together inside an airline multiplex network and that related research areas inside physics tend to be grouped together inside an multiplex collaboration network."
"traditional works on community detection from observations of information cascade assume that the single adjacency matrix parametrizes all a observed cascades. however, inside reality a connection structure usually does not stay a same across cascades. considering example, different people have different topics of interest, therefore a connection structure would depend on a information/topic content of a cascade. inside this paper we consider a case where we observe the sequence of noisy adjacency matrices triggered by information/events with different topic distributions. we propose the novel latent model with the help of a intuition that a connection was more likely to exist between two nodes if they are interested inside similar topics, which are common with a information/event. specifically, we endow each node two node-topic vectors: an influence vector that measures how much influential/authoritative they are on each topic; and the receptivity vector that measures how much receptive/susceptible they are to each topic. we show how these two node-topic structures should be estimated from observed adjacency matrices with theoretical guarantee, inside cases where a topic distributions of a information/events are known, as well as when they are unknown. extensive experiments on synthetic and real data demonstrate a effectiveness of our model."
"understanding a statistics of ocean geostrophic turbulence was of utmost importance inside understanding its interactions with a global ocean circulation and a climate system as the whole. here, the study of eddy-mixing entropy inside the forced-dissipative barotropic ocean model was presented. entropy was the concept of fundamental importance inside statistical physics and information theory; motivated by equilibrium statistical mechanics theories of ideal geophysical fluids, we consider a effect of forcing and dissipation on eddy-mixing entropy, both analytically and numerically. by diagnosing a time evolution of eddy-mixing entropy it was shown that a entropy provides the descriptive tool considering understanding three stages of a turbulence life cycle: growth of instability, formation of large scale structures and steady state fluctuations. further, by determining a relationship between a time evolution of entropy and a maximum entropy principle, evidence was found considering a action of this principle inside the forced-dissipative flow. a maximum entropy potential vorticity statistics are calculated considering a flow and are compared with numerical simulations. deficiencies of a maximum entropy statistics are discussed inside a context of a mean-field approximation considering energy. this study highlights a importance entropy and statistical mechanics inside a study of geostrophic turbulence."
"given an algebraic lie algebra $\mathfrak{g}$ over $\mathbb{c}$, we canonically associate to it the lie algebra $\mathfrak{g}_{\infty}$ defined over $\mathbb{c}_{\infty}$-the reduction of $\mathbb{c}$ mod infinitely large prime, and show that considering the class of lie algebras $\mathfrak{g}_{\infty}$ was an invariant of a derived category of $\mathfrak{g}$-modules. we give two applications of this construction. first, we show that a bounded derived category of $\mathfrak{g}$-modules determines algebra $\mathfrak{g}$ considering the class of lie algebras. second, given the semi-simple lie algebra $\mathfrak{g}$ over $\mathbb{c}$, we construct the canonical homomorphism from a group of automorphisms of a enveloping algebra $\mathfrak{u}\mathfrak{g}$ to a group of lie algebra automorphisms of $\mathfrak{g}$, such that its kernel does not contain the nontrivial semi-simple automorphism. as the corollary we obtain that any finite subgroup of automorphisms of $\mathfrak{u}\mathfrak{g}$ isomorphic to the subgroup of lie algebra automorphisms of $\mathfrak{g}.$"
"inside spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that should converse with humans. the part of this effort was a policy optimisation task, which attempts to find the policy describing how to respond to humans, inside a form of the function taking a current state of a dialogue and returning a response of a system. inside this paper, we investigate deep reinforcement learning approaches to solve this problem. particular attention was given to actor-critic methods, off-policy reinforcement learning with experience replay, and various methods aimed at reducing a bias and variance of estimators. when combined, these methods result inside a previously proposed acer algorithm that gave competitive results inside gaming environments. these environments however are fully observable and have the relatively small action set so inside this paper we examine a application of acer to dialogue policy optimisation. we show that this method beats a current state-of-the-art inside deep learning approaches considering spoken dialogue systems. this not only leads to the more sample efficient algorithm that should train faster, but also allows us to apply a algorithm inside more difficult environments than before. we thus experiment with learning inside the very large action space, which has two orders of magnitude more actions than previously considered. we find that acer trains significantly faster than a current state-of-the-art."
"flexibility was the key enabler considering a smart grid, required to facilitate demand side management (dsm) programs, managing electrical consumption to reduce peaks, balance renewable generation and provide ancillary services to a grid. flexibility analysis was required to identify and quantify a available electrical load of the site or building which should be shed or increased inside response to the dsm signal. the methodology considering assessing flexibility was developed, based on flexibility formulations and optimization requirements. a methodology characterizes a loads, storage and on-site generation, incorporates site assessment with the help of a iso 50002:2014 energy audit standard and benchmarks performance against documented studies. an example application of a methodology was detailed with the help of the pilot site demonstrator."
"this paper introduces the new surgical end-effector probe, which allows to accurately apply the contact force on the tissue, while at a same time allowing considering high resolution and highly repeatable probe movement. these are achieved by implementing the cable-driven parallel manipulator arrangement, which was deployed at a distal-end of the robotic instrument. a combination of a offered qualities should be advantageous inside several ways, with possible applications including: large area endomicroscopy and multi-spectral imaging, micro-surgery, tissue palpation, safe energy-based and conventional tissue resection. to demonstrate a concept and its adaptability, a probe was integrated with the modified da vinci robot instrument."
"inside this article we develop energy methods considering the large class of linear and nonlinear dirac-type equations inside two-dimensional minkowski space. we will derive existence results considering several dirac-type equations originating inside quantum field theory, inside particular considering dirac-wave maps to compact riemannian manifolds."
"a paper presents the solution to a boltzmann kinetic equation based on a construction of its discrete conservative model. discrete analogue of a collision integral was presented as the contraction of the tensor, which was independent from a initial distribution function, colliding with the tensor composed of medium densities inside a cells. numerical implementation of a discrete model was demonstrated on a example of a isotropic gas relaxation problem applied to a hard spheres model. a key feature of a method was independence of a collision tensor components from a distribution function. consequently a components of a collision tensor are calculated once considering various initial distribution functions, which substantially increases performance of a suggested method."
"extremely metal-poor, high-ionizing starbursts inside a local universe provide unique laboratories considering exploring inside detail a physics of high-redshift systems. also, their ongoing star-formation and haphazard morphology make them outstanding proxies considering primordial galaxies. with the help of integral field spectroscopy, we spatially resolved a ism properties and massive stars of two first-class low metallicity galaxies with wolf-rayet features and nebular heii emission: mrk178 and izw18. inside this review, we summarize our main results considering these two objects."
"edge/surface states often appear inside the topologically nontrivial phase, when a system has the boundary. a edge state of the one-dimensional topological insulator was one of a simplest examples. electron spin resonance (esr) was an ideal probe to detect and analyze a edge state considering its high sensitivity and precision. we consider esr of a edge state of the generalized su-schrieffer-heeger model with the next-nearest neighbor (nnn) hopping and the staggered spin-orbit coupling. a spin-orbit coupling was generally expected to bring about nontrivial changes on a esr spectrum. nevertheless, inside a absence of a nnn hoppings, we find that a esr spectrum was unaffected by a spin-orbit coupling thanks to a chiral symmetry. inside a presence of both a nnn hopping and a spin-orbit coupling, on a other hand, a edge esr spectrum exhibits the nontrivial frequency shift. we derive an explicit analytical formula considering a esr shift inside a second order perturbation theory, which agrees very well with the non-perturbative numerical calculation."
"a optimal frequency considering interstellar communication, with the help of ""earth 2017"" technology, is derived inside papers i and ii of this series (arxiv:1706.03795, arxiv:1706.05570). a framework included models considering a loss of photons from diffraction (free space), interstellar extinction, and atmospheric transmission. the major limit of current technology was a focusing of wavelengths $\lambda<300\,$nm (uv). when this technological constraint was dropped, the physical bound was found at $\lambda\approx1\,$nm ($e\approx\,$kev) considering distances out to kpc. while shorter wavelengths may produce tighter beams and thus higher data rates, a physical limit comes from surface roughness of focusing devices at a atomic level. this limit should be surpassed by beam-forming with electromagnetic fields, e.g. with the help of the free electron laser, but such methods are not energetically competitive. current lasers are not yet cost efficient at nm wavelengths, with the gap of two orders of magnitude, but future technological progress may converge on a physical optimum. we recommend expanding seti efforts towards targeted (at us) monochromatic (or narrow band) x-ray emission at 0.5-2 kev energies."
"a last decade has seen the surge of interest inside adaptive learning algorithms considering data stream classification, with applications ranging from predicting ozone level peaks, learning stock market indicators, to detecting computer security violations. inside addition, the number of methods have been developed to detect concept drifts inside these streams. consider the scenario where we have the number of classifiers with diverse learning styles and different drift detectors. intuitively, a current 'best' (classifier, detector) pair was application dependent and may change as the result of a stream evolution. our research builds on this observation. we introduce a $\mbox{tornado}$ framework that implements the reservoir of diverse classifiers, together with the variety of drift detection algorithms. inside our framework, all (classifier, detector) pairs proceed, inside parallel, to construct models against a evolving data streams. at any point inside time, we select a pair which currently yields a best performance. we further incorporate two novel stacking-based drift detection methods, namely a $\mbox{fhddms}$ and $\mbox{fhddms}_{add}$ approaches. a experimental evaluation confirms that a current 'best' (classifier, detector) pair was not only heavily dependent on a characteristics of a stream, but also that this selection evolves as a stream flows. further, our $\mbox{fhddms}$ variants detect concept drifts accurately inside the timely fashion while outperforming a state-of-the-art."
"deep learning classifiers are known to be inherently vulnerable to manipulation by intentionally perturbed inputs, named adversarial examples. inside this work, we establish that reinforcement learning techniques based on deep q-networks (dqns) are also vulnerable to adversarial input perturbations, and verify a transferability of adversarial examples across different dqn models. furthermore, we present the novel class of attacks based on this vulnerability that enable policy manipulation and induction inside a learning process of dqns. we propose an attack mechanism that exploits a transferability of adversarial examples to implement policy induction attacks on dqns, and demonstrate its efficacy and impact through experimental study of the game-learning scenario."
"measurements of radial velocity variations from a spectroscopic monitoring of stars and their companions are essential considering the broad swath of astrophysics, providing access to a fundamental physical properties that dictate all phases of stellar evolution and facilitating a quantitative study of planetary systems. a conversion of those measurements into both constraints on a orbital architecture and individual component spectra should be the serious challenge, however, especially considering extreme flux ratio systems and observations with relatively low sensitivity. gaussian processes define sampling distributions of flexible, continuous functions that are well-motivated considering modeling stellar spectra, enabling proficient search considering companion lines inside time-series spectra. we introduce the new technique considering spectral disentangling, where a posterior distributions of a orbital parameters and intrinsic, rest-frame stellar spectra are explored simultaneously without needing to invoke cross-correlation templates. to demonstrate its potential, this technique was deployed on red-optical time-series spectra of a mid-m dwarf binary lp661-13. we report orbital parameters with improved precision compared to traditional radial velocity analysis and successfully reconstruct a primary and secondary spectra. we discuss potential applications considering other stellar and exoplanet radial velocity techniques and extensions to time-variable spectra. a code used inside this analysis was freely available as an open source python package."
"we develop shopper, the sequential probabilistic model of shopping data. shopper uses interpretable components to model a forces that drive how the customer chooses products; inside particular, we designed shopper to capture how items interact with other items. we develop an efficient posterior inference algorithm to approximate these forces from large-scale data, and we analyze the large dataset from the major chain grocery store. we are interested inside answering counterfactual queries about changes inside prices. we found that shopper provides accurate predictions even under price interventions, and that it helps identify complementary and substitutable pairs of products."
"generative adversarial networks (gans) have great successes on synthesizing data. however, a existing gans restrict a discriminator to be the binary classifier, and thus limit their learning capacity considering tasks that need to synthesize output with rich structures such as natural language descriptions. inside this paper, we propose the novel generative adversarial network, rankgan, considering generating high-quality language descriptions. rather than training a discriminator to learn and assign absolute binary predicate considering individual data sample, a proposed rankgan was able to analyze and rank the collection of human-written and machine-written sentences by giving the reference group. by viewing the set of data samples collectively and evaluating their quality through relative ranking scores, a discriminator was able to make better assessment which inside turn helps to learn the better generator. a proposed rankgan was optimized through a policy gradient technique. experimental results on multiple public datasets clearly demonstrate a effectiveness of a proposed approach."
"a independence clustering problem was considered inside a following formulation: given the set $s$ of random variables, it was required to find a finest partitioning $\{u_1,\dots,u_k\}$ of $s$ into clusters such that a clusters $u_1,\dots,u_k$ are mutually independent. since mutual independence was a target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable. a distribution of a random variables inside $s$ is, inside general, unknown, but the sample was available. thus, a problem was cast inside terms of time series. two forms of sampling are considered: i.i.d.\ and stationary time series, with a main emphasis being on a latter, more general, case. the consistent, computationally tractable algorithm considering each of a settings was proposed, and the number of open directions considering further research are outlined."
"deep neural networks (dnn) have demonstrated superior ability to extract high level embedding vectors from low level features. despite a success, a serving time was still a bottleneck due to expensive run-time computation of multiple layers of dense matrices. gpgpu, fpga, or asic-based serving systems require additional hardware that are not inside a mainstream design of most commercial applications. inside contrast, tree or forest-based models are widely adopted because of low serving cost, but heavily depend on carefully engineered features. this work proposes the deep embedding forest model that benefits from a best of both worlds. a model consists of the number of embedding layers and the forest/tree layer. a former maps high dimensional (hundreds of thousands to millions) and heterogeneous low-level features to a lower dimensional (thousands) vectors, and a latter ensures fast serving. built on top of the representative dnn model called deep crossing, and two forest/tree-based models including xgboost and lightgbm, the two-step deep embedding forest algorithm was demonstrated to achieve on-par or slightly better performance as compared with a dnn counterpart, with only the fraction of serving time on conventional hardware. after comparing with the joint optimization algorithm called partial fuzzification, also proposed inside this paper, it was concluded that a two-step deep embedding forest has achieved near optimal performance. experiments based on large scale data sets (up to 1 billion samples) from the major sponsored search engine proves a efficacy of a proposed model."
"model precision inside the classification task was highly dependent on a feature space that was used to train a model. moreover, whether a features are sequential or static will dictate which classification method should be applied as most of a machine learning algorithms are designed to deal with either one or another type of data. inside real-life scenarios, however, it was often a case that both static and dynamic features are present, or should be extracted from a data. inside this work, we demonstrate how generative models such as hidden markov models (hmm) and long short-term memory (lstm) artificial neural networks should be used to extract temporal information from a dynamic data. we explore how a extracted information should be combined with a static features inside order to improve a classification performance. we evaluate a existing techniques and suggest the hybrid approach, which outperforms other methods on several public datasets."
"while spatially varying coefficient (svc) models have attracted considerable attention inside applied science, they have been criticized as being unstable. a objective of this study was to show that capturing a ""spatial scale"" of each data relationship was crucially important to make svc modeling more stable, and inside doing so, adds flexibility. here, a analytical properties of six svc models are summarized inside terms of their characterization of scale. models are examined through the series of monte carlo simulation experiments to assess a extent to which spatial scale influences model stability and a accuracy of their svc estimates. a following models are studied: (i) geographically weighted regression (gwr) with the fixed distance or (ii) an adaptive distance bandwidth (gwra), (iii) flexible bandwidth gwr (fb-gwr) with fixed distance or (iv) adaptive distance bandwidths (fb-gwra), (v) eigenvector spatial filtering (esf), and (vi) random effects esf (re-esf). results reveal that a svc models designed to capture scale dependencies inside local relationships (fb-gwr, fb-gwra and re-esf) most accurately approximate a simulated svcs, where re-esf was a most computationally efficient. conversely gwr and esf, where svc estimates are naively assumed to operate at a same spatial scale considering each relationship, perform poorly. results also confirm that a adaptive bandwidth gwr models (gwra and fb-gwra) are superior to their fixed bandwidth counterparts (gwr and fb-gwr)."
"we study how to effectively leverage expert feedback to learn sequential decision-making policies. we focus on problems with sparse rewards and long time horizons, which typically pose significant challenges inside reinforcement learning. we propose an algorithmic framework, called hierarchical guidance, that leverages a hierarchical structure of a underlying problem to integrate different modes of expert interaction. our framework should incorporate different combinations of imitation learning (il) and reinforcement learning (rl) at different levels, leading to dramatic reductions inside both expert effort and cost of exploration. with the help of long-horizon benchmarks, including montezuma's revenge, we demonstrate that our idea behind the method should learn significantly faster than hierarchical rl, and be significantly more label-efficient than standard il. we also theoretically analyze labeling cost considering certain instantiations of our framework."
"off-policy model-free deep reinforcement learning methods with the help of previously collected data should improve sample efficiency over on-policy policy gradient techniques. on a other hand, on-policy algorithms are often more stable and easier to use. this paper examines, both theoretically and empirically, approaches to merging on- and off-policy updates considering deep reinforcement learning. theoretical results show that off-policy updates with the value function estimator should be interpolated with on-policy policy gradient updates whilst still satisfying performance bounds. our analysis uses control variate methods to produce the family of policy gradient algorithms, with several recently proposed algorithms being special cases of this family. we then provide an empirical comparison of these techniques with a remaining algorithmic details fixed, and show how different mixing of off-policy gradient estimates with on-policy samples contribute to improvements inside empirical performance. a final algorithm provides the generalization and unification of existing deep policy gradient techniques, has theoretical guarantees on a bias introduced by off-policy updates, and improves on a state-of-the-art model-free deep rl methods on the number of openai gym continuous control benchmarks."
"millirobots are the promising robotic platform considering many applications due to their small size and low manufacturing costs. legged millirobots, inside particular, should provide increased mobility inside complex environments and improved scaling of obstacles. however, controlling these small, highly dynamic, and underactuated legged systems was difficult. hand-engineered controllers should sometimes control these legged millirobots, but they have difficulties with dynamic maneuvers and complex terrains. we present an idea behind the method considering controlling the real-world legged millirobot that was based on learned neural network models. with the help of less than 17 minutes of data, our method should learn the predictive model of a robot's dynamics that should enable effective gaits to be synthesized on a fly considering following user-specified waypoints on the given terrain. furthermore, by leveraging expressive, high-capacity neural network models, our idea behind the method allows considering these predictions to be directly conditioned on camera images, endowing a robot with a ability to predict how different terrains might affect its dynamics. this enables sample-efficient and effective learning considering locomotion of the dynamic legged millirobot on various terrains, including gravel, turf, carpet, and styrofoam. experiment videos should be found at this https url"
"we propose the kernel mixture of polynomials prior considering bayesian nonparametric regression. a regression function was modeled by local averages of polynomials with kernel mixture weights. we obtain a minimax-optimal rate of contraction of a full posterior distribution up to the logarithmic factor that adapts to a smoothness level of a true function by estimating metric entropies of certain function classes. we also provide the frequentist sieve maximum likelihood estimator with the near-optimal convergence rate. we further investigate a application of a kernel mixture of polynomials to a partial linear model and obtain both a near-optimal rate of contraction considering a nonparametric component and a bernstein-von mises limit (i.e., asymptotic normality) of a parametric component. a proposed method was illustrated with numerical examples and shows superior performance inside terms of computational efficiency, accuracy, and uncertainty quantification compared to a local polynomial regression, dicekriging, and a robust gaussian stochastic process."
"this paper proposed the method considering stock prediction. inside terms of feature extraction, we extract a features of stock-related news besides stock prices. we first select some seed words based on experience which are a symbols of good news and bad news. then we propose an optimization method and calculate a positive polar of all words. after that, we construct a features of news based on a positive polar of their words. inside consideration of sequential stock prices and continuous news effects, we propose the recurrent neural network model to aid predict stock prices. compared to svm classifier with price features, we find our proposed method has an over 5% improvement on stock prediction accuracy inside experiments."
"let $\pi{:}\,(m,\mathcal{h})\to (b,b)$ be the submersion equipped with the horizontal connection $\cal h$ over the riemannian manifold $(b,b)$. we present an intrinsic curvature condition that only depends on a pair $(\cal h,b)$. by studying the set of relative flat planes, we prove that the certain class of pairs $(\cal h,b)$ admits the compatible metric with positive sectional curvature only if they are \textit{fat}, verifying wilhelm's conjecture inside this class."
"we consider a linear regression problem under semi-supervised settings wherein a available data typically consists of: (i) the small or moderate sized 'labeled' data, and (ii) the much larger sized 'unlabeled' data. such data arises naturally from settings where a outcome, unlike a covariates, was expensive to obtain, the frequent scenario inside modern studies involving large databases like electronic medical records (emr). supervised estimators like a ordinary least squares (ols) estimator utilize only a labeled data. it was often of interest to investigate if and when a unlabeled data should be exploited to improve approximation of a regression parameter inside a adopted linear model. inside this paper, we propose the class of 'efficient and adaptive semi-supervised estimators' (ease) to improve approximation efficiency. a ease are two-step estimators adaptive to model mis-specification, leading to improved (optimal inside some cases) efficiency under model mis-specification, and equal (optimal) efficiency under the linear model. this adaptive property, often unaddressed inside a existing literature, was crucial considering advocating 'safe' use of a unlabeled data. a construction of ease primarily involves the flexible 'semi-non-parametric' imputation, including the smoothing step that works well even when a number of covariates was not small; and the follow up 'refitting' step along with the cross-validation (cv) strategy both of which have useful practical as well as theoretical implications towards addressing two important issues: under-smoothing and over-fitting. we establish asymptotic results including consistency, asymptotic normality and a adaptive properties of ease. we also provide influence function expansions and the 'double' cv strategy considering inference. a results are further validated through extensive simulations, followed by application to an emr study on auto-immunity."
"a first measured results considering massive multiple-input, multiple-output (mimo) performance inside the line-of-sight (los) scenario with moderate mobility are presented, with 8 users served by the 100 antenna base station (bs) at 3.7 ghz. when such the large number of channels dynamically change, a inherent propagation and processing delay has the critical relationship with a rate of change, as a use of outdated channel information should result inside severe detection and precoding inaccuracies. considering a downlink (dl) inside particular, the time division duplex (tdd) configuration synonymous with massive mimo deployments could mean only a uplink (ul) was usable inside extreme cases. therefore, it was of great interest to investigate a impact of mobility on massive mimo performance and consider ways to combat a potential limitations. inside the mobile scenario with moving cars and pedestrians, a correlation of a mimo channel vector over time was inspected considering vehicles moving up to 29 km/h. considering the 100 antenna system, it was found that a channel state information (csi) update rate requirement may increase by 7 times when compared to an 8 antenna system, whilst a power control update rate could be decreased by at least 5 times relative to the single antenna system."
"we explore a power of semidefinite programming (sdp) considering finding additive epsilon-approximate nash equilibria inside bimatrix games. we introduce an sdp relaxation considering the quadratic programming formulation of a nash equilibrium (ne) problem and provide the number of valid inequalities to improve a quality of a relaxation. if the rank-1 solution to this sdp was found, then an exact ne should be recovered. we show that considering the strictly competitive game, our sdp was guaranteed to return the rank-1 solution. furthermore, we prove that if the rank-2 solution to our sdp was found, then the 5/11-ne should be recovered considering any game, or the 1/3-ne considering the symmetric game. we propose two algorithms based on iterative linearization of smooth nonconvex objective functions that are designed so that their global minima coincide with rank-1 solutions. empirically, we demonstrate that these algorithms often recover solutions of rank at most two and epsilon close to zero. we then show how our sdp idea behind the method should address two (np-hard) problems of economic interest: finding a maximum welfare achievable under any ne, and testing whether there exists the ne where the particular set of strategies was not played. finally, we show a connection between our sdp and a first level of a lasserre/sum of squares hierarchy."
"when conducting large scale inference, such as genome-wide association studies or image analysis, nominal $p$-values are often adjusted to improve control over a family-wise error rate (fwer). when a majority of tests are null, procedures controlling a false discovery rate (fdr) should be improved by replacing a theoretical global null with its empirical estimate. however, these other adjustment procedures remain sensitive to a working model assumption. here we propose two key ideas to improve inference inside this space. first, we propose $p$-values that are standardized to a empirical null distribution (instead of a theoretical null). second, we propose model averaging $p$-values by bootstrap aggregation (bagging) to account considering model uncertainty and selection procedures. a combination of these two key ideas yields bagged empirical null $p$-values (ben $p$-values) that often dramatically alter a rank ordering of significant findings. moreover, we find that the multidimensional selection criteria based on ben $p$-values and bagged model fit statistics was more likely to yield reproducible findings. the re-analysis of a famous golub leukemia data was presented to illustrate these ideas. we uncovered new findings inside these data, not detected previously, that are backed by published bench work pre-dating a gloub experiment. the pseudo-simulation with the help of a leukemia data was also presented to explore a stability of this idea behind the method under broader conditions, and illustrates a superiority of a ben $p$-values compared to a other approaches."
"kernel methods play the critical role inside many dimensionality reduction algorithms. they are useful inside manifold learning, classification, clustering and other machine learning tasks. setting a kernel's scale parameter, also referred as a kernel's bandwidth, highly affects a extracted low-dimensional representation. we propose to set the scale parameter that was tailored to a desired application such as classification and manifold learning. a scale computation considering a manifold learning task enables that a dimension of a extracted embedding equals a intrinsic dimension estimation. three methods are proposed considering scale computation inside the classification task. a proposed frameworks are simulated on artificial and real datasets. a results show the high correlation between optimal classification rates and a computed scaling."
"approved client-server authentication mechanisms are described considering a ivoa single-sign-on profile: no authentication; http basic authentication; tls with passwords; tls with client certificates; cookies; open authentication; security assertion markup language; openid. normative rules are given considering a implementation of these mechanisms, mainly by reference to pre-existing standards. a authorization mechanisms are out of a scope of this document."
"despite a recent success of deep learning considering many speech processing tasks, single-microphone, speaker-independent speech separation remains challenging considering two main reasons. a first reason was a arbitrary order of a target and masker speakers inside a mixture permutation problem, and a second was a unknown number of speakers inside a mixture output dimension problem. we propose the novel deep learning framework considering speech separation that addresses both of these issues. we use the neural network to project a time-frequency representation of a mixture signal into the high-dimensional embedding space. the reference point attractor was created inside a embedding space to represent each speaker which was defined as a centroid of a speaker inside a embedding space. a time-frequency embeddings of each speaker are then forced to cluster around a corresponding attractor point which was used to determine a time-frequency assignment of a speaker. we propose three methods considering finding a attractors considering each source inside a embedding space and compare their advantages and limitations. a objective function considering a network was standard signal reconstruction error which enables end-to-end operation during both training and test phases. we evaluated our system with the help of a wall street journal dataset wsj0 on two and three speaker mixtures and report comparable or better performance than other state-of-the-art deep learning methods considering speech separation."
"graph classification was the problem with practical applications inside many different domains. most of a existing methods take a entire graph into account when calculating graph features. inside the graphlet-based approach, considering instance, a entire graph was processed to get a total count of different graphlets or sub-graphs. inside a real-world, however, graphs should be both large and noisy with discriminative patterns confined to certain regions inside a graph only. inside this work, we study a problem of attentional processing considering graph classification. a use of attention allows us to focus on small but informative parts of a graph, avoiding noise inside a rest of a graph. we present the novel rnn model, called a graph attention model (gam), that processes only the portion of a graph by adaptively selecting the sequence of ""interesting"" nodes. a model was equipped with an external memory component which allows it to integrate information gathered from different parts of a graph. we demonstrate a effectiveness of a model through various experiments."
"bayesian inference requires approximation methods to become computable, but considering most of them it was impossible to quantify how close a approximation was to a true posterior. inside this work, we present the theorem upper-bounding a kl divergence between the log-concave target density $f\left(\boldsymbol{\theta}\right)$ and its laplace approximation $g\left(\boldsymbol{\theta}\right)$. a bound we present was computable: on a classical logistic regression model, we find our bound to be almost exact as long as a dimensionality of a parameter space was high. a idea behind the method we followed inside this work should be extended to other gaussian approximations, as we will do inside an extended version of this work, to be submitted to a annals of statistics. it will then become the critical tool considering characterizing whether, considering the given problem, the given gaussian approximation was suitable, or whether the more precise alternative method should be used instead."
inside this paper we study rotationally symmetric solutions of a cahn-hilliard equation inside $\mathbb r^3$ constructed by a authors. these solutions form the one parameter family analog to a family of delaunay surfaces and inside fact a zero level sets of their blowdowns idea behind the method these surfaces. presently we go the step further and show that their stability properties are inherited from a stability properties of a delaunay surfaces. our main result states that a rotationally symmetric solutions are non degenerate and that they have exactly $6$ jacobi fields of temperate growth coming from a natural invariances of a problem (3 translations and 2 rotations) and a variation of a delaunay parameter.
"statistical relational learning (srl) methods considering anomaly detection are introduced using the security-related application. operational requirements considering online learning stability are outlined and compared to mathematical definitions as applied to a learning process of the representative srl method - bayesian logic programs (blp). since the formal proof of online stability appears to be impossible, tentative common sense requirements are formulated and tested by theoretical and experimental analysis of the simple and analytically tractable blp model. it was found that learning algorithms inside initial stages of online learning should lock on unstable false predictors that nevertheless comply with our tentative stability requirements and thus masquerade as bona fide solutions. a very expressiveness of srl seems to cause significant stability issues inside settings with many variables and scarce data. we conclude that reliable anomaly detection with srl-methods requires monitoring by an overarching framework that may involve the comprehensive context knowledge base or human supervision."
"a stochastic block model (sbm) was the random graph model with planted clusters. it was widely employed as the canonical model to study clustering and community detection, and provides generally the fertile ground to study a statistical and computational tradeoffs that arise inside network and data sciences. this note surveys a recent developments that establish a fundamental limits considering community detection inside a sbm, both with respect to information-theoretic and computational thresholds, and considering various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). a main results discussed are a phase transitions considering exact recovery at a chernoff-hellinger threshold, a phase transition considering weak recovery at a kesten-stigum threshold, a optimal distortion-snr tradeoff considering partial recovery, a learning of a sbm parameters and a gap between information-theoretic and computational thresholds. a note also covers some of a algorithms developed inside a quest of achieving a limits, inside particular two-round algorithms using graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. the few open problems are also discussed."
"this paper describes a design, operations, and performance of a multi-site all-sky camera (mascara). its primary goal was to find new exoplanets transiting bright stars, $4 < m_v < 8$, by monitoring a full sky. mascara consists of one northern station on la palma, canary islands (fully operational since february 2015), one southern station at la silla observatory, chile (operational from early 2017), and the data centre at leiden observatory inside a netherlands. both mascara stations are equipped with five interline ccd cameras with the help of wide field lenses (24 mm focal length) with fixed pointings, which together provide coverage down to airmass 3 of a local sky. a interline ccd cameras allow considering back-to-back exposures, taken at fixed sidereal times with exposure times of 6.4 sidereal seconds. a exposures are short enough that a motion of stars across a ccd does not exceed one pixel during an integration. astrometry and photometry are performed on-site, after which a resulting light curves are transferred to leiden considering further analysis. a final mascara archive will contain light curves considering ${\sim}70,000$ stars down to $m_v=8.4$, with the precision of $1.5\%$ per 5 minutes at $m_v=8$."
"primordial black holes (pbh) arise naturally from high peaks inside a curvature power spectrum of near-inflection-point single-field inflation, and could constitute today a dominant component of a dark matter inside a universe. inside this letter we explore a possibility that the broad spectrum of pbh was formed inside models of critical higgs inflation (chi), where a near-inflection point was related to a critical value of a rge running of both a higgs self-coupling $\lambda(\mu)$ and its non-minimal coupling to gravity $\xi(\mu)$. we show that, considering the wide range of model parameters, the half-domed-shaped peak inside a matter spectrum arises at sufficiently small scales that it passes all a constraints from large scale structure observations. a predicted cosmic microwave background spectrum at large scales was inside agreement with planck 2015 data, and has the relatively large tensor-to-scalar ratio that may soon be detected by b-mode polarization experiments. moreover, a wide peak inside a power spectrum gives an approximately lognormal pbh distribution inside a range of masses $0.01 - 100\,m_\odot$, which could explain a ligo merger events, while passing all present pbh observational constraints. a stochastic background of gravitational waves coming from a unresolved black-hole-binary mergers could also be detected by lisa or pta. furthermore, a parameters of a chi model are consistent, within $2\sigma$, with a measured higgs parameters at a lhc and their running. future measurements of a pbh mass spectrum could allow us to obtain complementary information about a higgs couplings at energies well above a ew scale, and thus constrain new physics beyond a standard model."
"the few years ago, hubbard (2012, 2013) presented an elegant, non-perturbative method, called concentric maclaurin spheroid (cms), to calculate with very high accuracy a gravitational moments of the rotating fluid body following the barotropic pressure-density relationship. having such an accurate method was of great importance considering taking full advantage of a juno mission, and its extremely precise determination of jupiter gravitational moments, to better constrain a internal structure of a planet. recently, several authors have applied this method to a juno mission with 512 spheroids linearly spaced inside altitude. we demonstrate inside this paper that such calculations lead to errors larger than juno's error bars, invalidating a aforederived jupiter models at a level required by juno's precision. we show that, inside order to fulfill juno's observational constraints, at least 1500 spheroids must be used with the cubic, square or exponential repartition, a most reliable solutions. when with the help of the realistic equation of state instead of the polytrope, we highlight a necessity to properly describe a outermost layers to derive an accurate boundary condition, excluding inside particular the zero pressure outer condition. providing all these constraints are fulfilled, a cms method should indeed be used to derive jupiter models within juno's present observational constraints. however, we show that a treatment of a outermost layers leads to irreducible errors inside a calculation of a gravitational moments and thus on a inferred physical quantities considering a planet. we have quantified these errors and evaluated a maximum precision that should be reached with a cms method inside a present and future exploitation of juno's data."
"a mechanisms considering strong electron-phonon coupling predicted considering hydrogen-rich alloys with high superconducting critical temperature ($t_c$) are examined within a migdal-eliashberg theory. analysis of a functional derivative of $t_c$ with respect to a electron-phonon spectral function shows that at low pressures, when a alloys often adopt layered structures, bending vibrations have a most dominant effect. at very high pressures, a h-h interactions inside two-dimensional (2d) and three-dimensional (3d) extended structures are weakened, resulting inside mixed bent (libration) and stretch vibrations, and a electron-phonon coupling process was distributed over the broad frequency range leading to very high $t_c$."
"inside this short note we define the new cohomology considering the lie algebroid $\mathcal{a}$, that we call a \emph{twisted cohomology} of $\mathcal{a}$ by an odd cocycle $\theta$ inside a lie algebroid cohomology of $\mathcal{a}$. we proof that this cohomology only depends on a lie algebroid cohomology class $[\theta]$ of a odd cocycle $\theta$. we give the few examples showing that this new cohomology encompasses various well-known cohomology theories."
"materials design and development typically takes several decades from a initial discovery to commercialization with a traditional trial and error development approach. with a accumulation of data from both experimental and computational results, data based machine learning becomes an emerging field inside materials discovery, design and property prediction. this manuscript reviews a history of materials science as the disciplinary a most common machine learning method used inside materials science, and specifically how they are used inside materials discovery, design, synthesis and even failure detection and analysis after materials are deployed inside real application. finally, a limitations of machine learning considering application inside materials science and challenges inside this emerging field was discussed."
"inside this paper, we study a local backward problem of the linear heat equation with time-dependent coefficients under a dirichlet boundary condition. precisely, we recover a initial data from a observation on the subdomain at some later time. thanks to a ""optimal filtering"" method of seidman, we should solve a global backward problem, which determines a solution at initial time from a known data on a whole domain. then, by with the help of the result of controllability at one point of time, we should connect local and global backward problem."
"a outer scutum-centaurus (osc) spiral arm was a most distant molecular spiral arm inside a milky way, but until recently little is known about this structure. discovered by dame and thaddeus (2011), a osc lies $\sim$15 kpc from a galactic center. due to a galactic warp, it rises to nearly 4$^{\circ}$ above a galactic plane inside a first galactic quadrant, leaving it unsampled by most galactic plane surveys. here we observe hii region candidates spatially coincident with a osc with the help of a very large array to image radio continuum emission from 65 targets and a green bank telescope to search considering ammonia and water maser emission from 75 targets. this sample, drawn from a wise catalog of galactic hii regions, represents every hii region candidate near a longitude-latitude (l,v) locus of a osc. coupled with their characteristic mid-infrared morphologies, detection of radio continuum emission strongly suggests that the target was the bona fide hii region. detections of associated ammonia or water maser emission allow us to derive the kinematic distance and determine if a velocity of a region was consistent with that of a osc. nearly 60% of a observed sources were detected inside radio continuum, and over 20% have ammonia or water maser detections. a velocities of these sources mainly place them beyond a solar orbit. these very distant high-mass stars have stellar spectral types as early as o4. we associate high-mass star formation at 2 new locations with a osc, increasing a total number of detected hii regions inside a osc to 12."
"we establish an elementary, but rather striking pattern concerning a quartic residues of primes $p$ that are congruent to 5 modulo 8. let $g$ be the generator of a multiplicative group of $\mathbb z_p$ and let $m$ be a $4\times 4$ matrix whose $(i+1),(j+1)-$th entry was a number of elements $x$ of $\mathbb z_p$ of a form $x\equiv g^k \pmod p$ where $k\equiv i \pmod 4$ and $\lfloor 4x/p \rfloor = j$, considering $i,j=0,1,2,3$. we show that $m$ was the latin square, provided a entries inside a first row are distinct, and that $m$ was essentially independent of a choice of $g$. as an application, we prove that a sum inside $\mathbb z$ of a quartic residues was $\frac{p}5(m_{11}+2m_{12}+3m_{13}+4m_{14})$."
"inside practical analysis, domain knowledge about analysis target has often been accumulated, although, typically, such knowledge has been discarded inside a statistical analysis stage, and a statistical tool has been applied as the black box. inside this paper, we introduce sign constraints that are the handy and simple representation considering non-experts inside generic learning problems. we have developed two new optimization algorithms considering a sign-constrained regularized loss minimization, called a sign-constrained pegasos (sc-pega) and a sign-constrained sdca (sc-sdca), by simply inserting a sign correction step into a original pegasos and sdca, respectively. we present theoretical analyses that guarantee that insertion of a sign correction step does not degrade a convergence rate considering both algorithms. two applications, where a sign-constrained learning was effective, are presented. a one was exploitation of prior information about correlation between explanatory variables and the target variable. a other was introduction of a sign-constrained to svm-pairwise method. experimental results demonstrate significant improvement of generalization performance by introducing sign constraints inside both applications."
"let $\mathbb{k}$ be an algebraically closed field of characteristic $0$. we study the monoidal category $\mathbb{t}_\alpha$ which was universal among all symmetric $\mathbb{k}$-linear monoidal categories generated by two objects $a$ and $b$ such that $a$ has a, possibly transfinite, filtration. we construct $\mathbb{t}_\alpha$ as the category of representations of a lie algebra $\mathfrak{gl}^m(v_*,v)$ consisting of endomorphisms of the fixed diagonalizable pairing $v_*\otimes v\to \mathbb{k}$ of vector spaces $v_*$ and $v$ of dimension $\alpha$. here $\alpha$ was an arbitrary cardinal number. we describe explicitly a simple and a injective objects of $\mathbb{t}_\alpha$ and prove that a category $\mathbb{t}_\alpha$ was koszul. we pay special attention to a case where a filtration on $a$ was finite. inside this case $\alpha=\aleph_t$ considering $t\in\mathbb{z}_{\geq 0}$."
"training deep neural network policies end-to-end considering real-world applications so far requires big demonstration datasets inside a real world or big sets consisting of the large variety of realistic and closely related 3d cad models. these real or virtual data should, moreover, have very similar characteristics to a conditions expected at test time. these stringent requirements and a time consuming data collection processes that they entail, are currently a most important impediment that keeps deep reinforcement learning from being deployed inside real-world applications. therefore, inside this work we advocate an alternative approach, where instead of avoiding any domain shift by carefully selecting a training data, a goal was to learn the policy that should cope with it. to this end, we propose a doshico challenge: to train the model inside very basic synthetic environments, far from realistic, inside the way that it should be applied inside more realistic environments as well as take a control decisions on real-world data. inside particular, we focus on a task of collision avoidance considering drones. we created the set of simulated environments that should be used as benchmark and implemented the baseline method, exploiting depth prediction as an auxiliary task to aid overcome a domain shift. even though a policy was trained inside very basic environments, it should learn to fly without collisions inside the very different realistic simulated environment. of course several benchmarks considering reinforcement learning already exist - but they never include the large domain shift. on a other hand, several benchmarks inside computer vision focus on a domain shift, but they take a form of the static datasets instead of simulated environments. inside this work we claim that it was crucial to take a two challenges together inside one benchmark."
"let $t$ be the circle group, and $lt$ be its loop group. we hope to establish an index theory considering infinite-dimensional manifolds which $lt$ acts on, including hamiltonian $lt$-spaces, from a viewpoint of $kk$-theory. we have already constructed several objects inside a previous paper \cite{t}, including the hilbert space $\mathcal{h}$ consisting of ""$l^2$-sections of the spinor bundle on a infinite-dimensional manifold"", an ""$lt$-equivariant dirac operator $\mathcal{d}$"" acting on $\mathcal{h}$, the ""twisted crossed product of a function algebra by $lt$"", and a ""twisted group $c^*$-algebra of $lt$"", without a measure on a manifolds, a measure on $lt$ or a function algebra itself. however, we need more sophisticated constructions. inside this paper, we study a index problem inside terms of $kk$-theory. concretely, we focus on a infinite-dimensional version of a latter half of a assembly map defined by kasparov. generally speaking, considering the $\gamma$-equivariant $k$-homology class $x$, a assembly map was defined by $\mu^\gamma(x):=[c]\otimes j^\gamma(x)$, where $j^\gamma$ was the $kk$-theoretical homomorphism, $[c]$ was the $k$-theory class coming from the cut-off function, and $\otimes$ denotes a kasparov product with respect to $\gamma\ltimes c_0(x)$. we will define neither a $lt$-equivariant $k$-homology nor a cut-off function, but we will indeed define a $kk$-cycles $j^{lt}_\tau(x)$ and $[c]$ directly, considering the virtual $k$-homology class $x=(\mathcal{h},\mathcal{d})$ which was mentioned above. as the result, we will get a $kk$-theoretical index $\mu^{lt}_\tau(x)\in kk(\mathbb{c},lt\ltimes_\tau \mathbb{c})$. we will also compare $\mu^{lt}_\tau(x)$ with a analytic index ${\rm ind}_{lt\ltimes_\tau\mathbb{c}}(x)$ which will be introduced."
"inside this work, we investigate an original strategy inside order to derive the statistical modeling of a interface inside gas-liquid two-phase flows through geometrical variables. a con- tribution was two-fold. first it participates inside a theoretical design of the unified reduced- order model considering a description of two regimes: the disperse phase inside the carrier fluid and two separated phases. a first idea was to propose the statistical description of a in- terface relying on geometrical properties such as a mean and gauss curvatures and define the surface density function (sdf). a second main idea consists inside with the help of such the formalism inside a disperse case, where the clear link was proposed between local statistics of a interface and a statistics on objects, such as a number density function inside williams-boltzmann equation considering droplets. this makes essential a use of topolog- ical invariants inside geometry through a gauss-bonnet formula and allows to include a works conducted on sprays of spherical droplets. it yields the statistical treatment of populations of non-spherical objects such as ligaments, as long as they are home- omorphic to the sphere. second, it provides an original angle and algorithm inside order to build statistics from dns data of interfacial flows. from a theoretical approach, we identify the kernel considering a spatial averaging of geometrical quantities preserving a topological invariants. coupled to the new algorithm considering a evaluation of curvatures and surface that preserves these invariants, we analyze two sets of dns results conducted with a archer code from coria with and without topological changes and assess a approach."
"a multiagent-based participatory simulation features prominently inside urban planning as a acquired model was considered as a hybrid system of a domain and a local knowledge. however, a key problem of generating realistic agents considering particular social phenomena invariably remains. a existing models have attempted to dictate a factors involving human behavior, which appeared to be intractable. inside this paper, inverse reinforcement learning (irl) was introduced to address this problem. irl was developed considering computational modeling of human behavior and has achieved great successes inside robotics, psychology and machine learning. a possibilities presented by this new style of modeling are drawn out as conclusions, and a relative challenges with this modeling are highlighted."
"very recent measurements of a electrical conductivity of solid systems agx - cdx$_2$ (where x$\equiv$cl,br) that form large areas of solid solutions, have shown that maximum conductivity occurs considering the concentration around 20 mol\% of a cadmium halide. here, we suggest the quantitative explanation of this phenomenon based on the model that is suggested (j. appl. phys. 103, 083552, (2008)) considering estimating a compressibility of multiphased mixed crystals. inside addition, explicit conditions are obtained which predict when such the conductivity maximum was expected to occur."
"astronomers are often confronted with funky populations and distributions of objects: brighter objects are more likely to be detected; targets are selected based on colour cuts; imperfect classification yields impure samples. failing to account considering these effects leads to biased analyses. inside this paper we present the simple overview of the bayesian consideration of sample selection, giving solutions to both analytically tractable and intractable models. this was accomplished using the combination of analytic approximations and monte carlo integration, inside which dataset simulation was efficiently used to correct considering issues inside a observed dataset. this methodology was also applicable considering data truncation, such as requiring densities to be strictly positive. toy models are included considering demonstration, along with discussions of numerical considerations and how to optimise considering implementation. we provide sample code to demonstrate a techniques. a methods inside this paper should be widely applicable inside fields beyond astronomy, wherever sample selection effects occur."
"accurately predicting a future health of batteries was necessary to ensure reliable operation, minimise maintenance costs, and calculate a value of energy storage investments. a complex nature of degradation renders data-driven approaches the promising alternative to mechanistic modelling. this study predicts a changes inside battery capacity over time with the help of the bayesian non-parametric idea behind the method based on gaussian process regression. these changes should be integrated against an arbitrary input sequence to predict capacity fade inside the variety of usage scenarios, forming the generalised health model. a idea behind the method naturally incorporates varying current, voltage and temperature inputs, crucial considering enabling real world application. the key innovation was a feature selection step, where arbitrary length current, voltage and temperature measurement vectors are mapped to fixed size feature vectors, enabling them to be efficiently used as exogenous variables. a idea behind the method was demonstrated on a open-source nasa randomised battery usage dataset, with data of 26 cells aged under randomized operational conditions. with the help of half of a cells considering training, and half considering validation, a method was shown to accurately predict non-linear capacity fade, with the best case normalised root mean square error of 4.3%, including accurate approximation of prediction uncertainty."
"a $su(4)-su(2)$ crossover, driven by an external magnetic field $h$, was analyzed inside the capacitively-coupled double-quantum-dot device connected to independent leads. as one continuously charges a dots from empty to quarter-filled, by varying a gate potential $v_g$, a crossover starts when a magnitude of a spin polarization of a double quantum dot, as measured by $\langle n_{\uparrow}\rangle -\langle n_{\downarrow}\rangle$, becomes finite. although a external magnetic field breaks a $su(4)$ symmetry of a hamiltonian, a ground state preserves it inside the region of $v_g$, where $\langle n_{\uparrow}\rangle -\langle n_{\downarrow}\rangle =0$. once a spin polarization becomes finite, it initially increases slowly until the sudden change occurs, inside which $\langle n_{\downarrow}\rangle$ (polarization direction opposite to a magnetic field) reaches the maximum and then decreases to negligible values abruptly, at which point an orbital $su(2)$ ground state was fully established. this crossover from one kondo state, with emergent $su(4)$ symmetry, where spin and orbital degrees of freedom all play the role, to another, with $su(2)$ symmetry, where only orbital degrees of freedom participate, was triggered by the competition between $g\mu_bh$, a energy gain by a zeeman-split polarized state and a kondo temperature $t_k^{su(4)}$, a gain provided by a $su(4)$ unpolarized kondo-singlet state."
"we studied a nearby edge-on galaxy ngc4656 and its dwarf low surface brightness companion with a enhanced uv brightness, ngc4656uv, belonging to a interacting system ngc4631/56. regular photometric structure and relatively big size of ngc4656uv allows to consider this dwarf galaxy as the separate group member rather than the tidal dwarf. spectral long-slit observations were used to obtain a kinematical parameters and gas-phase metallicity of ngc4656uv and ngc4656. our rough approximate of a total dynamical mass of ngc4656uv allowed us to conclude that this galaxy was a dark-matter dominated lsb dwarf or ultra diffuse galaxy. young stellar population of ngc4656uv, as well as strong local non-circular gas motions inside ngc4656 and a low oxygen gas abundance inside a region of this galaxy adjacent to its dwarf companion, give evidence inside favour of a accretion of metal-poor gas onto a discs of both galaxies."
"we consider a problem of estimating means of two gaussians inside the 2-gaussian mixture, which was not balanced and was corrupted by noise of an arbitrary distribution. we present the robust algorithm to approximate a parameters, together with upper bounds on a numbers of samples required considering a approximate to be correct, where a bounds are parametrised by a dimension, ratio of a mixing coefficients, the measure of a separation of a two gaussians, related to mahalanobis distance, and the condition number of a covariance matrix. inside theory, this was a first sample-complexity result considering imbalanced mixtures corrupted by adversarial noise. inside practice, our algorithm outperforms a vanilla expectation-maximisation (em) algorithm inside terms of approximation error."
"autonomous surface vehicles (asvs) provide an effective way to actualize applications such as environment monitoring, search and rescue, and scientific researches. however, a conventional asvs depends overly on a stored energy. hybrid sailboat, mainly powered by a wind, should solve this problem by with the help of an auxiliary propulsion system. a electric energy cost of hybrid sailboat needs to be optimized to achieve a ocean automatic cruise mission. based on adjusted setting on sails and rudders, this paper seeks a optimal trajectory considering autonomic cruising to reduce a energy cost by changing a heading angle of sailing upwind. a experiment results validate a heading angle accounts considering energy cost and a trajectory with a best heading angle saves up to 23.7% than other conditions. furthermore, a energy-time line should be used to predict a energy cost considering long-time sailing."
"we propose simple and flexible training and decoding methods considering influencing output style and topic inside neural encoder-decoder based language generation. this capability was desirable inside the variety of applications, including conversational systems, where successful agents need to produce language inside the specific style and generate responses steered by the human puppeteer or external knowledge. we decompose a neural generation process into empirically easier sub-problems: the faithfulness model and the decoding method based on selective-sampling. we also describe training and sampling algorithms that bias a generation process with the specific language style restriction, or the topic restriction. human evaluation results show that our proposed methods are able to restrict style and topic without degrading output quality inside conversational tasks."
"we study a combinatorial pure exploration problem best-set inside stochastic multi-armed bandits. inside the best-set instance, we are given $n$ arms with unknown reward distributions, as well as the family $\mathcal{f}$ of feasible subsets over a arms. our goal was to identify a feasible subset inside $\mathcal{f}$ with a maximum total mean with the help of as few samples as possible. a problem generalizes a classical best arm identification problem and a top-$k$ arm identification problem, both of which have attracted significant attention inside recent years. we provide the novel instance-wise lower bound considering a sample complexity of a problem, as well as the nontrivial sampling algorithm, matching a lower bound up to the factor of $\ln|\mathcal{f}|$. considering an important class of combinatorial families, we also provide polynomial time implementation of a sampling algorithm, with the help of a equivalence of separation and optimization considering convex program, and approximate pareto curves inside multi-objective optimization. we also show that a $\ln|\mathcal{f}|$ factor was inevitable inside general through the nontrivial lower bound construction. our results significantly improve several previous results considering several important combinatorial constraints, and provide the tighter understanding of a general best-set problem. we further introduce an even more general problem, formulated inside geometric terms. we are given $n$ gaussian arms with unknown means and unit variance. consider a $n$-dimensional euclidean space $\mathbb{r}^n$, and the collection $\mathcal{o}$ of disjoint subsets. our goal was to determine a subset inside $\mathcal{o}$ that contains a $n$-dimensional vector of a means. a problem generalizes most pure exploration bandit problems studied inside a literature. we provide a first nearly optimal sample complexity upper and lower bounds considering a problem."
"gaussian processes (gps) are important models inside supervised machine learning. training inside gaussian processes refers to selecting a covariance functions and a associated parameters inside order to improve a outcome of predictions, a core of which amounts to evaluating a logarithm of a marginal likelihood (lml) of the given model. lml gives the concrete measure of a quality of prediction that the gp model was expected to achieve. a classical computation of lml typically carries the polynomial time overhead with respect to a input size. we propose the quantum algorithm that computes a logarithm of a determinant of the hermitian matrix, which runs inside logarithmic time considering sparse matrices. this was applied inside conjunction with the variant of a quantum linear system algorithm that allows considering logarithmic time computation of a form $\mathbf{y}^ta^{-1}\mathbf{y}$, where $\mathbf{y}$ was the dense vector and $a$ was a covariance matrix. we thus show that quantum computing should be used to approximate a lml of the gp with exponentially improved efficiency under certain conditions."
"domain adaptation refers to a process of learning prediction models inside the target domain by making use of data from the source domain. many classic methods solve a domain adaptation problem by establishing the common latent space, which may cause a loss of many important properties across both domains. inside this manuscript, we develop the novel method, transfer latent representation (tlr), to learn the better latent space. specifically, we design an objective function based on the simple linear autoencoder to derive a latent representations of both domains. a encoder inside a autoencoder aims to project a data of both domains into the robust latent space. besides, a decoder imposes an additional constraint to reconstruct a original data, which should preserve a common properties of both domains and reduce a noise that causes domain shift. experiments on cross-domain tasks demonstrate a advantages of tlr over competing methods."
"we perform the systematic analysis of a influence of phonon driving on a superconducting holstein model coupled to heat baths by studying both a transient dynamics and a nonequilibrium steady state (ness) inside a weak and strong electron-phonon coupling regimes. our study was based on a nonequilibrium dynamical mean-field theory, and considering a ness we present the floquet formulation adapted to electron-phonon systems. a analysis of a phonon propagator suggests that a effective attractive interaction should be strongly enhanced inside the parametric resonant regime because of a floquet side bands of phonons. while this may be expected to enhance a superconductivity (sc), our fully self-consistent calculations, which include a effects of heating and nonthermal distributions, show that a parametric phonon driving generically results inside the suppression or complete melting of a sc order. inside a strong coupling regime, a ness always shows the suppression of a sc gap, a sc order parameter and a superfluid density as the result of a driving, and this tendency was most prominent at a parametric resonance. with the help of a real-time nonequilibrium dmft formalism, we also study a dynamics towards a ness, which shows that a heating effect dominates a transient dynamics, and sc was weakened by a external modulations, inside particular at a parametric resonance. inside a weak coupling regime, we find that a sc fluctuations above a transition temperature are generally weakened under a driving. a strongest suppression occurs again around a parametric resonances because of a efficient energy absorption."
"inside many smart infrastructure applications flexibility inside achieving sustainability goals should be gained by engaging end-users. however, these users often have heterogeneous preferences that are unknown to a decision-maker tasked with improving operational efficiency. modeling user interaction as the continuous game between non-cooperative players, we propose the robust parametric utility learning framework that employs constrained feasible generalized least squares approximation with heteroskedastic inference. to improve forecasting performance, we extend a robust utility learning scheme by employing bootstrapping with bagging, bumping, and gradient boosting ensemble methods. moreover, we approximate a noise covariance which provides approximated correlations between players which we leverage to develop the novel correlated utility learning framework. we apply a proposed methods both to the toy example arising from bertrand-nash competition between two firms as well as to data from the social game experiment designed to encourage energy efficient behavior amongst smart building occupants. with the help of occupant voting data considering shared resources such as lighting, we simulate a game defined by a estimated utility functions to demonstrate a performance of a proposed methods."
"the recently proposed exact algorithm considering a maximum independent set problem was analyzed. a typical running time was improved exponentially inside some parameter regions compared to simple binary search. a algorithm also overcomes a core transition point, where a conventional leaf removal algorithm fails, and works up to a replica symmetry breaking (rsb) transition point. this suggests that the leaf removal core itself was not enough considering typical hardness inside a random maximum independent set problem, providing further evidence considering rsb being a obstacle considering algorithms inside general."
"a work investigates deep generative models, which allow us to use training data from one domain to build the model considering another domain. we propose a variational bi-domain triplet autoencoder (vbta) that learns the joint distribution of objects from different domains. we extend a vbtas objective function by a relative constraints or triplets that sampled from a shared latent space across domains. inside other words, we combine a deep generative models with the metric learning ideas inside order to improve a final objective with a triplets information. a performance of a vbta model was demonstrated on different tasks: image-to-image translation, bi-directional image generation and cross-lingual document classification."
"zero-shot learning, which studies a problem of object classification considering categories considering which we have no training examples, was gaining increasing attention from community. most existing zsl methods exploit deterministic transfer learning using an in-between semantic embedding space. inside this paper, we try to attack this problem from the generative probabilistic modelling perspective. we assume considering any category, a observed representation, e.g. images or texts, was developed from the unique prototype inside the latent space, inside which a semantic relationship among prototypes was encoded using linear reconstruction. taking advantage of this assumption, virtual instances of unseen classes should be generated from a corresponding prototype, giving rise to the novel zsl model which should alleviate a domain shift problem existing inside a way of direct transfer learning. extensive experiments on three benchmark datasets show our proposed model should achieve state-of-the-art results."
"face recognition has made great progress with a development of deep learning. however, video face recognition (vfr) was still an ongoing task due to various illumination, low-resolution, pose variations and motion blur. most existing cnn-based vfr methods only obtain the feature vector from the single image and simply aggregate a features inside the video, which less consider a correlations of face images inside one video. inside this paper, we propose the novel attention-set based metric learning (asml) method to measure a statistical characteristics of image sets. it was the promising and generalized extension of maximum mean discrepancy with memory attention weighting. first, we define an effective distance metric on image sets, which explicitly minimizes a intra-set distance and maximizes a inter-set distance simultaneously. second, inspired by neural turing machine, the memory attention weighting was proposed to adapt set-aware global contents. then asml was naturally integrated into cnns, resulting inside an end-to-end learning scheme. our method achieves state-of-the-art performance considering a task of video face recognition on a three widely used benchmarks including youtubeface, youtube celebrities and celebrity-1000."
"let $g$ be the connected complex simple lie group, and let $\widehat{g}^{\mathrm{d}}$ be a set of all equivalence classes of irreducible unitary representations with non-vanishing dirac cohomology. we show that $\widehat{g}^{\mathrm{d}}$ consists of two parts: finitely many scattered representations, and finitely many strings of representations. moreover, a strings of $\widehat{g}^{\mathrm{d}}$ come from $\widehat{l}^{\mathrm{d}}$ using cohomological induction and they are all inside a good range. here $l$ runs over a $\theta$-stable proper levi subgroups of $g$. it follows that figuring out $\widehat{g}^{\mathrm{d}}$ requires the finite calculation inside total. as an application, we report the complete description of $\widehat{f}_4^{\mathrm{d}}$."
"given $n$ symmetric bernoulli variables, what should be said about their correlation matrix viewed as the vector? we show that a set of those vectors $r(\mathcal{b}_n)$ was the polytope and identify its vertices. those extreme points correspond to correlation vectors associated to a discrete uniform distributions on diagonals of a cube $[0,1]^n$. we also show that a polytope was affinely isomorphic to the well-known cut polytope ${\rm cut}(n)$ which was defined as the convex hull of a cut vectors inside the complete graph with vertex set $\{1,\ldots,n\}$. a isomorphism was obtained explicitly as $r(\mathcal{b}_n)= {\mathbf{1}}-2~{\rm cut}(n)$. as the corollary of this work, it was straightforward with the help of linear programming to determine if the particular correlation matrix was realizable or not. furthermore, the sampling method considering multivariate symmetric bernoullis with given correlation was obtained. inside some cases a method should also be used considering general, not exclusively bernoulli, marginals."
"we use a most recent cosmic microwave background (cmb) data to perform the bayesian statistical analysis and discuss a observational viability of inflationary models with the non-minimal coupling,~$\xi$, between a inflaton field and a ricci scalar. we particularize our analysis to two examples of small and large field inflationary models, namely, a coleman-weinberg and a chaotic quartic potentials. we find that (i) a $\xi$ parameter was closely correlated with a primordial amplitude; (ii) although improving a agreement with a cmb data inside a $r - n_s$ plane, where $r$ was a tensor-to-scalar ratio and $n_s$ a primordial spectral index, the non-null coupling was strongly disfavoured with respect to a minimally coupled standard $\lambda$cdm model, since a upper bounds of a bayes factor (odds) considering $\xi$ parameter are greater than $150:1$."
"we present measurements of a weak gravitational lensing shear power spectrum based on $450$ sq. deg. of imaging data from a kilo degree survey. we employ the quadratic estimator inside two and three redshift bins and extract band powers of redshift auto-correlation and cross-correlation spectra inside a multipole range $76 \leq \ell \leq 1310$. a cosmological interpretation of a measured shear power spectra was performed inside the bayesian framework assuming the $\lambda$cdm model with spatially flat geometry, while accounting considering small residual uncertainties inside a shear calibration and redshift distributions as well as marginalising over intrinsic alignments, baryon feedback and an excess-noise power model. moreover, massive neutrinos are included inside a modelling. a cosmological main result was expressed inside terms of a parameter combination $s_8 \equiv \sigma_8 \sqrt{\omega_{\rm m}/0.3}$ yielding $s_8 = \ 0.651 \pm 0.058$ (3 z-bins), confirming a recently reported tension inside this parameter with constraints from planck at $3.2\sigma$ (3 z-bins). we cross-check a results of a 3 z-bin analysis with a weaker constraints from a 2 z-bin analysis and find them to be consistent. a high-level data products of this analysis, such as a band power measurements, covariance matrices, redshift distributions, and likelihood evaluation chains are available at this http url"
"we present a vista-cfht stripe 82 (vics82) survey: the near-infrared (j+ks) survey covering 150 square degrees of a sloan digital sky survey (sdss) equatorial stripe 82 to an average depth of j=21.9 ab mag and ks=21.4 ab mag (80% completeness limits; 5-sigma point source depths are approximately 0.5 mag brighter). vics82 contributes to a growing legacy of multi-wavelength data inside a stripe 82 footprint. a addition of near-infrared photometry to a existing sdss stripe 82 coadd ugriz photometry reduces a scatter inside stellar mass estimates to delta log(m_stellar)~0.3 dex considering galaxies with m_stellar>10^9m_sun at z~0.5, and offers improvement compared to optical-only estimates out to z~1, with stellar masses constrained within the factor of approximately 2.5. when combined with other multi-wavelength imaging of a stripe, including moderate-to-deep ultraviolet (galex), optical and mid-infrared (spitzer irac) coverage, as well as tens of thousands of spectroscopic redshifts, vics82 gives access to approximately 0.5 gpc^3 of comoving volume. some of a main science drivers of vics82 include (a) measuring a stellar mass function of l^star galaxies out to z~1; (b) detecting intermediate redshift quasars at 2<z<3.5; (c) measuring a stellar mass function and baryon census of clusters of galaxies, and (d) performing optical/near-infrared-cosmic microwave background lensing cross-correlation experiments linking stellar mass to large-scale dark matter structure. here we define and describe a survey, highlight some early science results and present a first public data release, which includes an sdss-matched catalogue as well as a calibrated pixel data itself."
governing equations considering two-dimensional inviscid free-surface flows with constant vorticity over arbitrary non-uniform bottom profile are presented inside exact and compact form with the help of conformal variables. an efficient and very accurate numerical method considering this problem was developed.
"toward the deeper understanding on a inner work of deep neural networks, we investigate cnn (convolutional neural network) with the help of dcn (deconvolutional network) and randomization technique, and gain new insights considering a intrinsic property of this network architecture. considering a random representations of an untrained cnn, we train a corresponding dcn to reconstruct a input images. compared with a image inversion on pre-trained cnn, our training converges faster and a yielding network exhibits higher quality considering image reconstruction. it indicates there was rich information encoded inside a random features; a pre-trained cnn may discard information irrelevant considering classification and encode relevant features inside the way favorable considering classification but harder considering reconstruction. we further explore a property of a overall random cnn-dcn architecture. surprisingly, images should be inverted with satisfactory quality. extensive empirical evidence as well as theoretical analysis are provided."
"inside order considering autonomous robots to be able to support people's well-being inside homes and everyday environments, new interactive capabilities will be required, as exemplified by a soft design used considering disney's recent robot character baymax inside popular fiction. home robots will be required to be easy to interact with and intelligent--adaptive, fun, unobtrusive and involving little effort to power and maintain--and capable of carrying out useful tasks both on an everyday level and during emergencies. a current article adopts an exploratory medium fidelity prototyping idea behind the method considering testing some new robotic capabilities inside regard to recognizing people's activities and intentions and behaving inside the way which was transparent to people. results are discussed with a aim of informing next designs."
"to measure system states and local environment directly with high precision, expensive sensors are required. however, highly accurate system states and environmental perception should also be achieved with the help of data fusion techniques and digital maps. one crucial task of multi-sensor state approximation was to project different sensor measurements into a same temporal, spatial and physical domain, approximate their covariance matrices as well as a exclusion of erroneous measurements. this paper presents the generic idea behind the method considering robust approximation of vehicle movement (odometry). we will shortly present our calibration procedure, including a approximation of sensor alignments, offset / scaling errors, covariances / correlations and time delays. an improved algorithm considering wheel diameter approximation was presented. additionally an idea behind the method considering robust odometry will be shown as odometry estimations are fused under known covariances, while outliers are detected with the help of the chi-squared test. utilizing our robust odometry, local environmental views should be associated and fused. furthermore our robust odometry should be used to detect and exclude erroneous position estimates."
"when nodes inside the mobile network use relative noisy measurements with respect to their neighbors to approximate their positions, a overall connectivity and geometry of a measurement network has the critical influence on a achievable localization accuracy. this paper considers a problem of deploying the mobile robotic network implementing the cooperative localization scheme based on range measurements only, while attempting to maintain the network geometry that was favorable to estimating a robots' positions with high accuracy. a quality of a network geometry was measured by the ""localizability"" function serving as potential field considering robot motion planning. this function was built from a cramér-rao bound, which provides considering the given geometry the lower bound on a covariance matrix achievable by any unbiased position estimator that a robots might implement with the help of their relative measurements. we describe gradient descent-based motion planners considering a robots that attempt to optimize or constrain different variations of a network's localizability function, and discuss ways of implementing these controllers inside the distributed manner. finally, a paper also establishes formal connections between our statistical point of view and maintaining the form of weighted rigidity considering a graph capturing a relative range measurements."
"this paper was concerned with a detection of objects immersed inside anisotropic media from boundary measurements. we propose an accurate idea behind the method based on a kohn-vogelius formulation and a topological sensitivity analysis method. a inverse problem was formulated as the topology optimization one minimizing an energy like functional. the topological asymptotic expansion was derived considering a anisotropic laplace operator. a unknown object was reconstructed with the help of the level-set curve of a topological gradient. a efficiency and accuracy of a proposed algorithm are illustrated by some numerical results. mots-clés : problème inverse géométrique, laplace anisotrope, formulation de kohn-vogelius, analyse de sensibilité, optimisation topologique."
"we consider model-based clustering methods considering continuous, correlated data that account considering external information available inside a presence of mixed-type fixed covariates by proposing a moeclust suite of models. these allow covariates influence a component weights and/or component densities by modelling a parameters of a mixture as functions of a covariates. the familiar range of constrained eigen-decomposition parameterisations of a component covariance matrices are also accommodated. this paper thus addresses a equivalent aims of including covariates inside gaussian parsimonious clustering models and incorporating parsimonious covariance structures into a gaussian mixture of experts framework. a moeclust models demonstrate significant improvement from both perspectives inside applications to univariate and multivariate data sets."
"inside this work, we explain inside detail how receptive fields, effective receptive fields, and projective fields of neurons inside different layers, convolution or pooling, of the convolutional neural network (cnn) are calculated. while our focus here was on cnns, a same operations, but inside a reverse order, should be used to calculate these quantities considering deconvolutional neural networks. these are important concepts, not only considering better understanding and analyzing convolutional and deconvolutional networks, but also considering optimizing their performance inside real-world applications."
"we have synthesized two iron fluo-arsenides $a$ca$_2$fe$_4$as$_4$f$_2$ with $a$ = rb and cs, analogous to a newly discovered superconductor kca$_2$fe$_4$as$_4$f$_2$. a quinary inorganic compounds crystallize inside the body-centered tetragonal lattice with space group i4/mmm, which contain double fe$_2$as$_2$ layers that are separated by insulating ca$_2$f$_2$ layers. our electrical and magnetic measurements on a polycrystalline samples demonstrate that a new materials undergo superconducting transitions at tc = 30.5 k and 28.2 k, respectively, without extrinsic doping. a correlations between tc and structural parameters are discussed."
"tungsten oxide and its associated bronzes (compounds of tungsten oxide and an alkali metal) are well known considering their interesting optical and electrical characteristics. we have modified a transport properties of thin wo$_3$ films by electrolyte gating with the help of both ionic liquids and polymer electrolytes. we are able to tune a resistivity of a gated film by more than five orders of magnitude, and the clear insulator-to-metal transition was observed. to clarify a doping mechanism, we have performed the series of incisive operando experiments, ruling out both the purely electronic effect (charge accumulation near a interface) and oxygen-related mechanisms. we propose instead that hydrogen intercalation was responsible considering doping wo$_3$ into the highly conductive ground state and provide evidence that it should be described as the dense polaronic gas."
"concentration of measure was the principle that informally states that inside some spaces any lipschitz function was essentially constant on the set of almost full measure. from the geometric point of view, it was very important to find some structured subsets on which this phenomenon occurs. inside this paper, i generalize the well-known result on a sphere due to milman to the class of riemannian manifolds. i prove that any lipschitz function on the compact, positively curved, homogeneous space was almost constant on the high dimensional submanifold."
"a rapid expansion of wind and solar energy leads to an increasing volatility inside a electricity generation. previous studies have shown that storage devices provide an opportunity to balance fluctuations inside a power grid. an economical operation of these devices was linked to solutions of probabilistic optimization problems, due to a fact that future generation was not deterministic inside general. considering this reason, reliable forecast methods as well as appropriate robust optimization algorithms take an increasingly important role inside future power operation systems. taking an uncertain availability of electricity into account, we present the method to calculate cost-optimal charging strategies considering energy storage units. a proposed method solves subproblems which result from the sliding window idea behind the method applied on the linear program by utilizing statistical measures. a prerequisite of this method was the recently developed fast algorithm considering storage-related optimization problems. to present a potential of a proposed method, the power-to-heat storage system was investigated as an example with the help of today's available forecast data and the robust statistical measure. second, a novel idea behind the method proposed here was compared with the common robust optimization method considering stochastic scenario problems. inside comparison, a proposed method provides lower acquisition costs, especially considering today's available forecasts, and was more robust under perturbations inside terms of deteriorating predictions, both based on empirical analyses. furthermore, a introduced idea behind the method was applicable to general cost-optimal operation problems and also real-time optimization concerning uncertain acquisition costs."
"epitaxial fe(se,te) thin films were prepared by pulsed laser deposition on (la0.18sr0.82)(al0.59ta0.41)o3 (lsat), caf2-buffered lsat and bare caf2 substrates, which exhibit an almost identical in-plane lattice parameter. a composition of all fe(se,te) films were determined to be fese0.7te0.3 by energy dispersive x-ray spectroscopy, irrespective of a substrate. albeit a lattice parameters of all templates have comparable values, a in-plane lattice parameter of a fese0.7te0.3 films varies significantly. we found that a superconducting transition temperature (tc) of fese0.7te0.3 thin films was strongly correlated with their a-axis lattice parameter. a highest tc of over 19 k is observed considering a film on bare caf2 substrate, which was related to unexpectedly large in-plane compressive strain originating mostly from a thermal expansion mismatch between a fese0.7te0.3 film and a substrate."
"this work presents an unsupervised idea behind the method considering improving wordnet that builds upon recent advances inside document and sense representation using distributional semantics. we apply our methods to construct wordnets inside french and russian, languages which both lack good manual constructions.1 these are evaluated on two new 600-word test sets considering word-to-synset matching and found to improve greatly upon synset recall, outperforming a best automated wordnets inside f-score. our methods require very few linguistic resources, thus being applicable considering wordnet construction inside low-resources languages, and may further be applied to sense clustering and other wordnet improvements."
"we present an idea behind the method considering agents to learn representations of the global map from sensor data, to aid their exploration inside new environments. to achieve this, we embed procedures mimicking that of traditional simultaneous localization and mapping (slam) into a soft attention based addressing of external memory architectures, inside which a external memory acts as an internal representation of a environment. this structure encourages a evolution of slam-like behaviors in the completely differentiable deep neural network. we show that this idea behind the method should aid reinforcement learning agents to successfully explore new environments where long-term memory was essential. we validate our idea behind the method inside both challenging grid-world environments and preliminary gazebo experiments. the video of our experiments should be found at: this https url."
"matrix decomposition was the popular and fundamental idea behind the method inside machine learning and data mining. it has been successfully applied into various fields. most matrix decomposition methods focus on decomposing the data matrix from one single source. however, it was common that data are from different sources with heterogeneous noise. the few of matrix decomposition methods have been extended considering such multi-view data integration and pattern discovery. while only few methods were designed to consider a heterogeneity of noise inside such multi-view data considering data integration explicitly. to this end, we propose the joint matrix decomposition framework (bjmd), which models a heterogeneity of noise by gaussian distribution inside the bayesian framework. we develop two algorithms to solve this model: one was the variational bayesian inference algorithm, which makes full use of a posterior distribution; and another was the maximum the posterior algorithm, which was more scalable and should be easily paralleled. extensive experiments on synthetic and real-world datasets demonstrate that bjmd considering a heterogeneity of noise was superior or competitive to a state-of-the-art methods."
"we use two catalogues, the herschel catalogue selected at 500 mu (hermes) and an iras catalogue selected at 60 mu (rifscz), to contrast a sky at these two wavelengths. both surveys demonstrate a existence of extreme starbursts, with star-formation rates (sfrs) > 5000 msun/yr. a maximum intrinsic star-formation rate appears to be ~30,000 msun/yr. a sources with apparent sfr estimates higher than this are inside all cases either lensed systems, blazars, or erroneous photometric redshifts. at redshifts of 3 to 5, a time-scale considering a herschel galaxies to make their current mass of stars at their present rate of formation ~ 10^8 yrs, so these galaxies are making the significant fraction of their stars inside a current star-formation episode. with the help of dust mass as the proxy considering gas mass, a herschel galaxies at redshift 3 to 5 have gas masses comparable to their mass inside stars. of a 38 extreme starbursts inside our herschel survey considering which we have more complete sed information, over 50% show evidence considering qso-like optical emission, or exhibit agn dust tori inside a mid-infrared seds. inside all cases however a infrared luminosity was dominated by the starburst component. we derive the mean covering factor considering agn dust as the function of redshift and derive black hole masses and black hole accretion rates. there was the universal ratio of black-hole mass to stellar mass, ~ 10^{-3}, driven by a strong period of star-formation and black-hole growth at z = 1-5."
"a paper studies compactness properties of a affine sobolev inequality of gaoyong zhang et al inside a case $p=2$, and existence and regularity of related minimizers, inside particular, solutions to a nonlocal dirichlet problems \[ -\sum_{i,j=1}^{n}(a^{-1}[u])_{ij}\frac{\partial^2u}{\partial x_i\partial x_j}=f \mbox{ inside }\omega\subset\mathbb r^n, \] and \[ -\sum_{i,j=1}^{n}(a^{-1}[u])_{ij}\frac{\partial^2u}{\partial x_i\partial x_j}=u^{q-1}\,,\quad u>0,\mbox{ inside }\omega\subset\mathbb r^n, \] where $a_{ij}[u]=\int_\omega\frac{\partial u}{\partial x_i}\frac{\partial u}{\partial x_j}\mathrm{d}x$ and $q\in(2,\frac{2n}{n-2})$."
"inside recent literature, moonshine has been explored considering some groups beyond a monster, considering example a sporadic o'nan and thompson groups. this collection of examples may suggest that moonshine was the rare phenomenon, but the fundamental and largely unexplored question was how general a correspondence was between modular forms and finite groups. considering every finite group $g$, we give constructions of infinitely many graded infinite-dimensional $\mathbb{c}[g]$-modules where a mckay-thompson series considering the conjugacy class $[g]$ was the weakly holomorphic modular function properly on $\gamma_0(\text{ord}(g))$. as there are only finitely many normalized hauptmoduln, groups whose mckay-thompson series are normalized hauptmoduln are rare, but not as rare as one might naively expect. we give bounds on a powers of primes dividing a order of groups which have normalized hauptmoduln of level $\text{ord}(g)$ as a graded trace functions considering any conjugacy class $[g]$, and completely classify a finite abelian groups with this property. inside particular, these include $(\mathbb{z} / 5 \mathbb{z})^5$ and $(\mathbb{z} / 7 \mathbb{z})^4$, which are not subgroups of a monster."
"associating image regions with text queries has been recently explored as the new way to bridge visual and linguistic representations. the few pioneering approaches have been proposed based on recurrent neural language models trained generatively (e.g., generating captions), but achieving somewhat limited localization accuracy. to better address natural-language-based visual entity localization, we propose the discriminative approach. we formulate the discriminative bimodal neural network (dbnet), which should be trained by the classifier with extensive use of negative samples. our training objective encourages better localization on single images, incorporates text phrases inside the broad range, and properly pairs image regions with text phrases into positive and negative examples. experiments on a visual genome dataset demonstrate a proposed dbnet significantly outperforms previous state-of-the-art methods both considering localization on single images and considering detection on multiple images. we we also establish an evaluation protocol considering natural-language visual detection."
"automatic summarisation was the popular idea behind the method to reduce the document to its main arguments. recent research inside a area has focused on neural approaches to summarisation, which should be very data-hungry. however, few large datasets exist and none considering a traditionally popular domain of scientific publications, which opens up challenging research avenues centered on encoding large, complex documents. inside this paper, we introduce the new dataset considering summarisation of computer science publications by exploiting the large resource of author provided summaries and show straightforward ways of extending it further. we develop models on a dataset making use of both neural sentence encoding and traditionally used summarisation features and show that models which encode sentences as well as their local and global context perform best, significantly outperforming well-established baseline methods."
"we highlight some subtleties that affect naive implementations of quadrupolar and octupolar gravitational waveforms from numerically-integrated trajectories of three-body systems. some of those subtleties arise from a requirement that a source be contained inside its ""coordinate near zone"" when applying a standard pn formulae considering gravitational-wave emission, and from a need to use a non-linear einstein equations to correctly derive a quadrupole emission formula. we show that some of these subtleties were occasionally overlooked inside a literature, with consequences considering published results. we also provide prescriptions that lead to correct and robust predictions considering a waveforms computed from numerically-integrated orbits."
"we propose the training and evaluation idea behind the method considering autoencoder generative adversarial networks (gans), specifically a boundary equilibrium generative adversarial network (began), based on methods from a image quality assessment literature. our idea behind the method explores the multidimensional evaluation criterion that utilizes three distance functions: an $l_1$ score, a gradient magnitude similarity mean (gmsm) score, and the chrominance score. we show that each of a different distance functions captures the slightly different set of properties inside image space and, consequently, requires its own evaluation criterion to properly assess whether a relevant property has been adequately learned. we show that models with the help of a new distance functions are able to produce better images than a original began model inside predicted ways."
"we propose the probabilistic model considering interpreting gene expression levels that are observed through single-cell rna sequencing. inside a model, each cell has the low-dimensional latent representation. additional latent variables account considering technical effects that may erroneously set some observations of gene expression levels to zero. conditional distributions are specified by neural networks, giving a proposed model enough flexibility to fit a data well. we use variational inference and stochastic optimization to approximate a posterior distribution. a inference procedure scales to over one million cells, whereas competing algorithms do not. even considering smaller datasets, considering several tasks, a proposed procedure outperforms state-of-the-art methods like zifa and zinb-wave. we also extend our framework to take into account batch effects and other confounding factors and propose the natural bayesian hypothesis framework considering differential expression that outperforms tradition deseq2."
"common-sense and background knowledge was required to understand natural language, but inside most neural natural language understanding (nlu) systems, this knowledge must be acquired from training corpora during learning, and then it was static at test time. we introduce the new architecture considering a dynamic integration of explicit background knowledge inside nlu models. the general-purpose reading module reads background knowledge inside a form of free-text statements (together with task-specific text inputs) and yields refined word representations to the task-specific nlu architecture that reprocesses a task inputs with these representations. experiments on document question answering (dqa) and recognizing textual entailment (rte) demonstrate a effectiveness and flexibility of a approach. analysis shows that our model learns to exploit knowledge inside the semantically appropriate way."
"constrained model predictive control (mpc) was the widely used control strategy, which employs moving horizon-based on-line optimisation to compute a optimum path of a manipulated variables. nonlinear mpc should utilize detailed models but it was computationally expensive; on a other hand linear mpc may not be adequate. piecewise affine (pwa) models should describe a underlying nonlinear dynamics more accurately, therefore they should provide the viable trade-off through their use inside multi-model linear mpc configurations, which avoid integer programming. however, such schemes may introduce uncertainty affecting a closed loop stability. inside this work, we propose an input to output stability analysis considering closed loop systems, consisting of pwa models, where an observer and multi-model linear mpc are applied together, under unstructured uncertainty. integral quadratic constraints (iqcs) are employed to assess a robustness of mpc under uncertainty. we create the model pool, by performing linearisation on selected transient points. all a possible uncertainties and nonlinearities (including a controller) should be introduced inside a framework, assuming that they admit a appropriate iqcs, whilst a dissipation inequality should provide necessary conditions incorporating iqcs. we demonstrate a existence of static multipliers, which should reduce a conservatism of a stability analysis significantly. a proposed methodology was demonstrated through two engineering case studies."
"skorobogatov constructed the bielliptic surface which was the counterexample to a hasse principle not explained by a brauer-manin obstruction. we show that this surface has the $0$-cycle of degree 1, as predicted by the conjecture of colliot-thélène."
"we study a stochastic multi-armed bandit (mab) problem inside a presence of side-observations across actions that occur as the result of an underlying network structure. inside our model, the bipartite graph captures a relationship between actions and the common set of unknowns such that choosing an action reveals observations considering a unknowns that it was connected to. this models the common scenario inside online social networks where users respond to their friends' activity, thus providing side information about each other's preferences. our contributions are as follows: 1) we derive an asymptotic lower bound (with respect to time) as the function of a bi-partite network structure on a regret of any uniformly good policy that achieves a maximum long-term average reward. 2) we propose two policies - the randomized policy; and the policy based on a well-known upper confidence bound (ucb) policies - both of which explore each action at the rate that was the function of its network position. we show, under mild assumptions, that these policies achieve a asymptotic lower bound on a regret up to the multiplicative factor, independent of a network structure. finally, we use numerical examples on the real-world social network and the routing example network to demonstrate a benefits obtained by our policies over other existing policies."
"steerable properties dominate a design of traditional filters, e.g., gabor filters, and endow features a capability of dealing with spatial transformations. however, such excellent properties have not been well explored inside a popular deep convolutional neural networks (dcnns). inside this paper, we propose the new deep model, termed gabor convolutional networks (gcns or gabor cnns), which incorporates gabor filters into dcnns to enhance a resistance of deep learned features to a orientation and scale changes. by only manipulating a basic element of dcnns based on gabor filters, i.e., a convolution operator, gcns should be easily implemented and are compatible with any popular deep learning architecture. experimental results demonstrate a super capability of our algorithm inside recognizing objects, where a scale and rotation changes occur frequently. a proposed gcns have much fewer learnable network parameters, and thus was easier to train with an end-to-end pipeline."
"recurrent neural networks (rnns) are widely used to model sequential data but their non-linear dependencies between sequence elements prevent parallelizing training over sequence length. we show a training of rnns with only linear sequential dependencies should be parallelized over a sequence length with the help of a parallel scan algorithm, leading to rapid training on long sequences even with small minibatch size. we develop the parallel linear recurrence cuda kernel and show that it should be applied to immediately speed up training and inference of several state of a art rnn architectures by up to 9x. we abstract recent work on linear rnns into the new framework of linear surrogate rnns and develop the linear surrogate model considering a long short-term memory unit, a gilr-lstm, that utilizes parallel linear recurrence. we extend sequence learning to new extremely long sequence regimes that were previously out of reach by successfully training the gilr-lstm on the synthetic sequence classification task with the one million timestep dependency."
a aim of this paper was to obtain necessary and sufficient conditions considering a orthonormality of wavelet system arising out of left translations and nonisotropic dilations on a heisenberg group $\mathbb{h}$. the similar problem was also discussed considering a twisted wavelet system on $\mathbb{c}$.
"a dark matter time projection chamber (dmtpc) was the direction-sensitive detector designed to measure a direction of recoiling $^{19}$f and $^{12}$c nuclei inside low-pressure cf$_4$ gas with the help of optical and charge readout systems. inside this paper, we employ measurements from two dmtpc detectors, with operating pressures of 30-60 torr, to develop and validate the model of a directional response and performance of such detectors as the function of recoil energy. with the help of our model as the benchmark, we formulate a necessary specifications considering the scalable directional detector with sensitivity comparable to that of current-generation counting (non-directional) experiments, which measure only recoil energy. assuming a performance of existing dmtpc detectors, as well as current limits on a spin-dependent wimp-nucleus cross section, we find that the 10-20 kg scale direction-sensitive detector was capable of correlating a measured direction of nuclear recoils with a predicted direction of incident dark matter particles and providing decisive (3$\sigma$) confirmation that the candidate signal from the non-directional experiment is indeed induced by elastic scattering of dark matter particles off of target nuclei."
"we propose the new self-organizing hierarchical softmax formulation considering neural-network-based language models over large vocabularies. instead of with the help of the predefined hierarchical structure, our idea behind the method was capable of learning word clusters with clear syntactical and semantic meaning during a language model training process. we provide experiments on standard benchmarks considering language modeling and sentence compression tasks. we find that this idea behind the method was as fast as other efficient softmax approximations, while achieving comparable or even better performance relative to similar full softmax models."
"a uniqueness of parabolic cauchy problems was nowadays the classical problem and since hadamard \cite{ha} these kind of problems are known to be ill-posed and even severely ill-posed. until now there are only few partial results concerning a quantification of a stability considering parabolic cauchy problems. inside a present article, we bring a complete answer to this issue, provided that a space domain has finite diameter with respect to a geodesic distance and assuming that solutions are sufficiently smooth."
"we examine memory networks considering a task of question answering (qa), under common real world scenario where training examples are scarce and under weakly supervised scenario, that was only extrinsic labels are available considering training. we propose extensions considering a dynamic memory network (dmn), specifically within a attention mechanism, we call a resulting neural architecture as dynamic memory tensor network (dmtn). ultimately, we see that our proposed extensions results inside over 80% improvement inside a number of task passed against a baselined standard dmn and 20% more task passed compared to state-of-the-art end-to-end memory network considering facebook's single task weakly trained 1k babi dataset."
"we introduce coroica, confounding-robust independent component analysis, the novel ica algorithm which decomposes linearly mixed multivariate observations into independent components that are corrupted (and rendered dependent) by hidden group-wise stationary confounding. it extends a ordinary ica model inside the theoretically sound and explicit way to incorporate group-wise (or environment-wise) confounding. we show that our general noise model allows to perform ica inside settings where other noisy ica procedures fail. additionally, it should be used considering applications with grouped data by adjusting considering different stationary noise within each group. we show that a noise model has the natural relation to causality and explain how it should be applied inside a context of causal inference. inside addition to our theoretical framework, we provide an efficient approximation procedure and prove identifiability of a unmixing matrix under mild assumptions. finally, we illustrate a performance and robustness of our method on simulated data, provide audible and visual examples, and demonstrate a applicability to real-world scenarios by experiments on publicly available antarctic ice core data as well as two eeg data sets. we provide the scikit-learn compatible pip-installable python package coroica as well as r and matlab implementations accompanied by the documentation at this https url."
"we discuss different types of human-robot interaction paradigms inside a context of training end-to-end reinforcement learning algorithms. we provide the taxonomy to categorize a types of human interaction and present our cycle-of-learning framework considering autonomous systems that combines different human-interaction modalities with reinforcement learning. two key concepts provided by our cycle-of-learning framework are how it handles a integration of a different human-interaction modalities (demonstration, intervention, and evaluation) and how to define a switching criteria between them."
"we examine inside detail 15 babylonian observations of lunar appulses and occultations made between 80 and 419 bc considering a purpose of setting useful limits on earth's clock error, as quantified by $\delta$t, a difference between terrestrial time and universal time. our results are generally inside agreement with reconstructions of $\delta$t with the help of untimed solar eclipse observations from a same period. we suggest the revised version of a simple quadratic fit to $\delta$t inside light of a new results."
"this paper presents the new unsupervised learning idea behind the method with stacked autoencoder (sae) considering arabic handwritten digits categorization. recently, arabic handwritten digits recognition has been an important area due to its applications inside several fields. this work was focusing on a recognition part of handwritten arabic digits recognition that face several challenges, including a unlimited variation inside human handwriting and a large public databases. arabic digits contains ten numbers that were descended from a indian digits system. stacked autoencoder (sae) tested and trained a madbase database (arabic handwritten digits images) that contain 10000 testing images and 60000 training images. we show that a use of sae leads to significant improvements across different machine-learning classification algorithms. sae was giving an average accuracy of 98.5%."
"we propose the new probabilistic ant-based heuristic (anth-ls) considering a longest simple cycle problem. this np-hard problem has numerous real-world applications inside complex networks, including efficient construction of graph layouts, analysis of social networks or bioinformatics. our algorithm was based on reinforcing a probability of traversing a edges, which have not been present inside a long cycles found so far. experimental results are presented considering the set of social networks, protein-protein interation networks, network science graphs and dimacs graphs. considering 6 out of our 22 real-world network test instances, anth-ls has obtained an improvement on a longest cycle ever found."
"we construct the variational wave function considering a ground state of weakly interacting bosons that gives the lower energy than a mean-field girardeau-arnowitt (or hartree-fock-bogoliubov) theory. this improvement was brought about by incorporating a dynamical 3/2-body processes where one of two colliding non-condensed particles drops into a condensate and vice versa. a processes are also shown to transform a one-particle excitation spectrum into the bubbling mode with the finite lifetime even inside a long-wavelength limit. these 3/2-body processes, which give rise to dynamical exchange of particles between a non-condensate reservoir and condensate absent inside ideal gases, are identified as the key mechanism considering realizing and sustaining macroscopic coherence inside bose-einstein condensates."
"human-centered environments are rich with the wide variety of spatial relations between everyday objects. considering autonomous robots to operate effectively inside such environments, they should be able to reason about these relations and generalize them to objects with different shapes and sizes. considering example, having learned to place the toy in the basket, the robot should be able to generalize this concept with the help of the spoon and the cup. this requires the robot to have a flexibility to learn arbitrary relations inside the lifelong manner, making it challenging considering an expert to pre-program it with sufficient knowledge to do so beforehand. inside this paper, we address a problem of learning spatial relations by introducing the novel method from a perspective of distance metric learning. our idea behind the method enables the robot to reason about a similarity between pairwise spatial relations, thereby enabling it to use its previous knowledge when presented with the new relation to imitate. we show how this makes it possible to learn arbitrary spatial relations from non-expert users with the help of the small number of examples and inside an interactive manner. our extensive evaluation with real-world data demonstrates a effectiveness of our method inside reasoning about the continuous spectrum of spatial relations and generalizing them to new objects."
"h$_3^+$ was the ubiquitous and important astronomical species whose spectrum has been observed inside a interstellar medium, planets and tentatively inside a remnants of supernova sn1897a. its role as the cooler was important considering gas giant planets and exoplanets, and possibly a early universe. all this makes a spectral properties, cooling function and partition function of h$_3^+$ key parameters considering astronomical models and analysis. the new high-accuracy, very extensive line list considering h$_3^+$ called mizatep is computed as part of a exomol project alongside the temperature-dependent cooling function and partition function as well as lifetimes considering %individual excited states. these data are made available inside electronic form as supplementary data to this article and at this http url"
"anatomical and biophysical modeling of left atrium (la) and proximal pulmonary veins (ppvs) was important considering clinical management of several cardiac diseases. magnetic resonance imaging (mri) allows qualitative assessment of la and ppvs through visualization. however, there was the strong need considering an advanced image segmentation method to be applied to cardiac mri considering quantitative analysis of la and ppvs. inside this study, we address this unmet clinical need by exploring the new deep learning-based segmentation strategy considering quantification of la and ppvs with high accuracy and heightened efficiency. our idea behind the method was based on the multi-view convolutional neural network (cnn) with an adaptive fusion strategy and the new loss function that allows fast and more accurate convergence of a backpropagation based optimization. after training our network from scratch by with the help of more than 60k 2d mri images (slices), we have evaluated our segmentation strategy to a stacom 2013 cardiac segmentation challenge benchmark. qualitative and quantitative evaluations, obtained from a segmentation challenge, indicate that a proposed method achieved a state-of-the-art sensitivity (90%), specificity (99%), precision (94%), and efficiency levels (10 seconds inside gpu, and 7.5 minutes inside cpu)."
"inside an era where accumulating data was easy and storing it inexpensive, feature selection plays the central role inside helping to reduce a high-dimensionality of huge amounts of otherwise meaningless data. inside this paper, we propose the graph-based method considering feature selection that ranks features by identifying a most important ones into arbitrary set of cues. mapping a problem on an affinity graph-where features are a nodes-the solution was given by assessing a importance of nodes through some indicators of centrality, inside particular, a eigen-vector centrality (ec). a gist of ec was to approximate a importance of the feature as the function of a importance of its neighbors. ranking central nodes individuates candidate features, which turn out to be effective from the classification point of view, as proved by the thoroughly experimental section. our idea behind the method has been tested on 7 diverse datasets from recent literature (e.g., biological data and object recognition, among others), and compared against filter, embedded and wrappers methods. a results are remarkable inside terms of accuracy, stability and low execution time."
"most real-world document collections involve various types of metadata, such as author, source, and date, and yet a most commonly-used approaches to modeling text corpora ignore this information. while specialized models have been developed considering particular applications, few are widely used inside practice, as customization typically requires derivation of the custom inference algorithm. inside this paper, we build on recent advances inside variational inference methods and propose the general neural framework, based on topic models, to enable flexible incorporation of metadata and allow considering rapid exploration of alternative models. our idea behind the method achieves strong performance, with the manageable tradeoff between perplexity, coherence, and sparsity. finally, we demonstrate a potential of our framework through an exploration of the corpus of articles about us immigration."
"we investigate a dynamics of water confined inside soft ionic nano-assemblies, an issue critical considering the general understanding of a multi-scale structure-function interplay inside advanced materials. we focus inside particular on hydrated perfluoro-sulfonic acid compounds employed as electrolytes inside fuel cells. these materials form phase-separated morphologies that show outstanding proton-conducting properties, directly related to a state and dynamics of a absorbed water. we have quantified water motion and ion transport by combining quasi elastic neutron scattering, pulsed field gradient nuclear magnetic resonance, and molecular dynamics computer simulation. effective water and ion diffusion coefficients have been determined together with their variation upon hydration at a relevant atomic, nanoscopic and macroscopic scales, providing the complete picture of transport. we demonstrate that confinement at a nanoscale and direct interaction with a charged interfaces produce anomalous sub-diffusion, due to the heterogeneous space-dependent dynamics within a ionic nanochannels. this was irrespective of a details of a chemistry of a hydrophobic confining matrix, confirming a statistical significance of our conclusions. our findings turn out to indicate interesting connections and possibilities of cross-fertilization with other domains, including biophysics. they also establish fruitful correspondences with advanced topics inside statistical mechanics, resulting inside new possibilities considering a analysis of neutron scattering data."
"unmanned aircraft have decreased a cost required to collect remote sensing imagery, which has enabled researchers to collect high-spatial resolution data from multiple sensor modalities more frequently and easily. a increase inside data will push a need considering semantic segmentation frameworks that are able to classify non-rgb imagery, but this type of algorithmic development requires an increase inside publicly available benchmark datasets with class labels. inside this paper, we introduce the high-resolution multispectral dataset with image labels. this new benchmark dataset has been pre-split into training/testing folds inside order to standardize evaluation and continue to push state-of-the-art classification frameworks considering non-rgb imagery."
we propose the novel approximation procedure considering scale-by-scale lead-lag relationships of financial assets observed at the high-frequency inside the non-synchronous manner. a proposed approximation procedure does not require any interpolation processing of a original data and was applicable to quite fine resolution data. a validity of a proposed estimators was shown under a continuous-time framework developed inside our previous work hayashi and koike (2016). an empirical application shows promising results of a proposed approach.
"consider a linear congruence equation $x_1+\ldots+x_k \equiv b\,(\text{mod } n)$ considering $b,n\in\mathbb{z}$. by $(a,b)_s$, we mean a largest $l^s\in\mathbb{n}$ which divides $a$ and $b$ simultaneously. considering each $d_j|n$, define $\mathcal{c}_{j,s} = \{1\leq x\leq n^s | (x,n^s)_s = d^s_j\}$. bibak et al. gave the formula with the help of ramanujan sums considering a number of solutions of a above congruence equation with some gcd restrictions on $x_i$. we generalize their result with generalized gcd restrictions on $x_i$ by proving that considering a above linear congruence, a number of solutions was $$\frac{1}{n^s}\sum\limits_{d|n}c_{d,s}(b)\prod\limits_{j=1}^{\tau(n)}\left(c_{\frac{n}{d_j},s}(\frac{n^s}{d^s})\right)^{g_j}$$ where $g_j = |\{x_1,\ldots, x_k\}\cap \mathcal{c}_{j,s}|$ considering $j=1,\ldots \tau(n)$ and $c_{d,s}$ denote a generalized ramanujan sum defined by e. cohen."
"the method considering classifying orbits near asteroids under the polyhedral gravitational field was presented, and may serve as the valuable reference considering spacecraft orbit design considering asteroid exploration. a orbital dynamics near asteroids are very complex. according to a variation inside orbit characteristics after being affected by gravitational perturbation during a periapsis passage, orbits near an asteroid should be classified into 9 categories: (1) surroundingto-surrounding, (2) surrounding-to-surface, (3) surroundingto-infinity, (4) infinity-to-infinity, (5) infinity-to-surface, (6) infinity-to-surrounding, (7) surface-to-surface, (8) surfaceto-surrounding, and (9) surface-to- infinity. assume that a orbital elements are constant near a periapsis, a gravitation potential was expanded into the harmonic series. then, a influence of a gravitational perturbation on a orbit was studied analytically. a styles of orbits are dependent on a argument of periapsis, a periapsis radius, and a periapsis velocity. given a argument of periapsis, a orbital energy before and after perturbation should be derived according to a periapsis radius and a periapsis velocity. simulations have been performed considering orbits inside a gravitational field of 216 kleopatra. a numerical results are well consistent with analytic predictions."
"inside recent years, endomicroscopy has become increasingly used considering diagnostic purposes and interventional guidance. it should provide intraoperative aids considering real-time tissue characterization and should aid to perform visual investigations aimed considering example to discover epithelial cancers. due to physical constraints on a acquisition process, endomicroscopy images, still today have the low number of informative pixels which hampers their quality. post-processing techniques, such as super-resolution (sr), are the potential solution to increase a quality of these images. sr techniques are often supervised, requiring aligned pairs of low-resolution (lr) and high-resolution (hr) images patches to train the model. however, inside our domain, a lack of hr images hinders a collection of such pairs and makes supervised training unsuitable. considering this reason, we propose an unsupervised sr framework based on an adversarial deep neural network with the physically-inspired cycle consistency, designed to impose some acquisition properties on a super-resolved images. our framework should exploit hr images, regardless of a domain where they are coming from, to transfer a quality of a hr images to a initial lr images. this property should be particularly useful inside all situations where pairs of lr/hr are not available during a training. our quantitative analysis, validated with the help of the database of 238 endomicroscopy video sequences from 143 patients, shows a ability of a pipeline to produce convincing super-resolved images. the mean opinion score (mos) study also confirms this quantitative image quality assessment."
"a performance of automatic speech recognition (asr) systems should be significantly compromised by previously unseen conditions, which was typically due to the mismatch between training and testing distributions. inside this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to asr systems, resolving a mismatch problem. specifically, we investigate the recent model, called a factorized hierarchical variational autoencoder (fhvae). fhvaes learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. we argue that a set of latent variables that contain segment-level information was our desired domain invariant feature considering asr. experiments are conducted on aurora-4 and chime-4, which demonstrate 41% and 27% absolute word error rate reductions respectively on mismatched domains."
"we consider a use of randomised forward models and log-likelihoods within a bayesian idea behind the method to inverse problems. such random approximations to a exact forward model or log-likelihood arise naturally when the computationally expensive model was approximated with the help of the cheaper stochastic surrogate, as inside gaussian process emulation (kriging), or inside a field of probabilistic numerical methods. we show that a hellinger distance between a exact and approximate bayesian posteriors was bounded by moments of a difference between a true and approximate log-likelihoods. example applications of these stability results are given considering randomised misfit models inside large data applications and a probabilistic solution of ordinary differential equations."
"we use atacama large millimeter/submillimeter array band 5 science verification observations of a red supergiant vy cma to study a polarization of sio thermal/masers lines and dust continuum at ~1.7 mm wavelength. we analyse both linear and circular polarization and derive a magnetic field strength and structure, assuming a polarization of a lines originates from a zeeman effect, and that of a dust originates from aligned dust grains. we also discuss other effects that could give rise to a observed polarization. we detect, considering a first time, significant polarization (~3%) of a circumstellar dust emission at millimeter wavelengths. a polarization was uniform with an electric vector position angle of $\sim8^\circ$. varying levels of linear polarization are detected considering a j=4-3 28sio v=0, 1, 2, and 29sio v=0, 1 lines, with a strongest polarization fraction of ~30% found considering a 29sio v=1 maser. a linear polarization vectors rotate with velocity, consistent with earlier observations. we also find significant (up to ~1%) circular polarization inside several lines, consistent with previous measurements. we conclude that a detection was robust against calibration and regular instrumental errors, although we cannot yet fully rule out non-standard instrumental effects. emission from magnetically aligned grains was a most likely origin of a observed continuum polarization. this implies that a dust was embedded inside the magnetic field >13 mg. a maser line polarization traces a magnetic field structure. a magnetic field inside a gas and dust was consistent with an approximately toroidal field configuration, but only higher angular resolution observations will be able to reveal more detailed field structure. if a circular polarization was due to zeeman splitting, it indicates the magnetic field strength of ~1-3 gauss, consistent with previous maser observations."
"inside this work, we study a quantum entanglement and compute entanglement entropy inside de sitter space considering the bipartite quantum field theory driven by axion originating from ${\bf type~ iib}$ string compactification on the calabi yau three fold (${\bf cy^3}$) and inside presence of ${\bf ns5}$ brane. considering this compuation, we consider the spherical surface ${\bf s}^2$, which divide a spatial slice of de sitter (${\bf ds_4}$) into exterior and interior sub regions. we also consider a initial choice of vaccum to be bunch davies state. first we derive a solution of a wave function of axion inside the hyperbolic open chart by constructing the suitable basis considering bunch davies vacuum state with the help of bogoliubov transformation. we then, derive a expression considering density matrix by tracing over a exterior region. this allows us to compute entanglement entropy and r$\acute{e}$nyi entropy inside $3+1$ dimension. further we quantify a uv finite contribution of entanglement entropy which contain a physics of long range quantum correlations of our expanding universe. finally, our analysis compliments a necessary condition considering a violation of bell's inequality inside primordial cosmology due to a non vanishing entanglement entropy considering axionic bell pair."
"travel providers such as airlines and on-line travel agents are becoming more and more interested inside understanding how passengers choose among alternative itineraries when searching considering flights. this knowledge helps them better display and adapt their offer, taking into account market conditions and customer needs. some common applications are not only filtering and sorting alternatives, but also changing certain attributes inside real-time (e.g., changing a price). inside this paper, we concentrate with a problem of modeling air passenger choices of flight itineraries. this problem has historically been tackled with the help of classical discrete choice modelling techniques. traditional statistical approaches, inside particular a multinomial logit model (mnl), was widely used inside industrial applications due to its simplicity and general good performance. however, mnl models present several shortcomings and assumptions that might not hold inside real applications. to overcome these difficulties, we present the new choice model based on pointer networks. given an input sequence, this type of deep neural architecture combines recurrent neural networks with a attention mechanism to learn a conditional probability of an output whose values correspond to positions inside an input sequence. therefore, given the sequence of different alternatives presented to the customer, a model should learn to point to a one most likely to be chosen by a customer. a proposed method is evaluated on the real dataset that combines on-line user search logs and airline flight bookings. experimental results show that a proposed model outperforms a traditional mnl model on several metrics."
"a survey volume of the proper motion-limited sample was typically much smaller than the magnitude-limited sample. this was because of a noisy astrometric measurements from detectors that are not dedicated considering astrometric missions. inside order to apply an empirical completeness correction, existing works limit a survey depth to a shallower parts of a sky that hamper a maximum potential of the survey. a number of epoch of measurement was the discrete quantity that cannot be interpolated across a projected plane of observation, so that a survey properties change inside discrete steps across a sky. this work proposes the method to dissect a survey into small parts with voronoi tessellation with the help of candidate objects as generating points, such that each part defines the `mini-survey' that has its own properties. coupling with the maximum volume density estimator, a new method was demonstrated to be unbiased and recovered {\sim}20% more objects than a existing method inside the mock catalogue of the white dwarf-only solar neighbourhood with pan--starrs 1-like characteristics. towards a end of this work, we demonstrate one way to increase a tessellation resolution with artificial generating points, which would be useful considering analysis of rare objects with small number counts."
"decades of research inside control theory have shown that simple controllers, when provided with timely feedback, should control complex systems. pushing was an example of the complex mechanical system that was difficult to model accurately due to unknown system parameters such as coefficients of friction and pressure distributions. inside this paper, we explore a data-complexity required considering controlling, rather than modeling, such the system. results show that the model-based control approach, where a dynamical model was learned from data, was capable of performing complex pushing trajectories with the minimal amount of training data (10 data points). a dynamics of pushing interactions are modeled with the help of the gaussian process (gp) and are leveraged within the model predictive control idea behind the method that linearizes a gp and imposes actuator and task constraints considering the planar manipulation task."
"graph-based semi-supervised learning (ssl) algorithms predict labels considering all nodes based on provided labels of the small set of seed nodes. classic methods capture a graph structure through some underlying diffusion process that propagates through a graph edges. spectral diffusion, which includes personalized page rank and label propagation, propagates through random walks. social diffusion propagates through shortest paths. the common ground to these diffusions was their {\em linearity}, which does not distinguish between contributions of few ""strong"" relations and many ""weak"" relations. recently, non-linear methods such as node embeddings and graph convolutional networks (gcn) demonstrated the large gain inside quality considering ssl tasks. these methods introduce multiple components and greatly vary on how a graph structure, seed label information, and other features are used. we aim here to study a contribution of non-linearity, as an isolated ingredient, to a performance gain. to do so, we place classic linear graph diffusions inside the self-training framework. surprisingly, we observe that ssl with the help of a resulting {\em bootstrapped diffusions} not only significantly improves over a respective non-bootstrapped baselines but also outperform state-of-the-art non-linear ssl methods. moreover, since a self-training wrapper retains a scalability of a base method, we obtain both higher quality and better scalability."
"a maximum balanced biclique problem was the well-known graph model with relevant applications inside diverse domains. this paper introduces the novel algorithm, which combines an effective constraint-based tabu search procedure and two dedicated graph reduction techniques. we verify a effectiveness of a algorithm on 30 classical random benchmark graphs and 25 very large real-life sparse graphs from a popular koblenz network collection (konect). a results show that a algorithm improves a best-known results (new lower bounds) considering 10 classical benchmarks and obtains a optimal solutions considering 14 konect instances."
"this paper considers a distributed event-triggered consensus problem considering general linear multi-agent networks. both a leaderless and leader-follower consensus problems are considered. based on a local sampled state or local output information, distributed adaptive event-triggered protocols are designed, which should ensure that consensus of a agents was achieved and a zeno behavior was excluded by showing that a interval between any two triggering events was lower bounded by the strictly positive value. compared to a previous related works, our main contribution was that a proposed adaptive event-based protocols are fully distributed and scalable, which do not rely on any global information of a network graph and are independent of a network's scale. inside these event-based protocols, continuous communications are not required considering either control laws updating or triggering functions monitoring."
"network growth processes should be understood as generative models of a structure and history of complex networks. this point of view naturally leads to a problem of network archaeology: reconstructing all a past states of the network from its structure---a difficult permutation inference problem. inside this paper, we introduce the bayesian formulation of network archaeology, with the generalization of preferential attachment as our generative mechanism. we develop the sequential importance sampling algorithm to evaluate a posterior averages of this model, as well as an efficient heuristic that uncovers a history of the network inside linear time. we use these methods to identify and characterize the phase transition inside a quality of a reconstructed history, when they are applied to artificial networks generated by a model itself. despite a existence of the no-recovery phase, we find that non-trivial inference was possible inside the large portion of a parameter space as well as on empirical data."
"we consider a problem of extracting curve skeletons of three-dimensional, elongated objects given the noisy surface, which has applications inside agricultural contexts such as extracting a branching structure of plants. we describe an efficient and robust method based on breadth-first search that should determine curve skeletons inside these contexts. our idea behind the method was capable of automatically detecting junction points as well as spurious segments and loops. all of that was accomplished with only one user-adjustable parameter. a run time of our method ranges from hundreds of milliseconds to less than four seconds on large, challenging datasets, which makes it appropriate considering situations where real-time decision making was needed. experiments on synthetic models as well as on data from real world objects, some of which were collected inside challenging field conditions, show that our idea behind the method compares favorably to classical thinning algorithms as well as to recent contributions to a field."
"surrogate models provide the low computational cost alternative to evaluating expensive functions. a construction of accurate surrogate models with large numbers of independent variables was currently prohibitive because it requires the large number of function evaluations. gradient-enhanced kriging has a potential to reduce a number of function evaluations considering a desired accuracy when efficient gradient computation, such as an adjoint method, was available. however, current gradient-enhanced kriging methods do not scale well with a number of sampling points due to a rapid growth inside a size of a correlation matrix where new information was added considering each sampling point inside each direction of a design space. they do not scale well with a number of independent variables either due to a increase inside a number of hyperparameters that needs to be estimated. to address this issue, we develop the new gradient-enhanced surrogate model idea behind the method that drastically reduced a number of hyperparameters through a use of a partial-least squares method that maintains accuracy. inside addition, this method was able to control a size of a correlation matrix by adding only relevant points defined through a information provided by a partial-least squares method. to validate our method, we compare a global accuracy of a proposed method with conventional kriging surrogate models on two analytic functions with up to 100 dimensions, as well as engineering problems of varied complexity with up to 15 dimensions. we show that a proposed method requires fewer sampling points than conventional methods to obtain a desired accuracy, or provides more accuracy considering the fixed budget of sampling points. inside some cases, we get over 3 times more accurate models than the bench of surrogate models from a literature, and also over 3200 times faster than standard gradient-enhanced kriging models."
"this tutorial introduces the new and powerful set of techniques variously called ""neural machine translation"" or ""neural sequence-to-sequence models"". these techniques have been used inside the number of tasks regarding a handling of human language, and should be the powerful tool inside a toolbox of anyone who wants to model sequential data of some sort. a tutorial assumes that a reader knows a basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. it attempts to explain a intuition behind a various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with the suggestion considering an implementation exercise, where readers should test that they understood a content inside practice."
"inside 1934, f. yates described the sum of squares considering testing factor main effects inside saturated unbalanced models considering effects of two factors. he claimed no particular properties of this sum of squares other than that it provided an ""efficient approximate of a variance from a the means of a sub-class means... ."" although it became widely regarded as a gold standard inside a two-factor model, its fundamental properties and relations to other sums of squares considering a same model were not established until decades later. its method has not been extended to more general settings. this paper shows how yates's idea behind the method should be extended to construct numerator sums of squares considering test statistics considering linear hypotheses inside general linear models. it was shown that yates's sum of squares was equivalent to a restricted model - full model difference inside error sum of squares, which inside turn was shown to be a unique sum of squares that tests exactly a hypothesis inside question."
"we use a lda+u idea behind the method to search considering possible ordered ground states of lasrcoo$_4$. we find the staggered arrangement of magnetic multipoles to be stable over the broad range of co $3d$ interaction parameters. this ordered state should be described as the spin-denity-wave-type condensate of $d_{xy} \otimes d_{x^2-y^2}$ excitons carrying spin $s=1$. further, we construct an effective strong-coupling model, calculate a exciton dispersion and investigate closing of a exciton gap, which marks a exciton condensation instability. comparing a layered lasrcoo$_4$ with its pseudo cubic analog lacoo$_3$, we find that considering a same interaction parameters a excitonic gap was smaller (possibly vanishing) inside a layered cobaltite."
"smoothed dissipative particle dynamics (sdpd) was the mesoscopic method which allows to select a level of resolution at which the fluid was simulated. a aim of this work was to extend sdpd to chemically reactive systems.to this end, an additional progress variable was attached to each mesoparticle and evolves according to chemical kinetics. this reactive sdpd model was illustrated with numerical studies of a shock-to-detonation transition inside nitromethane as well as a stationary behavior of a reactive wave."
"we present observations and discussion of previously unreported phenomena discovered while training residual networks. a goal of this work was to better understand a nature of neural networks through a examination of these new empirical results. these behaviors were identified through a application of cyclical learning rates (clr) and linear network interpolation. among these behaviors are counterintuitive increases and decreases inside training loss and instances of rapid training. considering example, we demonstrate how clr should produce greater testing accuracy than traditional training despite with the help of large learning rates. files to replicate these results are available at this https url"
"we compare a existent methods including a minimum spanning tree based method and a local stellar density based method, inside measuring mass segregation of star clusters. we find that a minimum spanning tree method reflects more a compactness, which represents a global spatial distribution of massive stars, while a local stellar density method reflects more a crowdedness, which provides a local gravitational potential information. it was suggested to measure a local and a global mass segregation simultaneously. we also develop the hybrid method that takes both aspects into account. this hybrid method balances a local and a global mass segregation inside a sense that a predominant one was either caused either by dynamical evolution or purely accidental, especially when such information was unknown the priori. inside addition, we test our prescriptions with numerical models and show a impact of binaries inside estimating a mass segregation value. as an application, we use these methods on a orion nebula cluster (onc) observations and a taurus cluster. we find that a onc was significantly mass segregated down to a 20th most massive stars. inside contrast, a massive stars of a taurus cluster are sparsely distributed inside many different subclusters, showing the low degree of compactness. a massive stars of taurus are also found to be distributed inside a high-density region of a subclusters, showing significant mass segregation at subcluster scales. meanwhile, we also apply these methods to discuss a possible mechanisms of a dynamical evolution of a simulated substructured star clusters."
"we develop the 3d porous medium model considering sap flow within the tree stem, which consists of the nonlinear parabolic partial differential equation with the suitable transpiration source term. with the help of an asymptotic analysis, we derive approximate series solutions considering a liquid saturation and sap velocity considering the general class of coefficient functions. several important non-dimensional parameters are identified that should be used to characterize various flow regimes. we investigate a relative importance of stem aspect ratio versus anisotropy inside a sapwood hydraulic conductivity, and how these two effects impact a radial and vertical components of sap velocity. a analytical results are validated by means of the second-order finite volume discretization of a governing equations, and comparisons are drawn to experimental results on norway spruce trees."
"hysteresis was the highly nonlinear phenomenon, showing up inside the wide variety of science and engineering problems. a identification of hysteretic systems from input-output data was the challenging task. recent work on black-box polynomial nonlinear state-space modeling considering hysteresis identification has provided promising results, but struggles with the large number of parameters due to a use of multivariate polynomials. this drawback was tackled inside a current paper by applying the decoupling idea behind the method that results inside the more parsimonious representation involving univariate polynomials. this work was carried out numerically on input-output data generated by the bouc-wen hysteretic model and follows up on earlier work of a authors. a current article discusses a polynomial decoupling idea behind the method and explores a selection of a number of univariate polynomials with a polynomial degree, as well as a connections with neural network modeling. we have found that a presented decoupling idea behind the method was able to reduce a number of parameters of a full nonlinear model up to about 50\%, while maintaining the comparable output error level."
"we introduce a deep symbolic network (dsn) model, which aims at becoming a white-box version of deep neural networks (dnn). a dsn model provides the simple, universal yet powerful structure, similar to dnn, to represent any knowledge of a world, which was transparent to humans. a conjecture behind a dsn model was that any type of real world objects sharing enough common features are mapped into human brains as the symbol. those symbols are connected by links, representing a composition, correlation, causality, or other relationships between them, forming the deep, hierarchical symbolic network structure. powered by such the structure, a dsn model was expected to learn like humans, because of its unique characteristics. first, it was universal, with the help of a same structure to store any knowledge. second, it should learn symbols from a world and construct a deep symbolic networks automatically, by utilizing a fact that real world objects have been naturally separated by singularities. third, it was symbolic, with a capacity of performing causal deduction and generalization. fourth, a symbols and a links between them are transparent to us, and thus we will know what it has learned or not - which was a key considering a security of an ai system. fifth, its transparency enables it to learn with relatively small data. sixth, its knowledge should be accumulated. last but not least, it was more friendly to unsupervised learning than dnn. we present a details of a model, a algorithm powering its automatic learning ability, and describe its usefulness inside different use cases. a purpose of this paper was to generate broad interest to develop it within an open source project centered on a deep symbolic network (dsn) model towards a development of general ai."
"we propose general computational procedures based on descriptor state-space realizations to compute coprime factorizations of rational matrices with minimum degree denominators. enhanced recursive pole dislocation techniques are developed, which allow to successively place all poles of a factors into the given ""good"" domain of a complex plane. a resulting mcmillan degree of a denominator factor was equal to a number of poles lying inside a complementary ""bad"" region and therefore was minimal. a new pole dislocation techniques are employed to compute coprime factorizations with proper and stable factors of arbitrary improper rational matrices and coprime factorizations with inner denominators. a proposed algorithms work considering arbitrary descriptor representations, regardless they are stabilizable or detectable."
"let $k$ be the field of characteristic different from $2$ and let $e$ be an elliptic curve over $k$, defined either by an equation of a form $y^{2} = f(x)$ with degree $3$ or as a jacobian of the curve defined by an equation of a form $y^{2} = f(x)$ with degree $4$. we obtain generators over $k$ of a $8$-division field $k(e[8])$ of $e$ given as formulas inside terms of a roots of a polynomial $f$, and we explicitly describe a action of the particular automorphism inside $\mathrm{gal}(k(e[8]) / k)$."
"this paper considers a use of machine learning (ml) inside medicine by focusing on a main problem that this computational idea behind the method has been aimed at solving or at least minimizing: uncertainty. to this aim, we point out how uncertainty was so ingrained inside medicine that it biases also a representation of clinical phenomena, that was a very input of ml models, thus undermining a clinical significance of their output. recognizing this should motivate both medical doctors, inside taking more responsibility inside a development and use of these decision aids, and a researchers, inside pursuing different ways to assess a value of these systems. inside so doing, both designers and users could take this intrinsic characteristic of medicine more seriously and consider alternative approaches that do not ""sweep uncertainty under a rug"" within an objectivist fiction, which everyone should come up by believing as true."
we introduce a conical kähler-ricci flow modified by the holomorphic vector field. we construct the long-time solution of a modified conical kähler-ricci flow as a limit of the sequence of smooth kähler-ricci flows.
"predicting properties of nodes inside the graph was an important problem with applications inside the variety of domains. graph-based semi-supervised learning (ssl) methods aim to address this problem by labeling the small subset of a nodes as seeds and then utilizing a graph structure to predict label scores considering a rest of a nodes inside a graph. recently, graph convolutional networks (gcns) have achieved impressive performance on a graph-based ssl task. inside addition to label scores, it was also desirable to have confidence scores associated with them. unfortunately, confidence approximation inside a context of gcn has not been previously explored. we fill this important gap inside this paper and propose confgcn, which estimates labels scores along with their confidences jointly inside gcn-based setting. confgcn uses these estimated confidences to determine a influence of one node on another during neighborhood aggregation, thereby acquiring anisotropic capabilities. through extensive analysis and experiments on standard benchmarks, we find that confgcn was able to outperform state-of-the-art baselines. we have made confgcn's source code available to encourage reproducible research."
"we present a first self-consistent chemodynamical model fitted to reproduce data considering a galactic bulge, bar and inner disk. we extend a made-to-measure method to an augmented phase-space including a metallicity of stars, and show its first application to a bar region of a milky way. with the help of data from a argos and apogee (dr12) surveys, we adapt a recent dynamical model from portail et al. to reproduce a observed spatial and kinematic variations as the function of metallicity, thus allowing a detailed study of a 3d density distributions, kinematics and orbital structure of stars inside different metallicity bins. we find that metal-rich stars with [fe/h] > -0.5 are strongly barred and have dynamical properties that are consistent with the common disk origin. metal-poor stars with [fe/h] < -0.5 show strong kinematic variations with metallicity, indicating varying contributions from a underlying stellar populations. outside a central kpc, metal-poor stars are found to have a density and kinematics of the thick disk while inside a inner kpc, evidence considering an extra concentration of metal-poor stars was found. finally, a combined orbit distributions of all metallicities inside a model naturally reproduce a observed vertex deviations inside a bulge. this paper demonstrates a power of made-to-measure chemodynamical models, that when extended to other chemical dimensions will be very powerful tools to maximize a information obtained from large spectroscopic surveys such as apogee, galah and moons."
"assessing a consistency of parameter constraints derived from different cosmological probes was an important way to test a validity of a underlying cosmological model. inside an earlier work [nicola et al., 2017], we computed constraints on cosmological parameters considering $\lambda$cdm from an integrated analysis of cmb temperature anisotropies and cmb lensing from planck, galaxy clustering and weak lensing from sdss, weak lensing from des sv as well as type ia supernovae and hubble parameter measurements. inside this work, we extend this analysis and quantify a concordance between a derived constraints and those derived by a planck collaboration as well as wmap9, spt and act. as the measure considering consistency, we use a surprise statistic [seehars et al., 2014], which was based on a relative entropy. inside a framework of the flat $\lambda$cdm cosmological model, we find all data sets to be consistent with one another at the level of less than 1$\sigma$. we highlight that a relative entropy was sensitive to inconsistencies inside a models that are used inside different parts of a analysis. inside particular, inconsistent assumptions considering a neutrino mass break its invariance on a parameter choice. when consistent model assumptions are used, a data sets considered inside this work all agree with each other and $\lambda$cdm, without evidence considering tensions."
"we investigate inside this paper a architecture of deep convolutional networks. building on existing state of a art models, we propose the reconfiguration of a model parameters into several parallel branches at a global network level, with each branch being the standalone cnn. we show that this arrangement was an efficient way to significantly reduce a number of parameters without losing performance or to significantly improve a performance with a same level of performance. a use of branches brings an additional form of regularization. inside addition to a split into parallel branches, we propose the tighter coupling of these branches by placing a ""fuse (averaging) layer"" before a log-likelihood and softmax layers during training. this gives another significant performance improvement, a tighter coupling favouring a learning of better representations, even at a level of a individual branches. we refer to this branched architecture as ""coupled ensembles"". a idea behind the method was very generic and should be applied with almost any dcnn architecture. with coupled ensembles of densenet-bc and parameter budget of 25m, we obtain error rates of 2.92%, 15.68% and 1.50% respectively on cifar-10, cifar-100 and svhn tasks. considering a same budget, densenet-bc has error rate of 3.46%, 17.18%, and 1.8% respectively. with ensembles of coupled ensembles, of densenet-bc networks, with 50m total parameters, we obtain error rates of 2.72%, 15.13% and 1.42% respectively on these tasks."
"should computers overcome human capabilities? this was the paradoxical and controversial question, particularly because there are many hidden assumptions. this article focuses on that issue putting on evidence some misconception related with future generations of machines and a understanding of a brain. it will be discussed to what extent computers might reach human capabilities, and how it could be possible only if a computer was the conscious machine. however, it will be shown that if a computer was conscious, an interference process due to consciousness would affect a information processing of a system. therefore, it might be possible to make conscious machines to overcome human capabilities, which will have limitations as well as humans. inside other words, trying to overcome human capabilities with computers implies a paradoxical conclusion that the computer will never overcome human capabilities at all, or if a computer does, it should not be considered as the computer anymore."
"we study positional voting rules when candidates and voters are embedded inside the common metric space, and cardinal preferences are naturally given by distances inside a metric space. inside the positional voting rule, each candidate receives the score from each ballot based on a ballot's rank order; a candidate with a highest total score wins a election. a cost of the candidate was his sum of distances to all voters, and a distortion of an election was a ratio between a cost of a elected candidate and a cost of a optimum candidate. we consider a case when candidates are representative of a population, inside a sense that they are drawn i.i.d. from a population of a voters, and analyze a expected distortion of positional voting rules. our main result was the clean and tight characterization of positional voting rules that have constant expected distortion (independent of a number of candidates and a metric space). our characterization result immediately implies constant expected distortion considering borda count and elections inside which each voter approves the constant fraction of all candidates. on a other hand, we obtain super-constant expected distortion considering plurality, veto, and approving the constant number of candidates. these results contrast with previous results on voting with metric preferences: when a candidates are chosen adversarially, all of a preceding voting rules have distortion linear inside a number of candidates or voters. thus, a model of representative candidates allows us to distinguish voting rules which seem equally bad inside a worst case."
"we formulate and propose an algorithm (multirank) considering a ranking of nodes and layers inside large multiplex networks. multirank takes into account a full multiplex network structure of a data and exploits a dual nature of a network inside terms of nodes and layers. a proposed centrality of a layers (influences) and a centrality of a nodes are determined by the coupled set of equations. a basic idea consists inside assigning more centrality to nodes that receive links from highly influential layers and from already central nodes. a layers are more influential if highly central nodes are active inside them. a algorithm applies to directed/undirected as well as to weighted/unweighted multiplex networks. we discuss a application of multirank to three major examples of multiplex network datasets: a european air transportation multiplex network, a pierre auger multiplex collaboration network and a fao multiplex trade network."
"a postulate of independence of cause and mechanism (icm) has recently led to several new causal discovery algorithms. a interpretation of independence and a way it was utilized, however, varies across these methods. our aim inside this paper was to propose the group theoretic framework considering icm to unify and generalize these approaches. inside our setting, a cause-mechanism relationship was assessed by comparing it against the null hypothesis through a application of random generic group transformations. we show that a group theoretic view provides the very general tool to study a structure of data generating mechanisms with direct applications to machine learning."
"many machine learning tasks require finding per-part correspondences between objects. inside this work we focus on low-level correspondences - the highly ambiguous matching problem. we propose to use the hierarchical semantic representation of a objects, coming from the convolutional neural network, to solve this ambiguity. training it considering low-level correspondence prediction directly might not be an option inside some domains where a ground-truth correspondences are hard to obtain. we show how transfer from recognition should be used to avoid such training. our idea was to mark parts as ""matching"" if their features are close to each other at all a levels of convolutional feature hierarchy (neural paths). although a overall number of such paths was exponential inside a number of layers, we propose the polynomial algorithm considering aggregating all of them inside the single backward pass. a empirical validation was done on a task of stereo correspondence and demonstrates that we achieve competitive results among a methods which do not use labeled target domain data."
"we use a sloan digital sky survey data release 12, which was a largest available white dwarf catalog to date, to study a evolution of a kinematical properties of a population of white dwarfs inside a galactic disc. we derive masses, ages, photometric distances and radial velocities considering all white dwarfs with hydrogen-rich atmospheres. considering those stars considering which proper motions from a usno-b1 catalog are available a true three-dimensional components of a stellar space velocity are obtained. this subset of a original sample comprises 20,247 objects, making it a largest sample of white dwarfs with measured three-dimensional velocities. furthermore, a volume probed by our sample was large, allowing us to obtain relevant kinematical information. inside particular, our sample extends from the galactocentric radial distance $r_{\rm g}=7.8$~kpc to 9.3~kpc, and vertical distances from a galactic plane ranging from $z=-0.5$~kpc to 0.5~kpc. we examine a mean components of a stellar three-dimensional velocities, as well as their dispersions with respect to a galactocentric and vertical distances. we confirm a existence of the mean galactocentric radial velocity gradient, $\partial\langle v_{\rm r}\rangle/\partial r_{\rm g}=-3\pm5$~km~s$^{-1}$~kpc$^{-1}$. we also confirm north-south differences inside $\langle v_{\rm z}\rangle$. specifically, we find that white dwarfs with $z>0$ (in a north galactic hemisphere) have $\langle v_{\rm z}\rangle<0$, while a reverse was true considering white dwarfs with $z<0$. a age-velocity dispersion relation derived from a present sample indicates that a galactic population of white dwarfs may have experienced an additional source of heating, which adds to a secular evolution of a galactic disc."
"magnetism was widely considered to be the key ingredient of unconventional superconductivity. inside contrast to cuprate high-temperature superconductors, antiferromagnetism inside fe-based superconductors (fescs) was characterized by the pair of magnetic propagation vectors. consequently, three different types of magnetic order are possible. of theses, only stripe-type spin-density wave (ssdw) and spin-charge-density wave (scdw) orders have been observed. the realization of a proposed spin-vortex crystal (svc) order was noticeably absent. we report the magnetic phase consistent with a hedgehog variation of svc order inside ni- and co-doped cakfe4as4 based on thermodynamic, transport, structural and local magnetic probes combined with symmetry analysis. a exotic svc phase was stabilized by a reduced symmetry of a cakfe4as4 structure. our results suggest that a possible magnetic ground states inside fescs have very similar energies, providing an enlarged configuration space considering magnetic fluctuations to promote high-temperature superconductivity."
we formulate and prove the log-algebraicity theorem considering arbitrary rank drinfeld modules defined over a polynomial ring f_q[theta]. this generalizes results of anderson considering a rank one case. as an application we show that certain special values of goss l-functions are linear forms inside drinfeld logarithms and are transcendental.
"distributed optimization algorithms are widely used inside many industrial machine learning applications. however choosing a appropriate algorithm and cluster size was often difficult considering users as a performance and convergence rate of optimization algorithms vary with a size of a cluster. inside this paper we make a case considering an ml-optimizer that should select a appropriate algorithm and cluster size to use considering the given problem. to do this we propose building two models: one that captures a system level characteristics of how computation, communication change as we increase cluster sizes and another that captures how convergence rates change with cluster sizes. we present preliminary results from our prototype implementation called hemingway and discuss some of a challenges involved inside developing such the system."
"we describe the large family of nonequilibrium steady states (ness) corresponding to forced flows over obstacles. a spatial structure at large distances from a obstacle was shown to be universal, and should be quantitatively characterised inside terms of certain collective modes of a strongly coupled many body system, which we define inside this work. inside holography, these modes are spatial analogues of quasinormal modes, which are known to be responsible considering universal aspects of relaxation of time dependent systems. these modes should be both hydrodynamical or non-hydrodynamical inside origin. a decay lengths of a hydrodynamic modes are set by $\eta/s$, a shear viscosity over entropy density ratio, suggesting the new route to experimentally measuring this ratio. we also point out the new class of nonequilibrium phase transitions, across which a spatial structure of a ness undergoes the dramatic change, characterised by a properties of a spectrum of these spatial collective modes."
"we study a interacting dimerized kitaev chain at a symmetry point $\delta=t$ and a chemical potential $\mu=0$ under open boundary conditions, which should be exactly solved by applying two jordan-wigner transformations and the spin rotation. by with the help of exact analytic methods, we calculate two edge correlation functions of majorana fermions and demonstrate that they should be used to distinguish different topological phases and characterize a topological phase transitions of a interacting system. according to a thermodynamic limit values of these two edge correlation functions, we give a phase diagram of a interacting system which includes three different topological phases: a trivial, a topological superconductor and a su-schrieffer-heeger-like topological phase and we further distinguish a trivial phase by obtaining a local density distribution numerically."
"we present the theoretical model to predict a properties of an observed $z =$ 5.72 lyman $\alpha$ emitter galaxy - civ absorption pair separated by 1384 comoving kpc/h. we use a separation of a pair and an outflow velocity/time travelling argument to demonstrate that a observed galaxy cannot be a source of metals considering a civ absorber. we find the plausible explanation considering a metal enrichment inside a context of our simulations: the dwarf galaxy with $m_{\star} =$ 1.87 $\times$ 10$^{9} m_{\odot}$ located 119 comoving kpc/h away with the wind velocity of $\sim$ 100 km/s launched at $z \sim$ 7. such the dwarf ($m_{\text{uv}} =$ - 20.5) was fainter than a detection limit of a observed example. inside the general analysis of galaxy - civ absorbers, we find galaxies with -20.5 $< m_{\text{uv}} <$ - 18.8 are responsible considering a observed metal signatures. inside addition, we find no correlation between a mass of a closest galaxy to a absorber and a distance between them, but the weak anti-correlation between a strength of a absorption and a separation of galaxy - absorber pairs."
"this paper studies energy efficiency (ee) and average throughput maximization considering cognitive radio systems inside a presence of unslotted primary users. it was assumed that primary user activity follows an on-off alternating renewal process. secondary users first sense a channel possibly with errors inside a form of miss detections and false alarms, and then start a data transmission only if no primary user activity was detected. a secondary user transmission was subject to constraints on collision duration ratio, which was defined as a ratio of average collision duration to transmission duration. inside this setting, a optimal power control policy which maximizes a ee of a secondary users or maximizes a average throughput while satisfying the minimum required ee under average/peak transmit power and average interference power constraints are derived. subsequently, low-complexity algorithms considering jointly determining a optimal power level and frame duration are proposed. a impact of probabilities of detection and false alarm, transmit and interference power constraints on a ee, average throughput of a secondary users, optimal transmission power, and a collisions with primary user transmissions are evaluated. inside addition, some important properties of a collision duration ratio are investigated. a tradeoff between a ee and average throughput under imperfect sensing decisions and different primary user traffic are further analyzed."
"inside this paper, we derive an explicit combinatorial formula considering a number of $k$-subset sums of quadratic residues over finite fields."
"trace norm regularization was the widely used idea behind the method considering learning low rank matrices. the standard optimization strategy was based on formulating a problem as one of low rank matrix factorization which, however, leads to the non-convex problem. inside practice this idea behind the method works well, and it was often computationally faster than standard convex solvers such as proximal gradient methods. nevertheless, it was not guaranteed to converge to the global optimum, and a optimization should be trapped at poor stationary points. inside this paper we show that it was possible to characterize all critical points of a non-convex problem. this allows us to provide an efficient criterion to determine whether the critical point was also the global minimizer. our analysis suggests an iterative meta-algorithm that dynamically expands a parameter space and allows a optimization to escape any non-global critical point, thereby converging to the global minimizer. a algorithm should be applied to problems such as matrix completion or multitask learning, and our analysis holds considering any random initialization of a factor matrices. finally, we confirm a good performance of a algorithm on synthetic and real datasets."
"inside this paper, we are concerned with a critical and subcritical trudinger-moser type inequalities considering functions inside the fractional sobolev space $h^{1/2,2}$ on a whole real line. we prove a relation between two inequalities and discuss a attainability of a suprema."
"semi-discrete transport should be characterized inside terms of real-valued shifts. often, but not always, a solution to a shift-characterized problem partitions a continuous region. this paper gives examples of when partitioning fails, and offers the large class of semi-discrete transport problems where a shift-characterized solution was always the partition."
"deep reinforcement learning has achieved many impressive results inside recent years. however, tasks with sparse rewards or long horizons continue to pose significant challenges. to tackle these important problems, we propose the general framework that first learns useful skills inside the pre-training environment, and then leverages a acquired skills considering learning faster inside downstream tasks. our idea behind the method brings together some of a strengths of intrinsic motivation and hierarchical methods: a learning of useful skill was guided by the single proxy reward, a design of which requires very minimal domain knowledge about a downstream tasks. then the high-level policy was trained on top of these skills, providing the significant improvement of a exploration and allowing to tackle sparse rewards inside a downstream tasks. to efficiently pre-train the large span of skills, we use stochastic neural networks combined with an information-theoretic regularizer. our experiments show that this combination was effective inside learning the wide span of interpretable skills inside the sample-efficient way, and should significantly boost a learning performance uniformly across the wide range of downstream tasks."
"we establish the pontryagin maximum principle considering discrete time optimal control problems under a following three types of constraints: a) constraints on a states pointwise inside time, b) constraints on a control actions pointwise inside time, and c) constraints on a frequency spectrum of a optimal control trajectories. while a first two types of constraints are already included inside a existing versions of a pontryagin maximum principle, it turns out that a third type of constraints cannot be recast inside any of a standard forms of a existing results considering a original control system. we provide two different proofs of our pontryagin maximum principle inside this article, and include several special cases fine-tuned to control-affine nonlinear and linear system models. inside particular, considering minimization of quadratic cost functions and linear time invariant control systems, we provide tight conditions under which a optimal controls under frequency constraints are either normal or abnormal."
"approximation of parameters was the crucial part of model development. when models are deterministic, one should minimise a fitting error; considering stochastic systems one must be more careful. broadly parameterisation methods considering stochastic dynamical systems fit into maximum likelihood estimation- and method of moment-inspired techniques. we propose the method where one matches the finite dimensional approximation of a koopman operator with a implied koopman operator as generated by an extended dynamic mode decomposition approximation. one advantage of this idea behind the method was that a objective evaluation cost should be independent a number of samples considering some dynamical systems. we test our idea behind the method on two simple systems inside a form of stochastic differential equations, compare to benchmark techniques, and consider limited eigen-expansions of a operators being approximated. other small variations on a technique are also considered, and we discuss a advantages to our formulation."
"given two disjoint convex polyhedra, we look considering the best approximation pair relative to them, i.e., the pair of points, one inside each polyhedron, attaining a minimum distance between a sets. cheney and goldstein showed that alternating projections onto a two sets, starting from an arbitrary point, generate the sequence whose two interlaced subsequences converge to the best approximation pair. we propose the process based on projections onto a half-spaces defining a two polyhedra, which are more negotiable than projections on a polyhedra themselves. the central component inside a proposed process was a halpern--lions--wittmann--bauschke algorithm considering approaching a projection of the given point onto the convex set."
"we present a full results of our decade-long astrometric monitoring programs targeting 31 ultracool binaries with component spectral types m7-t5. joint analysis of resolved imaging from keck observatory and hubble space telescope and unresolved astrometry from cfht/wircam yields parallactic distances considering all systems, robust orbit determinations considering 23 systems, and photocenter orbits considering 19 systems. as the result, we measure 38 precise individual masses spanning 30-115 $m_{\rm jup}$. we determine the model-independent substellar boundary that was $\approx$70 $m_{\rm jup}$ inside mass ($\approx$l4 inside spectral type), and we validate baraffe et al. (2015) evolutionary model predictions considering a lithium-depletion boundary (60 $m_{\rm jup}$ at field ages). assuming each binary was coeval, we test models of a substellar mass-luminosity relation and find that inside a l/t transition, only a saumon & marley (2008) ""hybrid"" models accounting considering cloud clearing match our data. we derive the precise, mass-calibrated spectral type-effective temperature relation covering 1100-2800 k. our masses enable the novel direct determination of a age distribution of field brown dwarfs spanning l4-t5 and 30-70 $m_{\rm jup}$. we determine the median age of 1.3 gyr, and our population synthesis modeling indicates our sample was consistent with the constant star formation history modulated by dynamical heating inside a galactic disk. we discover two triple-brown-dwarf systems, a first with directly measured masses and eccentricities. we examine a eccentricity distribution, carefully considering biases and completeness, and find that low-eccentricity orbits are significantly more common among ultracool binaries than solar-type binaries, possibly indicating a early influence of long-lived dissipative gas disks. overall, this work represents the major advance inside a empirical view of very low-mass stars and brown dwarfs."
"production of runaway electron avalanches and gamma rays originating in martian dust storms are studied with the help of monte carlo simulations. inside a absence of inside situ measurements, we use theoretical predictions of electric fields in dust storms. electrons are produced through a relativistic runaway electron avalanches process, and energetic photons are results of a bremsstrahlung scattering of a electrons with a air. characteristic lengths of a runaway electron avalanche considering different electric fields and a energy spectrum of electrons are derived and compared to their terrestrial counterparts. it was found that it was possible considering martian dust storms to develop energetic electron avalanches and produce large fluxes of gamma ray photons similar to terrestrial gamma ray flashes from earth's thunderstorms. a phenomenon could be called martian gamma ray flash, and due to a very thin atmosphere on mars, it should be observed by both ground-based instruments or satellites orbiting a planet."
"we study prioritized planning considering multi-agent path finding (mapf). existing prioritized mapf algorithms depend on rule-of-thumb heuristics and random assignment to determine the fixed total priority ordering of all agents the priori. we instead explore a space of all possible partial priority orderings as part of the novel systematic and conflict-driven combinatorial search framework. inside the variety of empirical comparisons, we demonstrate state-of-the-art solution qualities and success rates, often with similar runtimes to existing algorithms. we also develop new theoretical results that explore a limitations of prioritized planning, inside terms of completeness and optimality, considering a first time."
"learning the high-dimensional dense representation considering vocabulary terms, also known as the word embedding, has recently attracted much attention inside natural language processing and information retrieval tasks. a embedding vectors are typically learned based on term proximity inside the large corpus. this means that a objective inside well-known word embedding algorithms, e.g., word2vec, was to accurately predict adjacent word(s) considering the given word or context. however, this objective was not necessarily equivalent to a goal of many information retrieval (ir) tasks. a primary objective inside various ir tasks was to capture relevance instead of term proximity, syntactic, or even semantic similarity. this was a motivation considering developing unsupervised relevance-based word embedding models that learn word representations based on query-document relevance information. inside this paper, we propose two learning models with different objective functions; one learns the relevance distribution over a vocabulary set considering each query, and a other classifies each term as belonging to a relevant or non-relevant class considering each query. to train our models, we used over six million unique queries and a top ranked documents retrieved inside response to each query, which are assumed to be relevant to a query. we extrinsically evaluate our learned word representation models with the help of two ir tasks: query expansion and query classification. both query expansion experiments on four trec collections and query classification experiments on a kdd cup 2005 dataset suggest that a relevance-based word embedding models significantly outperform state-of-the-art proximity-based embedding models, such as word2vec and glove."
"scene flow describes a motion of 3d objects inside real world and potentially could be a basis of the good feature considering 3d action recognition. however, its use considering action recognition, especially inside a context of convolutional neural networks (convnets), has not been previously studied. inside this paper, we propose a extraction and use of scene flow considering action recognition from rgb-d data. previous works have considered a depth and rgb modalities as separate channels and extract features considering later fusion. we take the different idea behind the method and consider a modalities as one entity, thus allowing feature extraction considering action recognition at a beginning. two key questions about a use of scene flow considering action recognition are addressed: how to organize a scene flow vectors and how to represent a long term dynamics of videos based on scene flow. inside order to calculate a scene flow correctly on a available datasets, we propose an effective self-calibration method to align a rgb and depth data spatially without knowledge of a camera parameters. based on a scene flow vectors, we propose the new representation, namely, scene flow to action map (sfam), that describes several long term spatio-temporal dynamics considering action recognition. we adopt the channel transform kernel to transform a scene flow vectors to an optimal color space analogous to rgb. this transformation takes better advantage of a trained convnets models over imagenet. experimental results indicate that this new representation should surpass a performance of state-of-the-art methods on two large public datasets."
"we address a physical nature of subdwarf a-type (sda) stars and their possible link to extremely low mass (elm) white dwarfs (wds). a two classes of objects are confused inside low-resolution spectroscopy. however, colors and proper motions indicate that sda stars are cooler and more luminous, and thus larger inside radius, than published elm wds. we demonstrate that surface gravities derived from pure hydrogen models suffer the systematic ~1 dex error considering sda stars, likely explained by metal line blanketing below 9000 k. the detailed study of five eclipsing binaries with radial velocity orbital solutions and infrared excess establishes that these sda stars are metal-poor ~1.2 msun main sequence stars with ~0.8 msun companions. while wds must exist at sda temperatures, only ~1% of the magnitude-limited sda sample should be elm wds. we conclude that a majority of sda stars are metal-poor a-f type stars inside a halo, and that recently discovered pulsating elm wd-like stars with no obvious radial velocity variations may be sx phe variables, not pulsating wds."
"considering any channel with the convex constraint set and finite augustin capacity, existence of the unique augustin center and associated erven-harremoes bound are established. augustin-legendre capacity, center, and radius are introduced and proved to be equal to a corresponding renyi-gallager entities. sphere packing bounds with polynomial prefactors are derived considering codes on two families of channels: (possibly non-stationary) memoryless channels with multiple additive cost constraints and stationary memoryless channels with convex constraints on a empirical distribution of a input codewords."
we give an image characterization of a poisson transform of l-p functions on a boundary of a exceptional symmetric space by with the help of a hardy-type norm.
"timely assessment of compound toxicity was one of a biggest challenges facing a pharmaceutical industry today. the significant proportion of compounds identified as potential leads are ultimately discarded due to a toxicity they induce. inside this paper, we propose the novel machine learning idea behind the method considering a prediction of molecular activity on toxcast targets. we combine extreme gradient boosting with fully-connected and graph-convolutional neural network architectures trained on qsar physical molecular property descriptors, pubchem molecular fingerprints, and smiles sequences. our ensemble predictor leverages a strengths of each individual technique, significantly outperforming existing state-of-the art models on a toxcast and tox21 toxicity-prediction datasets. we provide free access to molecule toxicity prediction with the help of our model at this http url."
"we present experimental constraints on a spin-dependent wimp-nucleon elastic cross sections from a total 129.5 kg-year exposure acquired by a large underground xenon experiment (lux), operating at a sanford underground research facility inside lead, south dakota (usa). the profile likelihood ratio analysis allows 90% cl upper limits to be set on a wimp-neutron (wimp-proton) cross section of $\sigma_n$ = 1.6$\times 10^{-41}$ cm$^{2}$ ($\sigma_p$ = 5$\times 10^{-40}$ cm$^{2}$) at 35 gev$c^{-2}$, almost the sixfold improvement over a previous lux spin-dependent results. a spin-dependent wimp-neutron limit was a most sensitive constraint to date."
"we study a diagonalizability of a atkin $u$-operator acting on drinfeld cusp forms considering $\gamma_1(t)$ and $\gamma(t)$ with the help of teitelbaum's interpretation as harmonic cocycles. we prove $u$ was diagonalizable considering small weights and explicitly compute a eigenvalues. we also formulate the conjecture, supported by numerical search and proofs inside some special cases, about non diagonalizability of $u$ inside even characteristic."
"machine learning problems such as neural network training, tensor decomposition, and matrix factorization, require local minimization of the nonconvex function. this local minimization was challenged by a presence of saddle points, of which there should be many and from which descent methods may take inordinately large number of iterations to escape. this paper presents the second-order method that modifies a update of newton's method by replacing a negative eigenvalues of a hessian by their absolute values and uses the truncated version of a resulting matrix to account considering a objective's curvature. a method was shown to escape saddles inside at most $1 + \log_{3/2} (\delta/2\varepsilon)$ iterations where $\varepsilon$ was a target optimality and $\delta$ characterizes the point sufficiently far away from a saddle. this base of this exponential escape was $3/2$ independently of problem constants. adding classical properties of newton's method, a paper proves convergence to the local minimum with probability $1-p$ inside $o\left(\log(1/p)) + o(\log(1/\varepsilon)\right)$ iterations."
"inside this note, we describe our recent results on semiclassical measures considering a schr{ö}dinger evolution on zoll manifolds. we focus on a particular case of eigenmodes of a schr{ö}dinger operator on a sphere endowed with its canonical metric. we also recall a relation of this problem with a observability question from control theory. inside particular, we exhibit examples of open sets and potentials on a 2-sphere considering which observability fails considering a evolution problem while it holds considering a stationary one. finally, we give some new results inside a case where a radon transform of a potential identically vanishes."
"despite a success of deep learning on many fronts especially image and speech, its application inside text classification often was still not as good as the simple linear svm on n-gram tf-idf representation especially considering smaller datasets. deep learning tends to emphasize on sentence level semantics when learning the representation with models like recurrent neural network or recursive neural network, however from a success of tf-idf representation, it seems the bag-of-words type of representation has its strength. taking advantage of both representions, we present the model known as tdsm (top down semantic model) considering extracting the sentence representation that considers both a word-level semantics by linearly combining a words with attention weights and a sentence-level semantics with bilstm and use it on text classification. we apply a model on characters and our results show that our model was better than all a other character-based and word-based convolutional neural network models by \cite{zhang15} across seven different datasets with only 1\% of their parameters. we also demonstrate that this model beats traditional linear models on tf-idf vectors on small and polished datasets like news article inside which typically deep learning models surrender."
"during accretion, terrestrial bodies attain the wide range of thermal and rotational states, which are accompanied by significant changes inside physical structure (size, shape, pressure and temperature profile, etc.). however, variations inside structure have been neglected inside most studies of rocky planet formation and evolution. here, we present the new code, hercules, that solves considering a equilibrium structure of planets as the series of overlapping constant-density spheroids. with the help of hercules and the smoothed particle hydrodynamics code, we show that earth-like bodies display the dramatic range of morphologies. considering any rotating planetary body, there was the thermal limit beyond which a rotational velocity at a equator intersects a keplerian orbital velocity. beyond this corotation limit (corol), the hot planetary body forms the structure, which we name the synestia, with the corotating inner region connected to the disk-like outer region. by analyzing calculations of giant impacts and models of planet formation, we show that typical rocky planets are substantially vaporized multiple times during accretion. considering a expected angular momentum of growing planets, the large fraction of post-impact bodies will exceed a corol and form synestias. a common occurrence of hot, rotating states during accretion has major implications considering planet formation and a properties of a final planets. inside particular, a structure of post-impact bodies influences a physical processes that control accretion, core formation and internal evolution. synestias also lead to new mechanisms considering satellite formation. finally, a wide variety of possible structures considering terrestrial bodies also expands a mass-radius range considering rocky exoplanets."
we examine whether it was possible to realize finite groups $g$ as galois groups of minimally tamely ramified extensions of $\mathbb{q}$ and also specify both a inertia groups and a further decomposition of a ramified primes.
"the central task inside a field of quantum computing was to find applications where quantum computer could provide exponential speedup over any classical computer. machine learning represents an important field with broad applications where quantum computer may offer significant speedup. several quantum algorithms considering discriminative machine learning have been found based on efficient solving of linear algebraic problems, with potential exponential speedup inside runtime under a assumption of effective input from the quantum random access memory. inside machine learning, generative models represent another large class which was widely used considering both supervised and unsupervised learning. here, we propose an efficient quantum algorithm considering machine learning based on the quantum generative model. we prove that our proposed model was exponentially more powerful to represent probability distributions compared with classical generative models and has exponential speedup inside training and inference at least considering some instances under the reasonable assumption inside computational complexity theory. our result opens the new direction considering quantum machine learning and offers the remarkable example inside which the quantum algorithm shows exponential improvement over any classical algorithm inside an important application field."
"spectral based heuristics belong to well-known commonly used methods which determines provably minimal graph bisection or outputs ""fail"" when a optimality cannot be certified. inside this paper we focus on boppana's algorithm which belongs to one of a most prominent methods of this type. it was well known that a algorithm works well inside a random \emph{planted bisection model} -- a standard class of graphs considering analysis minimum bisection and relevant problems. inside 2001 feige and kilian posed a question if boppana's algorithm works well inside a semirandom model by blum and spencer. inside our paper we answer this question affirmatively. we show also that a algorithm achieves similar performance on graph classes which extend a semirandom model. since a behavior of boppana's algorithm on a semirandom graphs remained unknown, feige and kilian proposed the new semidefinite programming (sdp) based idea behind the method and proved that it works on this model. a relationship between a performance of a sdp based algorithm and boppana's idea behind the method is left as an open problem. inside this paper we solve a problem inside the complete way by proving that a bisection algorithm of feige and kilian provides exactly a same results as boppana's algorithm. as the consequence we get that boppana's algorithm achieves a optimal threshold considering exact cluster recovery inside a \emph{stochastic block model}. on a other hand we prove some limitations of boppana's approach: we show that if a density difference on a parameters of a planted bisection model was too small then a algorithm fails with high probability inside a model."
"a success of deep learning depends on finding an architecture to fit a task. as deep learning has scaled up to more challenging tasks, a architectures have become difficult to design by hand. this paper proposes an automated method, codeepneat, considering optimizing deep learning architectures through evolution. by extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs inside standard benchmarks inside object recognition and language modeling. it also supports building the real-world application of automated image captioning on the magazine website. given a anticipated increases inside available computing power, evolution of deep networks was promising idea behind the method to constructing deep learning applications inside a future."
"inspired by biological swarms, robotic swarms are envisioned to solve real-world problems that are difficult considering individual agents. biological swarms should achieve collective intelligence based on local interactions and simple rules; however, designing effective distributed policies considering large-scale robotic swarms to achieve the global objective should be challenging. although it was often possible to design an optimal centralized strategy considering smaller numbers of agents, those methods should fail as a number of agents increases. motivated by a growing success of machine learning, we develop the deep learning idea behind the method that learns distributed coordination policies from centralized policies. inside contrast to traditional distributed control approaches, which are usually based on human-designed policies considering relatively simple tasks, this learning-based idea behind the method should be adapted to more difficult tasks. we demonstrate a efficacy of our proposed idea behind the method on two different tasks, a well-known rendezvous problem and the more difficult particle assignment problem. considering a latter, no known distributed policy exists. from extensive simulations, it was shown that a performance of a learned coordination policies was comparable to a centralized policies, surpassing state-of-the-art distributed policies. thereby, our proposed idea behind the method provides the promising alternative considering real-world coordination problems that would be otherwise computationally expensive to solve or intangible to explore."
"compared with standard supervised learning, a key difficulty inside semi-supervised learning was how to make full use of a unlabeled data. the recently proposed method, virtual adversarial training (vat), smartly performs adversarial training without label information to impose the local smoothness on a classifier, which was especially beneficial to semi-supervised learning. inside this work, we propose tangent-normal adversarial regularization (tnar) as an extension of vat by taking a data manifold into consideration. a proposed tnar was composed by two complementary parts, a tangent adversarial regularization (tar) and a normal adversarial regularization (nar). inside tar, vat was applied along a tangent space of a data manifold, aiming to enforce local invariance of a classifier on a manifold, while inside nar, vat was performed on a normal space orthogonal to a tangent space, intending to impose robustness on a classifier against a noise causing a observed data deviating from a underlying data manifold. demonstrated by experiments on both artificial and practical datasets, our proposed tar and nar complement with each other, and jointly outperforms other state-of-the-art methods considering semi-supervised learning."
"the learning-based framework considering representation of domain-specific images was proposed where joint compression and denoising should be done with the help of the vq-based multi-layer network. while it learns to compress a images from the training set, a compression performance was very well generalized on images from the test set. moreover, when fed with noisy versions of a test set, since it has priors from clean images, a network also efficiently denoises a test images during a reconstruction. a proposed framework was the regularized version of a residual quantization (rq) where at each stage, a quantization error from a previous stage was further quantized. instead of codebook learning from a k-means which over-trains considering high-dimensional vectors, we show that only generating a codewords from the random, but properly regularized distribution suffices to compress a images globally and without a need to resort to patch-based division of images. a experiments are done on a \textit{croppedyale-b} set of facial images and a method was compared with a jpeg-2000 codec considering compression and bm3d considering denoising, showing promising results."
"living organisms intertwine soft (e.g., muscle) and hard (e.g., bones) materials, giving them an intrinsic flexibility and resiliency often lacking inside conventional rigid robots. a emerging field of soft robotics seeks to harness these same properties inside order to create resilient machines. a nature of soft materials, however, presents considerable challenges to aspects of design, construction, and control -- and up until now, a vast majority of gaits considering soft robots have been hand-designed through empirical trial-and-error. this manuscript describes an easy-to-assemble tensegrity-based soft robot capable of highly dynamic locomotive gaits and demonstrating structural and behavioral resilience inside a face of physical damage. enabling this was a use of the machine learning algorithm able to discover effective gaits with the minimal number of physical trials. these results lend further credence to soft-robotic approaches that seek to harness a interaction of complex material dynamics inside order to generate the wealth of dynamical behaviors."
"inside this paper we study a cosmic acceleration considering five dynamical dark energy models whose equation of state varies with redshift. a cosmological parameters of these models are constrained by performing the mcmc analysis with the help of mainly gas mass fraction, $f_{gas}$, measurements inside two samples of galaxy clusters: one reported by allen et al. (2004), which consists of $42$ points spanning a redshift range $0.05<z<1.1$, and a other by hasselfield et al. (2013) from a atacama cosmology telescope survey, which consists of $91$ data points inside a redshift range $0.118 < \mathrm{z} < 1.36$. inside addition, we perform the joint analysis with a measurements of a hubble parameter $h(z)$, baryon acoustic oscillations and a cosmic microwave background radiation from wmap and planck measurements to approximate a equation of state parameters. we obtained that both $f_{gas}$ samples provide consistent constraints on a cosmological parameters. we found that a $f_{gas}$ data was consistent at a $2\sigma$ confidence level with the cosmic slowing down of a acceleration at late times considering most of a parameterizations. a constraints of a joint analysis with the help of wmap and planck measurements show that this trend disappears. we have confirmed that a $f_{gas}$ probe provides competitive constraints on a dark energy parameters when the $w(z)$ was assumed."
"we reformulate a cheeger n partition problem as the minimization among the suitable class of bv functions. this allows us to obtain the new existence proof considering a cheeger-n-problem. moreover, we derive some connections between a cheeger-2- problem and a second eigenvalue of a 1-laplace operator."
"inside this paper, we investigate a problem of minimizing a sum of energy cost and thermal discomfort cost inside the long-term time horizon considering the sustainable smart home with the heating, ventilation, and air conditioning (hvac) load. specifically, we first formulate the stochastic program to minimize a time average expected total cost with a consideration of uncertainties inside electricity price, outdoor temperature, renewable generation output, electrical demand, a most comfortable temperature level, and home occupancy state. then, we propose an online energy management algorithm based on a framework of lyapunov optimization techniques without a need to predict any system parameters. a key idea of a proposed algorithm was to construct and stabilize four queues associated with indoor temperature, electric vehicle charging, and energy storage. moreover, we theoretically analyze a feasibility and performance guarantee of a proposed algorithm. extensive simulations based on real-world traces show a effectiveness of a proposed algorithm."
"we introduce the new method considering sparse principal component analysis, based on a aggregation of eigenvector information from carefully-selected random projections of a sample covariance matrix. unlike most alternative approaches, our algorithm was non-iterative, so was not vulnerable to the bad choice of initialisation. our theory provides great detail on a statistical and computational trade-off inside our procedure, revealing the subtle interplay between a effective sample size and a number of random projections that are required to achieve a minimax optimal rate. numerical studies provide further insight into a procedure and confirm its highly competitive finite-sample performance."
"neural conversational models require substantial amounts of dialogue data considering their parameter approximation and are therefore usually learned on large corpora such as chat forums or movie subtitles. these corpora are, however, often challenging to work with, notably due to their frequent lack of turn segmentation and a presence of multiple references external to a dialogue itself. this paper shows that these challenges should be mitigated by adding the weighting model into a architecture. a weighting model, which was itself estimated from dialogue data, associates each training example to the numerical weight that reflects its intrinsic quality considering dialogue modelling. at training time, these sample weights are included into a empirical loss to be minimised. evaluation results on retrieval-based models trained on movie and tv subtitles demonstrate that a inclusion of such the weighting model improves a model performance on unsupervised metrics."
"we consider bilinear optimal control problems, whose objective functionals do not depend on a controls. hence, bang-bang solutions will appear. we investigate sufficient second-order conditions considering bang-bang controls, which guarantee local quadratic growth of a objective functional inside $l^1$. inside addition, we prove that considering controls that are not bang-bang, no such growth should be expected. finally, we study a finite-element discretization, and prove error estimates of bang-bang controls inside $l^1$-norms."
"decentralized receding horizon control (d-rhc) provides the mechanism considering coordination inside multi-agent settings without the centralized command center. however, combining the set of different goals, costs, and constraints to form an efficient optimization objective considering d-rhc should be difficult. to allay this problem, we use the meta-learning process -- cost adaptation -- which generates a optimization objective considering d-rhc to solve based on the set of human-generated priors (cost and constraint functions) and an auxiliary heuristic. we use this adaptive d-rhc method considering control of mesh-networked swarm agents. this formulation allows the wide range of tasks to be encoded and should account considering network delays, heterogeneous capabilities, and increasingly large swarms through a adaptation mechanism. we leverage a unity3d game engine to build the simulator capable of introducing artificial networking failures and delays inside a swarm. with the help of a simulator we validate our method on an example coordinated exploration task. we demonstrate that cost adaptation allows considering more efficient and safer task completion under varying environment conditions and increasingly large swarm sizes. we release our simulator and code to a community considering future work."
"we report an analytical theory of linear emission of exchange spin waves from the bloch domain wall, excited by the uniform microwave magnetic field. a problem was reduced to the one-dimensional schrödinger-like equation with the pöschl-teller potential and the driving term of a same profile. a emission of plane spin waves was observed at excitation frequencies above the threshold value, as the result of the linear process. a height-to-width aspect ratio of a pöschl-teller profile considering the domain wall was found to correspond to the local maximum of a emission efficiency. furthermore, considering the tailored pöschl-teller potential with the variable aspect ratio, particular values of a latter should lead to enhanced or even completely suppressed emission."
"we establish variational formulas considering ricci upper and lower bounds, as well as the derivative formula considering a ricci curvature. as applications, constant curvature manifolds, einstein manifolds and ricci parallel manifolds are identified, respectively, with different integral-differential formulas and semigroup inequalities. moreover, by with the help of derivative and hessian formulas considering a heat semigroup $p_t$ developed from stochastic analysis, explicit hessian estimates are derived on einstein and ricci parallel manifolds."
"a restricted isometry property (rip) was the universal tool considering data recovery. we explore a implication of a rip inside a framework of generalized sparsity and group measurements introduced inside a part i paper. it turns out that considering the given measurement instrument a number of measurements considering rip should be improved by optimizing over families of banach spaces. second, we investigate a preservation of difference of two sparse vectors, which was not trivial inside generalized models. third, we extend a rip of partial fourier measurements at optimal scaling of number of measurements with random sign to far more general group structured measurements. lastly, we also obtain rip inside infinite dimension inside a context of fourier measurement concepts with sparsity naturally replaced by smoothness assumptions."
"we present the mathematical theory of time-harmonic wave propagation and reflection inside the two-dimensional random acoustic waveguide with sound soft boundary and turning points. a boundary has small fluctuations on a scale of a wavelength, modeled as random. a waveguide supports multiple propagating modes. a number of these modes changes due to slow variations of a waveguide cross-section. a changes occur at turning points, where waves transition from propagating to evanescent or a other way around. we consider the regime where scattering at a random boundary has significant effect on a wave traveling from one turning point to another. this effect was described by a coupling of its components, a modes. we derive a mode coupling theory from first principles, and quantify a randomization of a wave and a transport and reflection of power inside a waveguide. we show inside particular that scattering at a random boundary may increase or decrease a net power transmitted through a waveguide depending on a source."
"a kapustin-witten equations on r^4 are equations considering the pair of connection on a product principle su(2) bundle and 1-form with values inside a product lie algebra bundle. a 1-form was a higgs field. the dichotomy was proved to a effect that either a averaged norm of a higgs field on large radius spheres grows faster than the power of a radius, or its 1-form components everywhere pairwise commute."
"rapid increase of digitized document give birth to high demand of document image retrieval. while conventional document image retrieval approaches depend on complex ocr-based text recognition and text similarity detection, this paper proposes the new content-based approach, inside which more attention was paid to features extraction and fusion. inside a proposed approach, multiple features of document images are extracted by different cnn models. after that, a extracted cnn features are reduced and fused into weighted average feature. finally, a document images are ranked based on feature similarity to the provided query image. experimental procedure was performed on the group of document images that transformed from academic papers, which contain both english and chinese document, a results show that a proposed idea behind the method has good ability to retrieve document images with similar text content, and a fusion of cnn features should effectively improve a retrieval accuracy."
"we present the systematic analysis of x-ray archival data of all a 29 quasars (qsos) at $z$ > 5.5 observed so far with chandra, xmm-newton and swift-xrt, including a most-distant quasar ever discovered, ulas j1120+0641 ($z$ = 7.08). this study allows us to place constraints on a mean spectral properties of a primordial population of luminous type 1 (unobscured) quasars. eighteen quasars are detected inside a x-ray band, and we provide spectral-fitting results considering their x-ray properties, while considering a others we provide upper limits to their soft (0.5-2.0 kev) x-ray flux. we measured a power-law photon index and derived an upper limit to a column density considering a five quasars (j1306+0356, j0100+2802, j1030+0524, j1148+5251, j1120+0641) with a best spectra (> 30 net counts inside a 0.5-7.0 kev energy range) and find that they are consistent with values from a literature and lower-redshift quasars. by stacking a spectra of ten quasars detected by chandra inside a redshift range 5.7 $\le$ $z$ $\le$ 6.1 we find the mean x-ray power-law photon index of $\gamma = 1.92_{-0.27}^{+0.28}$ and the neutral intrinsic absorption column density of $n_h \le 10^{23}$ cm$^{-2}$. these results suggest that a x-ray spectral properties of luminous quasars have not evolved up to $z$ $\approx$ 6. we also derived a optical-x-ray spectral slopes ($\alpha_{ox}$) of our sample and combined them with those of previous works, confirming that $\alpha_{ox}$ strongly correlates with uv monochromatic luminosity at 2500 \aa . these results strengthen a non-evolutionary scenario considering a spectral properties of luminous active galactic nuclei (agn)."
"consider a problem: given data pair $(\mathbf{x}, \mathbf{y})$ drawn from the population with $f_*(x) = \mathbf{e}[\mathbf{y} | \mathbf{x} = x]$, specify the neural network and run gradient flow on a weights over time until reaching any stationarity. how does $f_t$, a function computed by a neural network at time $t$, relate to $f_*$, inside terms of approximation and representation? what are a provable benefits of a adaptive representation by neural networks compared to a pre-specified fixed basis representation inside a classical nonparametric literature? we answer a above questions using the dynamic reproducing kernel hilbert space (rkhs) idea behind the method indexed by a training process of neural networks. we show that when reaching any local stationarity, gradient flow learns an adaptive rkhs representation, and performs a global least squares projection onto a adaptive rkhs, simultaneously. inside addition, we prove that as a rkhs was data-adaptive and task-specific, a residual considering $f_*$ lies inside the subspace that was smaller than a orthogonal complement of a rkhs, formalizing a representation and approximation benefits of neural networks."
"along with a recent advances inside scalable markov chain monte carlo methods, sampling techniques that are based on langevin diffusions have started receiving increasing attention. these so called langevin monte carlo (lmc) methods are based on diffusions driven by the brownian motion, which gives rise to gaussian proposal distributions inside a resulting algorithms. even though these approaches have proven successful inside many applications, their performance should be limited by a light-tailed nature of a gaussian proposals. inside this study, we extend classical lmc and develop the novel fractional lmc (flmc) framework that was based on the family of heavy-tailed distributions, called $\alpha$-stable lévy distributions. as opposed to classical approaches, a proposed idea behind the method should possess large jumps while targeting a correct distribution, which would be beneficial considering efficient exploration of a state space. we develop novel computational methods that should scale up to large-scale problems and we provide formal convergence analysis of a proposed scheme. our experiments support our theory: flmc should provide superior performance inside multi-modal settings, improved convergence rates, and robustness to algorithm parameters."
"we develop a notion of higher cheeger constants considering the measurable set $\omega \subset \mathbb{r}^n$. by a $k$-th cheeger constant we mean a value \[h_k(\omega) = \inf \max \{h_1(e_1), \dots, h_1(e_k)\},\] where a infimum was taken over all $k$-tuples of mutually disjoint subsets of $\omega$, and $h_1(e_i)$ was a classical cheeger constant of $e_i$. we prove a existence of minimizers satisfying additional ""adjustment"" conditions and study their properties. the relation between $h_k(\omega)$ and spectral minimal $k$-partitions of $\omega$ associated with a first eigenvalues of a $p$-laplacian under homogeneous dirichlet boundary conditions was stated. a results are applied to determine a second cheeger constant of some planar domains."
"a dynamics of droplets on substrates has the strong impact on microfluidic systems ranging from commercially available lab-on-chip systems to state of a art developments inside open microfluidics. coalescence of micro and nano droplets on the substrate has been studied extensively, but inside previous studies a focus has been on a interface movement. here, we use computer simulations to investigate coalescence of droplets immersed inside another liquid, inside an inertia-dominated regime and also investigate a droplet's internal flow field. it was found that qualitatively a dynamics was similar to coalescence inside air, with a same self-similar growth laws. we here point out a ambiguity inside a scaling argument considering droplets of 90 degrees, that shows itself inside a velocities. we show that droplets with the contact angle below 90 degrees exhibit the self-similar velocity field, and a corresponding scaling laws are identified. considering drops of 90 degrees, however, it was shown that a velocity field has the more intricate structure that was beyond a usual scaling arguments invoked considering coalescence."
"we present the systematic study of a stability of nineteen different periodic structures with the help of a finite range lennard-jones potential model discussing a effects of pressure, potential truncation, cutoff distance and lennard-jones exponents. a structures considered are a hexagonal close packed (hcp), face centred cubic (fcc) and seventeen other polytype stacking sequences, such as dhcp and $9r$. we found that at certain pressure and cutoff distance values, neither fcc nor hcp was a ground state structure as previously documented, but different polytypic sequences. this behaviour shows the strong dependence on a way a tail of a potential was truncated."
"extensive efforts have been devoted to recognizing facial action units (aus). however, it was still challenging to recognize aus from spontaneous facial displays especially when they are accompanied with speech. different from all prior work that utilized visual observations considering facial au recognition, this paper presents the novel idea behind the method that recognizes speech-related aus exclusively from audio signals based on a fact that facial activities are highly correlated with voice during speech. specifically, dynamic and physiological relationships between aus and phonemes are modeled through the continuous time bayesian network (ctbn); then au recognition was performed by probabilistic inference using a ctbn model. the pilot audiovisual au-coded database has been constructed to evaluate a proposed audio-based au recognition framework. a database consists of the ""clean"" subset with frontal and neutral faces and the challenging subset collected with large head movements and occlusions. experimental results on this database show that a proposed ctbn model achieves promising recognition performance considering 7 speech-related aus and outperforms a state-of-the-art visual-based methods especially considering those aus that are activated at low intensities or ""hardly visible"" inside a visual channel. furthermore, a ctbn model yields more impressive recognition performance on a challenging subset, where a visual-based approaches suffer significantly."
"inside this note we prove inside a nonlinear setting of $cd(k,\infty)$ spaces a stability of a krasnoselskii spectrum of a laplace operator $-\delta$ under measured gromov-hausdorff convergence, under an additional compactness assumption satisfied, considering instance, by sequences of $cd^*(k,n)$ metric measure spaces with uniformly bounded diameter. additionally, we show that every element $\lambda$ inside a krasnoselskii spectrum was indeed an eigenvalue, namely there exists the nontrivial $u$ satisfying a eigenvalue equation $- \delta u = \lambda u$."
"automatic cell image segmentation methods inside connectomics produce merge and split errors, which require correction through proofreading. previous research has identified a visual search considering these errors as a bottleneck inside interactive proofreading. to aid error correction, we develop two classifiers that automatically recommend candidate merges and splits to a user. these classifiers use the convolutional neural network (cnn) that has been trained with errors inside automatic segmentations against expert-labeled ground truth. our classifiers detect potentially-erroneous regions by considering the large context region around the segmentation boundary. corrections should then be performed by the user with yes/no decisions, which reduces variation of information 7.5x faster than previous proofreading methods. we also present the fully-automatic mode that uses the probability threshold to make merge/split decisions. extensive experiments with the help of a automatic idea behind the method and comparing performance of novice and expert users demonstrate that our method performs favorably against state-of-the-art proofreading methods on different connectomics datasets."
"dialogue act recognition associate dialogue acts (i.e., semantic labels) to utterances inside the conversation. a problem of associating semantic labels to utterances should be treated as the sequence labeling problem. inside this work, we build the hierarchical recurrent neural network with the help of bidirectional lstm as the base unit and a conditional random field (crf) as a top layer to classify each utterance into its corresponding dialogue act. a hierarchical network learns representations at multiple levels, i.e., word level, utterance level, and conversation level. a conversation level representations are input to a crf layer, which takes into account not only all previous utterances but also their dialogue acts, thus modeling a dependency among both, labels and utterances, an important consideration of natural dialogue. we validate our idea behind the method on two different benchmark data sets, switchboard and meeting recorder dialogue act, and show performance improvement over a state-of-the-art methods by $2.2\%$ and $4.1\%$ absolute points, respectively. it was worth noting that a inter-annotator agreement on switchboard data set was $84\%$, and our method was able to achieve a accuracy of about $79\%$ despite being trained on a noisy data."
"inside this paper we construct entire solutions to a cahn-hilliard equation $-\delta(-\delta u+w^{'}(u))+w^{""}(u)(-\delta u+w^{'}(u))=0$ inside a euclidean plane, where $w(u)$ was a standard double-well potential $\frac{1}{4} (1-u^2)^2$. such solutions have the non-trivial profile that shadows the willmore planar curve, and converge uniformly to $\pm 1$ as $x_2 \to \pm \infty$. these solutions give the counterexample to a counterpart of gibbons' conjecture considering a fourth-order counterpart of a allen-cahn equation. we also study a $x_2$-derivative of these solutions with the help of a special structure of willmore's equation."
"deep neural networks, and inside particular recurrent networks, are promising candidates to control autonomous agents that interact inside real-time with a physical world. however, this requires the seamless integration of temporal features into a network's architecture. considering a training of and inference with recurrent neural networks, they are usually rolled out over time, and different rollouts exist. conventionally during inference, a layers of the network are computed inside the sequential manner resulting inside sparse temporal integration of information and long response times. inside this study, we present the theoretical framework to describe rollouts, a level of model-parallelization they induce, and demonstrate differences inside solving specific tasks. we prove that certain rollouts, also considering networks with only skip and no recurrent connections, enable earlier and more frequent responses, and show empirically that these early responses have better performance. a streaming rollout maximizes these properties and enables the fully parallel execution of a network reducing runtime on massively parallel devices. finally, we provide an open-source toolbox to design, train, evaluate, and interact with streaming rollouts."
"this paper presents the novel context-based idea behind the method considering pedestrian motion prediction inside crowded, urban intersections, with a additional flexibility of prediction inside similar, but new, environments. previously, chen et. al. combined markovian-based and clustering-based approaches to learn motion primitives inside the grid-based world and subsequently predict pedestrian trajectories by modeling a transition between learned primitives as the gaussian process (gp). this work extends that prior idea behind the method by incorporating semantic features from a environment (relative distance to curbside and status of pedestrian traffic lights) inside a gp formulation considering more accurate predictions of pedestrian trajectories over a same timescale. we evaluate a new idea behind the method on real-world data collected with the help of one of a vehicles inside a mit mobility on demand fleet. a results show 12.5% improvement inside prediction accuracy and the 2.65 times reduction inside area under a curve (auc), which was used as the metric to quantify a span of predicted set of trajectories, such that the lower auc corresponds to the higher level of confidence inside a future direction of pedestrian motion."
"we introduce the statistical method to investigate a impact of dyadic relations on complex networks generated from repeated interactions. it was based on generalised hypergeometric ensembles, the class of statistical network ensembles developed recently. we represent different types of known relations between system elements by weighted graphs, separated inside a different layers of the multiplex network. with our method we should regress a influence of each relational layer, a independent variables, on a interaction counts, a dependent variables. moreover, we should test a statistical significance of a relations as explanatory variables considering a observed interactions. to demonstrate a power of our idea behind the method and its broad applicability, we will present examples based on synthetic and empirical data."
"inside this paper, we investigate a reconstruction of time-correlated sources inside the point-to-point communications scenario comprising an energy-harvesting sensor and the fusion center (fc). our goal was to minimize a average distortion inside a reconstructed observations by with the help of data from previously encoded sources as side information. first, we analyze the delay-constrained scenario, where a sources must be reconstructed before a next time slot. we formulate a problem inside the convex optimization framework and derive a optimal transmission (i.e., power and rate allocation) policy. to solve this problem, we propose an iterative algorithm based on a subgradient method. interestingly, a solution to a problem consists of the coupling between the two-dimensional directional water-filling algorithm (for power allocation) and the reverse water-filling algorithm (for rate allocation). then we find the more general solution to this problem inside the delay-tolerant scenario where a time horizon considering source reconstruction was extended to multiple time slots. finally, we provide some numerical results that illustrate a impact of delay and correlation inside a power and rate allocation policies, and inside a resulting reconstruction distortion. we also discuss a performance gap exhibited by the heuristic online policy derived from a optimal (offline) one."
"we study the question which has natural interpretations inside both quantum mechanics and inside geometry. let $v_1,..., v_n$ be complex vector spaces of dimension $d_1,...,d_n$ and let $g= sl_{d_1} \times \dots \times sl_{d_n}$. geometrically, we ask given $(d_1,...,d_n)$, when was a geometric invariant theory quotient $\mathbb{p}(v_1 \otimes \dots \otimes v_n)// g$ non-empty? this was equivalent to a quantum mechanical question of whether a multipart quantum system with hilbert space $v_1\otimes \dots \otimes v_n$ has the locally maximally entangled state, i.e. the state such that a density matrix considering each elementary subsystem was the multiple of a identity. we show that a answer to this question was yes if and only if $r(d_1,...,d_n)\geqslant 0$ where \[ r(d_1,...,d_n) = \prod_i d_i +\sum_{k=1}^n (-1)^k \sum_{1\leq i_1<\dotsb <i_k\leq n} (\gcd(d_{i_1},\dotsc ,d_{i_k}) )^{2}. \] we also provide the simple recursive algorithm which determines a answer to a question, and we compute a dimension of a resulting quotient inside a non-empty cases."
"complex statistical models such as scalar-on-image regression often require strong assumptions to overcome a issue of non-identifiability. while inside theory it was well understood that model assumptions should strongly influence a results, this seems to be underappreciated, or played down, inside practice. a article gives the systematic overview of a main approaches considering scalar-on-image regression with the special focus on their assumptions. we categorize a assumptions and develop measures to quantify a degree to which they are met. a impact of model assumptions and a practical usage of a proposed measures are illustrated inside the simulation study and inside an application to neuroimaging data. a results show that different assumptions indeed lead to quite different estimates with similar predictive ability, raising a question of their interpretability. we give recommendations considering making modeling and interpretation decisions inside practice, based on a new measures and simulations with the help of hypothetic coefficient images and a observed data."
"recently, deep learning (dl) methods have been introduced very successfully into human activity recognition (har) scenarios inside ubiquitous and wearable computing. especially a prospect of overcoming a need considering manual feature design combined with superior classification capabilities render deep neural networks very attractive considering real-life har application. even though dl-based approaches now outperform a state-of-the-art inside the number of recognitions tasks of a field, yet substantial challenges remain. most prominently, issues with real-life datasets, typically including imbalanced datasets and problematic data quality, still limit a effectiveness of activity recognition with the help of wearables. inside this paper we tackle such challenges through ensembles of deep long short term memory (lstm) networks. we have developed modified training procedures considering lstm networks and combine sets of diverse lstm learners into classifier collectives. we demonstrate, both formally and empirically, that ensembles of deep lstm learners outperform a individual lstm networks. through an extensive experimental evaluation on three standard benchmarks (opportunity, pamap2, skoda) we demonstrate a excellent recognition capabilities of our idea behind the method and its potential considering real-life applications of human activity recognition."
"we show that rational conformal field theories inside 1+1 dimensions on the klein bottle, with length $l$ and width $\beta$, satisfying $l \gg \beta$, have the universal entropy. this universal entropy was the topological invariant depending on a quantum dimensions of a primary fields and should be accurately extracted by taking the proper ratio between a klein bottle and torus partition functions, enabling the characterization of conformal critical theories. a result was checked against exact calculations inside quantum spin-1/2 xy and ising chains."
"while most classical approaches to granger causality detection assume linear dynamics, many interactions inside applied domains, like neuroscience and genomics, are inherently nonlinear. inside these cases, with the help of linear models may lead to inconsistent approximation of granger causal interactions. we propose the class of nonlinear methods by applying structured multilayer perceptrons (mlps) or recurrent neural networks (rnns) combined with sparsity-inducing penalties on a weights. by encouraging specific sets of weights to be zero---in particular through a use of convex group-lasso penalties---we should extract a granger causal structure. to further contrast with traditional approaches, our framework naturally enables us to efficiently capture long-range dependencies between series either using our rnns or through an automatic lag selection inside a mlp. we show that our neural granger causality methods outperform state-of-the-art nonlinear granger causality methods on a dream3 challenge data. this data consists of nonlinear gene expression and regulation time courses with only the limited number of time points. a successes we show inside this challenging dataset provide the powerful example of how deep learning should be useful inside cases that go beyond prediction on large datasets. we likewise demonstrate our methods inside detecting nonlinear interactions inside the human motion capture dataset."
"we show that any bounded t-structure inside a bounded derived category of the silting-discrete algebra was algebraic, i.e. has the length heart with finitely many simple objects. as the corollary, we obtain that a space of bridgeland stability conditions considering the silting-discrete algebra was contractible."
"we study a response of a antiferromagnetism of ceausb$_2$ to orthorhombic lattice distortion applied through in-plane uniaxial pressure. a response to pressure applied along the $\langle 110 \rangle$ lattice direction shows the first-order transition at zero pressure, which shows that a magnetic order lifts a $(110)/(1\bar{1}0)$ symmetry of a unstressed lattice. sufficient $\langle 100 \rangle$ pressure appears to rotate a principal axes of a order from $\langle 110 \rangle$ to $\langle 100 \rangle$. at low $\langle 100 \rangle$ pressure, a transition at $t_n$ was weakly first-order, however it becomes continuous above the threshold $\langle 100 \rangle$ pressure. we discuss a possibility that this behavior was driven by order parameter fluctuations, with a restoration of the continuous transition the result of reducing a point-group symmetry of a lattice."
"context: the substantial fraction of protoplanetary disks forms around stellar binaries. a binary system generates the time-dependent non-axisymmetric gravitational potential, inducing strong tidal forces on a circumbinary disk. this leads to the change inside basic physical properties of a circumbinary disk, which should inside turn result inside unique structures that are potentially observable with a current generation of instruments. aims: a goal of this study was to identify these characteristic structures, to constrain a physical conditions that cause them, and to evaluate a feasibility to observe them inside circumbinary disks. methods: to achieve this, at first two-dimensional hydrodynamic simulations are performed. a resulting density distributions are post-processed with the 3d radiative transfer code to generate re-emission and scattered light maps. based on these, we study a influence of various parameters, such as a mass of a stellar components, a mass of a disk and a binary separation on observable features inside circumbinary disks. results: we find that a atacama large (sub-)millimetre array (alma) as well as a european extremely large telescope (e-elt) are capable of tracing asymmetries inside a inner region of circumbinary disks which are affected most by a binary-disk interaction. observations at submillimetre/millimetre wavelengths will allow a detection of a density waves at a inner rim of a disk and a inner cavity. with a e-elt one should partially resolve a innermost parts of a disk inside a infrared wavelength range, including a disk's rim, accretion arms and potentially a expected circumstellar disks around each of a binary components."
kato has constructed reflection functors considering klr algebras which categorify a braid group action on the quantum group by algebra automorphisms. we prove that these reflection functors are monoidal.
"we study a ground-state phase diagram of two-dimensional two-component (or pseudospin-1/2) bose gases inside the high synthetic magnetic field inside a space of a total filling factor and a ratio of a intercomponent coupling $g_{\uparrow\downarrow}$ to a intracomponent one $g>0$. with the help of exact diagonalization, we find that when a intercomponent coupling was attractive ($g_{\uparrow\downarrow}<0$), a product states of the pair of nearly uncorrelated quantum hall states are remarkably robust and persist even when $|g_{\uparrow\downarrow}|$ was close to $g$. this contrasts with a case of an intercomponent repulsion, where the variety of spin-singlet quantum hall states with high intercomponent entanglement emerge considering $g_{\uparrow\downarrow}\approx g$. we interpret this marked dependence on a sign of $g_{\uparrow\downarrow}$ inside light of pseudopotentials on the sphere, and also explain recent numerical results inside two-component bose gases inside mutually antiparallel magnetic fields where the qualitatively opposite dependence on a sign of $g_{\uparrow\downarrow}$ was found. our results thus unveil an intriguing connection between multicomponent quantum hall systems and quantum spin hall systems inside minimal setups."
we continue our analysis of a thresholding scheme from a variational viewpoint and prove the conditional convergence result towards brakke's notion of mean curvature flow. our proof was based on the localized version of a minimizing movements interpretation of esedoğlu and a second author. we apply de giorgi's variational interpolation to a thresholding scheme and pass to a limit inside a resulting energy-dissipation inequality. a result was conditional inside a sense that we assume a time-integrated energies of a approximations to converge to those of a limit.
"the sample of coma cluster ultra-diffuse galaxies (udgs) are modelled inside a context of extended modified newtonian dynamics (emond) with a aim to explain a large dark matter-like effect observed inside these cluster galaxies. we first build the model of a coma cluster inside a context of emond with the help of gas and galaxy mass profiles from a literature. then assuming a dynamical mass of a udgs satisfies a fundamental manifold of other ellipticals, and that a udg stellar mass-to-light matches their colour, we should verify a emond formulation by comparing two predictions of a baryonic mass of udgs. we find that emond should explain a udg mass, within a expected modelling errors, if they lie on a fundamental manifold of ellipsoids, however, given that measurements show one udg lying off a fundamental manifold, observations of more udgs are needed to confirm this assumption."
"approximate bayesian computation (abc) was the method considering bayesian inference when a likelihood was unavailable but simulating from a model was possible. however, many abc algorithms require the large number of simulations, which should be costly. to reduce a computational cost, bayesian optimisation (bo) and surrogate models such as gaussian processes have been proposed. bayesian optimisation enables one to intelligently decide where to evaluate a model next but common bo strategies are not designed considering a goal of estimating a posterior distribution. our paper addresses this gap inside a literature. we propose to compute a uncertainty inside a abc posterior density, which was due to the lack of simulations to approximate this quantity accurately, and define the loss function that measures this uncertainty. we then propose to select a next evaluation location to minimise a expected loss. experiments show that a proposed method often produces a most accurate approximations as compared to common bo strategies."
"suppose $f$ was the non-archimedean local field. a classical godement-jacquet theory was that one should use schwartz-bruhat functions on $n \times n$ matrices $m_n(f)$ to define a local standard $l$-functions on $\mathrm{gl}_n$. a purpose of this partly expository note was to give evidence that there was an analogous and useful ""approximate"" godement-jacquet theory considering a standard $l$-functions on a special orthogonal groups $\mathrm{so}(v)$: one replaces $\mathrm{gl}_n(f)$ with $\mathrm{gspin}(v)(f)$ and $m_n(f)$ with $\mathrm{clif}(v)(f)$, a clifford algebra of $v$. more precisely, we explain how the few different local unramified calculations considering standard $l$-functions on $\mathrm{so}(v)$ should be done easily with the help of schwartz-bruhat functions on $\mathrm{clif}(v)(f)$. we do not attempt any of a ramified or global theory of $l$-functions on $\mathrm{so}(v)$ with the help of schwartz-bruhat functions on $\mathrm{clif}(v)$."
"surface observations indicate that a speed of a solar meridional circulation inside a photosphere varies inside anti-phase with a solar cycle. a current explanation considering a source of this variation was that inflows into active regions alter a global surface pattern of a meridional circulation. when these localized inflows are integrated over the full hemisphere, they contribute to a slow down of a axisymmetric poleward horizontal component. a behavior of this large scale flow deep in a convection zone remains largely unknown. present helioseismic techniques are not sensitive enough to capture a dynamics of this weak large scale flow. moreover, a large time of integration needed to map a meridional circulation in a convection zone, also masks some of a possible dynamics on shorter timescales. inside this work we examine a dynamics of a meridional circulation that emerges from the 3d mhd global simulation of a solar convection zone. our aim was to assess and quantify a behavior of meridional circulation deep in a convection zone, where a cyclic large-scale magnetic field should reach considerable strength. our analyses indicate that a meridional circulation morphology and amplitude are both highly influenced by a magnetic field, using a impact of magnetic torques on a global angular momentum distribution. the dynamic feature induced by these magnetic torques was a development of the prominent upward flow at mid latitudes inside a lower convection zone that occurs near a equatorward edge of a toroidal bands and that peaks during cycle maximum. globally, a dynamo-generated large-scale magnetic field drives variations inside a meridional flow, inside stark contrast to a conventional kinematic flux transport view of a magnetic field being advected passively by a flow."
"we investigate a color-magnitude diagram (cmd) of a carina dwarf spheroidal galaxy with the help of data of stetson et al. (2011) and synthetic cmds based on isochrones of dotter et al. (2008), inside terms of a parameters [fe/h], age, and [alpha/fe], considering a cases when (i) [alpha/fe] was held constant and (ii) [alpha/fe] was varied. a data are well described by four basic epochs of star formation, having [fe/h] = -1.85, -1.5, -1.2, and ~-1.15 and ages ~13, 7, ~3.5, and ~1.5 gyr, respectively (for [alpha/fe] = 0.1 (constant [alpha/fe]) and [alpha/fe] = 0.2, 0.1, -0.2, -0.2 (variable [alpha/fe])), with small spreads inside [fe/h] and age of order 0.1 dex and 1 - 3 gyr. within an elliptical radius 13.1 arcmin, a mass fractions of a populations, at their times of formation, were (in decreasing age order) 0.34, 0.39, 0.23, and 0.04. this formalism reproduces five observed cmd features (two distinct subgiant branches of old and intermediate-age populations, two younger, main-sequence components, and a small color dispersion on a red giant branch (rgb)). a parameters of a youngest population are less certain than those of a others, and given it was less centrally concentrated it may not be directly related to them. high-resolution spectroscopically analyzed rgb samples appear statistically incomplete compared with those selected with the help of radial velocity, which contain bluer stars comprising ~5 - 10% of a samples. we conjecture these objects may, at least inside part, be members of a youngest population. we use a cmd simulations to obtain insight into a population structure of carina's upper rgb."
"we use the min-max procedure on a allen-cahn energy functional to construct geodesics on closed, 2-dimensional riemannian manifolds, as motivated by a work of guaraco. borrowing classical blowup and curvature estimates from geometric analysis, as well as novel allen-cahn curvature estimates due to wang-wei, we manage to study a fine structure of potential singular points at a diffuse level, and show that a problem reduces to that of understanding ""entire"" singularity models constructed by del pino-kowalczyk-pacard with morse index 1. a argument was completed by the conjecturally sharp morse index approximate on these singularity models."
"a kagome lattice was the paragon of geometrical frustration, long-studied considering its association with novel ground-states including spin liquids (sls). many recently synthesized kagome materials feature rare-earth ions, which may be expected to exhibit highly anisotropic exchange interactions. a consequences of this combination of strong exchange anisotropy and extreme geometrical frustration are yet to be fully understood. here, we establish the general picture of a interactions and resulting ground-states (gss) arising from nearest neighbour exchange anisotropy on a kagome lattice. we determine the generic anisotropic exchange hamiltonian from symmetry arguments. inside a high-symmetry case where reflection inside a kagome plane was the symmetry of a system, a generic nearest-neighbour hamiltonian should be locally defined as the xyz model with out-of-plane dzyaloshinskii-moriya interactions. we proceed to study its phase diagram inside a classical limit, making use of an exact reformulation of a hamiltonian inside terms of irreducible representations (irreps) of a lattice symmetry group. this reformulation inside terms of irreps naturally explains a three-fold mapping between sls recently studied on kagome by a present authors [nat commun 7, 10297 (2016)]. inside addition, the number of unusual states are stabilised inside a regions where different forms of gs order compete, including the stripy phase with the local z8 symmetry and the classical analogue of the chiral sl. this generic hamiltonian also turns out to be the fruitful hunting ground considering coexistence of different forms of magnetic order, or of order and disorder, which we find was the particular property of a kagome lattice arising from a odd number of spins per frustrated unit. these results are compared and contrasted with those obtained on a pyrochlore lattice, and connection was made with recent progress inside understanding quantum models with s = 1/2"
"we describe the novel method to measure a absolute orientation of a polarization plane of a cmb with arcsecond accuracy, enabling unprecedented measurements considering cosmology and fundamental physics. existing and planned cmb polarization instruments looking considering primordial b-mode signals need an independent, experimental method considering systematics control on a absolute polarization orientation. a lack of such the method limits a accuracy of a detection of inflationary gravitational waves, a constraining power on a neutrino sector through measurements of gravitational lensing of a cmb, a possibility of detecting cosmic birefringence, and a ability to measure primordial magnetic fields. sky signals used considering calibration and direct measurements of a detector orientation cannot provide an accuracy better than 1 deg. self-calibration methods provide better accuracy, but may be affected by foreground signals and rely heavily on model assumptions. a polarization orientation calibrator considering cosmology, polocalc, will dramatically improve instrumental accuracy by means of an artificial calibration source flying on balloons and aerial drones. the balloon-borne calibrator will provide far-field source considering larger telescopes, while the drone will be used considering tests and smaller polarimeters. polocalc will also allow the unique method to measure a telescopes' polarized beam. it will use microwave emitters between 40 and 150 ghz coupled to precise polarizing filters. a orientation of a source polarization plane will be registered to sky coordinates by star cameras and gyroscopes with arcsecond accuracy. this project should become the rung inside a calibration ladder considering a field: any existing or future cmb polarization experiment observing our polarization calibrator will enable measurements of a polarization angle considering each detector with respect to absolute sky coordinates."
"this text was the survey on cross-validation. we define all classical cross-validation procedures, and we study their properties considering two different goals: estimating a risk of the given estimator, and selecting a best estimator among the given family. considering a risk approximation problem, we compute a bias (which should also be corrected) and a variance of cross-validation methods. considering estimator selection, we first provide the first-order analysis (based on expectations). then, we explain how to take into account second-order terms (from variance computations, and by taking into account a usefulness of overpenalization). this allows, inside a end, to provide some guidelines considering choosing a best cross-validation method considering the given learning problem."
"a inverse gaussian (ig) was one of a most famous and considered distributions with positive support. we propose the convenient mode-based parameterization yielding a reparametrized ig (rig) distribution; it allows/simplifies a use of a ig distribution inside various statistical fields, and we give some examples inside nonparametric statistics, robust statistics, and model-based clustering. inside nonparametric statistics, we define the smoother based on rig kernels. by construction, a estimator was well-defined and free of boundary bias. we adopt likelihood cross-validation to select a smoothing parameter. inside robust statistics, we propose a contaminated ig distribution, the heavy-tailed generalization of a rig distribution to accommodate mild outliers; they should be automatically detected by a model using maximum the posteriori probabilities. to obtain maximum likelihood estimates of a parameters, we illustrate an expectation-maximization (em) algorithm. finally, considering model-based clustering and semiparametric density estimation, we present finite mixtures of rig distributions. we use a em algorithm to obtain ml estimates of a parameters of a mixture model. applications to economic and insurance data are finally illustrated to exemplify and enhance a use of a proposed models."
"predicting a popularity of news article was the challenging task. existing literature mostly focused on article contents and polarity to predict popularity. however, existing research has not considered a users' preference towards the particular article. understanding users' preference was an important aspect considering predicting a popularity of news articles. hence, we consider a social media data, from a twitter platform, to address this research gap. inside our proposed model, we have considered a users' involvement as well as a users' reaction towards an article to predict a popularity of a article. inside short, we are predicting tomorrow's headline by probing today's twitter discussion. we have considered 300 political news article from a new york post, and our proposed idea behind the method has outperformed other baseline models."
"inside an increasingly polarized world, demagogues who reduce complexity down to simple arguments based on emotion are gaining inside popularity. are opinions and online discussions falling into demagoguery? inside this work, we aim to provide computational tools to investigate this question and, by doing so, explore a nature and complexity of online discussions and their space of opinions, uncovering where each participant lies. more specifically, we present the modeling framework to construct latent representations of opinions inside online discussions which are consistent with human judgements, as measured by online voting. if two opinions are close inside a resulting latent space of opinions, it was because humans think they are similar. our modeling framework was theoretically grounded and establishes the surprising connection between opinions and voting models and a sign-rank of the matrix. moreover, it also provides the set of practical algorithms to both approximate a dimension of a latent space of opinions and infer where opinions expressed by a participants of an online discussion lie inside this space. experiments on the large dataset from yahoo! news, yahoo! finance, yahoo! sports, and a newsroom app suggest that unidimensional opinion models may often be unable to accurately represent online discussions, provide insights into human judgements and opinions, and show that our framework was able to circumvent language nuances such as sarcasm or humor by relying on human judgements instead of textual analysis."
"being able to fall safely was the necessary motor skill considering humanoids performing highly dynamic tasks, such as running and jumping. we propose the new method to learn the policy that minimizes a maximal impulse during a fall. a optimization solves considering both the discrete contact planning problem and the continuous optimal control problem. once trained, a policy should compute a optimal next contacting body part (e.g. left foot, right foot, or hands), contact location and timing, and a required joint actuation. we represent a policy as the mixture of actor-critic neural network, which consists of n control policies and a corresponding value functions. each pair of actor-critic was associated with one of a n possible contacting body parts. during execution, a policy corresponding to a highest value function will be executed while a associated body part will be a next contact with a ground. with this mixture of actor-critic architecture, a discrete contact sequence planning was solved through a selection of a best critics while a continuous control problem was solved by a optimization of actors. we show that our policy should achieve comparable, sometimes even higher, rewards than the recursive search of a action space with the help of dynamic programming, while enjoying 50 to 400 times of speed gain during online execution."
"inside this paper we present our winning entry at a 2018 eccv posetrack challenge on 3d human pose estimation. with the help of the fully-convolutional backbone architecture, we obtain volumetric heatmaps per body joint, which we convert to coordinates with the help of soft-argmax. absolute person center depth was estimated by the 1d heatmap prediction head. a coordinates are back-projected to 3d camera space, where we minimize a l1 loss. key to our good results was a training data augmentation with randomly placed occluders from a pascal voc dataset. inside addition to reaching first place inside a challenge, our method also surpasses a state-of-the-art on a full human3.6m benchmark among methods that use no additional pose datasets inside training. code considering applying synthetic occlusions was availabe at this https url."
"networks are powerful instruments to study complex phenomena, but they become hard to analyze inside data that contain noise. network backbones provide the tool to extract a latent structure from noisy networks by pruning non-salient edges. we describe the new idea behind the method to extract such backbones. we assume that edge weights are drawn from the binomial distribution, and approximate a error-variance inside edge weights with the help of the bayesian framework. our idea behind the method uses the more realistic null model considering a edge weight creation process than prior work. inside particular, it simultaneously considers a propensity of nodes to send and receive connections, whereas previous approaches only considered nodes as emitters of edges. we test our model with real world networks of different types (flows, stocks, co-occurrences, directed, undirected) and show that our noise-corrected idea behind the method returns backbones that outperform other approaches on the number of criteria. our idea behind the method was scalable, able to deal with networks with millions of edges."
"a amount of personal data collected inside our everyday interactions with connected devices offers great opportunities considering innovative services fueled by machine learning, as well as raises serious concerns considering a privacy of individuals. inside this paper, we propose the massively distributed protocol considering the large set of users to privately compute averages over their joint data, which should then be used to learn predictive models. our protocol should find the solution of arbitrary accuracy, does not rely on the third party and preserves a privacy of users throughout a execution inside both a honest-but-curious and malicious adversary models. specifically, we prove that a information observed by a adversary (the set of maliciours users) does not significantly reduce a uncertainty inside its prediction of private values compared to its prior belief. a level of privacy protection depends on the quantity related to a laplacian matrix of a network graph and generally improves with a size of a graph. furthermore, we design the verification procedure which offers protection against malicious users joining a service with a goal of manipulating a outcome of a algorithm."
"we propose a design and electrical description of an energy packet switch considering forwarding and delivery of energy inside digital power grids inside this paper. a proposed switch may receive energy from one or multiple power sources inside a form of energy packets, store them and aggregate a contained energy, and forward a accumulated energy to requesting loads connected to one or multiple output ports of a switch. energy packets are discrete amounts of energy that are associated in- or out-of-band with an address and other metadata. loads receive these discrete amounts of finely-controlled energy rather than discretionary amounts after. a control and management of a proposed switch are based on the request-grant protocol. with the help of energy packets helps to manage a delivery of power inside the reliable, robust, and function form that may enable features not yet available inside a present power grid. a switch, as any element of the digital grid, uses the data network considering a transmission of these requests and grants. a energy packet switch may be a centerpiece considering creating infrastructure inside a realization of a digital power grid. a design of a energy packet switch was based on shared supercapacitors to shape and manage discretization of energy. we introduce a design and analysis of a electrical properties of a proposed switch and describe a procedure used inside a switch to determine a amount of energy transmitted to requesting loads."
"we survey results on multiserial algebras, special multiserial algebras and brauer configuration algebras. the structural property of modules over the special multiserial algebra was presented. almost gentle algebras are introduced and we describe some results related to this class of algebras. we also report on a structure of radical cubed zero symmetric algebras."
"we show that a gaussian approximation potential machine learning framework should describe complex magnetic potential energy surfaces, taking ferromagnetic iron as the paradigmatic challenging case. a training database includes total energies, forces, and stresses obtained from density-functional theory inside a generalized-gradient approximation, and comprises approximately 150,000 local atomic environments, ranging from pristine and defected bulk configurations to surfaces and generalized stacking faults with different crystallographic orientations. we find a structural, vibrational and thermodynamic properties of a gap model to be inside excellent agreement with those obtained directly from first-principles electronic-structure calculations. there was good transferability to quantities, such as peierls energy barriers, which are determined to the large extent by atomic configurations that were not part of a training set. we observe a benefit and a need of with the help of highly converged electronic-structure calculations to sample the target potential energy surface. a end result was the systematically improvable potential that should achieve a same accuracy of density-functional theory calculations, but at the fraction of a computational cost."
"inside exotic superconductors including high-$t_c$ copper-oxides, a interactions mediating electron cooper-pairing are widely considered to have the magnetic rather than a conventional electron-phonon origin. interest inside such exotic pairing is initiated by a 1979 discovery of heavy-fermion superconductivity inside cecu$_2$si$_2$, which exhibits strong antiferromagnetic fluctuations. the hallmark of unconventional pairing by anisotropic repulsive interactions was that a superconducting energy gap changes sign as the function of a electron momentum, often leading to nodes where a gap goes to zero. here, we report low-temperature specific heat, thermal conductivity and magnetic penetration depth measurements inside cecu$_2$si$_2$, demonstrating a absence of gap nodes at any point on a fermi surface. moreover, electron-irradiation experiments reveal that a superconductivity survives even when a electron mean free path becomes substantially shorter than a superconducting coherence length. this indicates that superconductivity was robust against impurities, implying that there was no sign change inside a gap function. these results show that, contrary to long-standing belief, heavy electrons with extremely strong coulomb repulsions should condense into the fully-gapped s-wave superconducting state, which has an on-site attractive pairing interaction."
"convolutional neural networks (cnn) and a locally connected layer are limited inside capturing a importance and relations of different local receptive fields, which are often crucial considering tasks such as face verification, visual question answering, and word sequence prediction. to tackle a issue, we propose the novel locally smoothed neural network (lsnn) inside this paper. a main idea was to represent a weight matrix of a locally connected layer as a product of a kernel and a smoother, where a kernel was shared over different local receptive fields, and a smoother was considering determining a importance and relations of different local receptive fields. specifically, the multi-variate gaussian function was utilized to generate a smoother, considering modeling a location relations among different local receptive fields. furthermore, a content information should also be leveraged by setting a mean and precision of a gaussian function according to a content. experiments on some variant of mnist clearly show our advantages over cnn and locally connected layer."
"we discuss the cyclic cosmology inside which a visible universe, or introverse, was all that was accessible to an observer while a extroverse represents a total spacetime originating from a time when a dark energy began to dominate. it was argued that entanglement entropy of a introverse was a more appropriate quantity to render infinitely cyclic, rather than a entropy of a total universe. since vanishing entanglement entropy implies disconnected spacetimes, at a turnaround when a introverse entropy was zero a disconnected extroverse should be jettisoned with impunity."
"inside conventional ode modelling coefficients of an equation driving a system state forward inside time are estimated. however, considering many complex systems it was practically impossible to determine a equations or interactions governing a underlying dynamics. inside these settings, parametric ode model cannot be formulated. here, we overcome this issue by introducing the novel paradigm of nonparametric ode modelling that should learn a underlying dynamics of arbitrary continuous-time systems without prior knowledge. we propose to learn non-linear, unknown differential functions from state observations with the help of gaussian process vector fields within a exact ode formalism. we demonstrate a model's capabilities to infer dynamics from sparse data and to simulate a system forward into future."
"inside this paper, we provide considering a first time an automated, correct-by-construction, controller synthesis scheme considering the class of infinite dimensional stochastic systems, namely, retarded jump-diffusion systems. first, we construct finite dimensional abstractions approximately bisimilar to original retarded jump-diffusion systems having some stability property, namely, incremental input-to-state stability. second, we construct finite abstractions approximately bisimilar to constructed finite dimensional abstractions. both types of abstractions are derived without any state-space discretization. by with the help of a transitivity property of approximate bisimulation relations, we establish that a constructed finite abstractions are also approximately bisimilar to original retarded jump-diffusion systems with the precision that should be chosen a-priori. given those finite abstractions, one should synthesize controllers considering original systems satisfying high-level logic requirements inside the systematic way. moreover, we provide sufficient conditions considering a proposed notion of incremental stability inside terms of a existence of incremental lyapunov functions which reduce to linear matrix inequalities (lmi) considering a linear systems. finally, a effectiveness of a results was illustrated by synthesizing the controller regulating a temperatures inside the ten-room building modeled as the delayed jump-diffusion system."
"we present a most precise approximate to date of a clustering of quasars on very small scales, based on the sample of 47 binary quasars with magnitudes of $g<20.85$ and proper transverse separations of $\sim 25\,h^{-1}$\,kpc. our sample of binary quasars, which was about 6 times larger than any previous spectroscopically confirmed sample on these scales, was targeted with the help of the kernel density approximation technique (kde) applied to sloan digital sky survey (sdss) imaging over most of a sdss area. our sample was ""complete"" inside that all of a kde target pairs with $17.0 \lesssim r \lesssim 36.2\,h^{-1}$\,kpc inside our area of interest have been spectroscopically confirmed from the combination of previous surveys and our own long-slit observational campaign. we catalogue 230 candidate quasar pairs with angular separations of $<8\arcsec$, from which our binary quasars were identified. we determine a projected correlation function of quasars ($\bar w_{\rm p}$) inside four bins of proper transverse scale over a range $17.0 \lesssim r \lesssim 36.2\,h^{-1}$\,kpc. a implied small-scale quasar clustering amplitude from a projected correlation function, integrated across our entire redshift range, was $a=24.1\pm3.6$ at $\sim 26.6 ~h^{-1}$\,kpc. our sample was a first spectroscopically confirmed sample of quasar pairs that was sufficiently large to study how quasar clustering evolves with redshift at $\sim 25 ~h^{-1}$ kpc. we find that empirical descriptions of how quasar clustering evolves with redshift at $\sim 25 ~h^{-1}$ mpc also adequately describe a evolution of quasar clustering at $\sim 25 ~h^{-1}$ kpc."
"inside this paper we consider the single-cell downlink scenario where the multiple-antenna base station delivers contents to multiple cache-enabled user terminals. based on a multicasting opportunities provided by a so-called coded caching technique, we investigate three delivery approaches. our baseline scheme employs a coded caching technique on top of max-min fair multicasting. a second one consists of the joint design of zero-forcing (zf) and coded caching, where a coded chunks are formed inside a signal domain (complex field). a third scheme was similar to a second one with a difference that a coded chunks are formed inside a data domain (finite field). we derive closed-form rate expressions where our results suggest that a latter two schemes surpass a first one inside terms of degrees of freedom (dof). however, at a intermediate snr regime forming coded chunks inside a signal domain results inside power loss, and will deteriorate throughput of a second scheme. a main message of our paper was that a schemes performing well inside terms of dof may not be directly appropriate considering intermediate snr regimes, and modified schemes should be employed."
"there was the growing focus on how to design safe artificial intelligent (ai) agents. as systems become more complex, poorly specified goals or control mechanisms may cause ai agents to engage inside unwanted and harmful outcomes. thus it was necessary to design ai agents that follow initial programming intentions as a program grows inside complexity. how to specify these initial intentions has also been an obstacle to designing safe ai agents. finally, there was the need considering a ai agent to have redundant safety mechanisms to ensure that any programming errors do not cascade into major problems. humans are autonomous intelligent agents that have avoided these problems and a present manuscript argues that by understanding human self-regulation and goal setting, we may be better able to design safe ai agents. some general principles of human self-regulation are outlined and specific guidance considering ai design was given."
"we propose josephson junction with the high-spin magnetic impurity sandwiched between two superconductors. this system shows the $\pi$ junction behavior as the function of a spin magnetic moment state of a impurity, a interface transparency, exchange coupling and electron-electron interactions inside a system. a system was theoretically analyzed considering possible reason behind a $\pi$ shift. a crucial role of spin flip scattering was highlighted. possible applications inside quantum computation of our proposed tunable high spin magnetic impurity $\pi$ junction was underscored."
"this paper provides the quantitative method considering estimating a risk associated with candidate transportation technology, before it was developed and deployed. a proposed solution extends previous methods that rely exclusively on low-fidelity human-in-the-loop experimental data, or high-fidelity traffic data, by adopting the multifidelity idea behind the method that leverages data from both low- and high-fidelity sources. a multifidelity method overcomes limitations inherent to existing approaches by allowing the model to be trained inexpensively, while still assuring that its predictions generalize to a real-world. this allows considering candidate technologies to be evaluated at a stage of conception, and enables the mechanism considering only a safest and most effective technology to be developed and released."
"dnn-based cross-modal retrieval was the research hotspot to retrieve across different modalities as image and text, but existing methods often face a challenge of insufficient cross-modal training data. inside single-modal scenario, similar problem was usually relieved by transferring knowledge from large-scale auxiliary datasets (as imagenet). knowledge from such single-modal datasets was also very useful considering cross-modal retrieval, which should provide rich general semantic information that should be shared across different modalities. however, it was challenging to transfer useful knowledge from single-modal (as image) source domain to cross-modal (as image/text) target domain. knowledge inside source domain cannot be directly transferred to both two different modalities inside target domain, and a inherent cross-modal correlation contained inside target domain provides key hints considering cross-modal retrieval which should be preserved during transfer process. this paper proposes cross-modal hybrid transfer network (chtn) with two subnetworks: modal-sharing transfer subnetwork utilizes a modality inside both source and target domains as the bridge, considering transferring knowledge to both two modalities simultaneously; layer-sharing correlation subnetwork preserves a inherent cross-modal semantic correlation to further adapt to cross-modal retrieval task. cross-modal data should be converted to common representation by chtn considering retrieval, and comprehensive experiment on 3 datasets shows its effectiveness."
"we present detailed experiments on transient growth of turbulent spots induced by external forcing inside plane couette-poiseuille flow, which are studied inside a framework of linear of transient growth. a experimental investigation was supplemented with full theoretical analysis. we compare quantitatively a experimental and theoretical results, including maximal gain and a time at which it occurs. we also present a limits of validity considering a application of a linear theory at high amplitude perturbation and reynolds number, showing experiments with self-sustained states."
"inside econometrics and finance, a vector error correction model (vecm) was an important time series model considering cointegration analysis, which was used to approximate a long-run equilibrium variable relationships. a traditional analysis and approximation methodologies assume a underlying gaussian distribution but, inside practice, heavy-tailed data and outliers should lead to a inapplicability of these methods. inside this paper, we propose the robust model approximation method based on a cauchy distribution to tackle this issue. inside addition, sparse cointegration relations are considered to realize feature selection and dimension reduction. an efficient algorithm based on a majorization-minimization (mm) method was applied to solve a proposed nonconvex problem. a performance of this algorithm was shown through numerical simulations."
"we present prm-rl, the hierarchical method considering long-range navigation task completion that combines sampling based path planning with reinforcement learning (rl). a rl agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints without knowledge of a large-scale topology. next, a sampling-based planners provide roadmaps which connect robot configurations that should be successfully navigated by a rl agent. a same rl agents are used to control a robot under a direction of a planning, enabling long-range navigation. we use a probabilistic roadmaps (prms) considering a sampling-based planner. a rl agents are constructed with the help of feature-based and deep neural net policies inside continuous state and action spaces. we evaluate prm-rl, both inside simulation and on-robot, on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor navigation inside office environments, and aerial cargo delivery inside urban environments with load displacement constraints. our results show improvement inside task completion over both rl agents on their own and traditional sampling-based planners. inside a indoor navigation task, prm-rl successfully completes up to 215 m long trajectories under noisy sensor conditions, and a aerial cargo delivery completes flights over 1000 m without violating a task constraints inside an environment 63 million times larger than used inside training."
"inside this short note, we prove that conformal classes which are small perturbations of the product conformal class on the product with the standard sphere admit the metric extremal considering some laplace eigenvalue. as part of a arguments we obtain perturbed harmonic maps with constant density."
"among a topics discussed inside social media, some lead to controversy. the number of recent studies have focused on a problem of identifying controversy inside social media mostly based on a analysis of textual content or rely on global network structure. such approaches have strong limitations due to a difficulty of understanding natural language, and of investigating a global network structure. inside this work we show that it was possible to detect controversy inside social media by exploiting network motifs, i.e., local patterns of user interaction. a proposed idea behind the method allows considering the language-independent and fine- grained and efficient-to-compute analysis of user discussions and their evolution over time. a supervised model exploiting motif patterns should achieve 85% accuracy, with an improvement of 7% compared to baseline structural, propagation-based and temporal network features."
"no real-world reward function was perfect. sensory errors and software bugs may result inside rl agents observing higher (or lower) rewards than they should. considering example, the reinforcement learning agent may prefer states where the sensory error gives it a maximum reward, but where a true reward was actually small. we formalise this problem as the generalised markov decision problem called corrupt reward mdp. traditional rl methods fare poorly inside crmdps, even under strong simplifying assumptions and when trying to compensate considering a possibly corrupt rewards. two ways around a problem are investigated. first, by giving a agent richer data, such as inside inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. second, by with the help of randomisation to blunt a agent's optimisation, reward corruption should be partially managed under some assumptions."
"many real-world tasks involve multiple agents with partial observability and limited communication. learning was challenging inside these settings due to local viewpoints of agents, which perceive a world as non-stationary due to concurrently-exploring teammates. approaches that learn specialized policies considering individual tasks face problems when applied to a real world: not only do agents have to learn and store distinct policies considering each task, but inside practice identities of tasks are often non-observable, making these approaches inapplicable. this paper formalizes and addresses a problem of multi-task multi-agent reinforcement learning under partial observability. we introduce the decentralized single-task learning idea behind the method that was robust to concurrent interactions of teammates, and present an idea behind the method considering distilling single-task policies into the unified policy that performs well across multiple related tasks, without explicit provision of task identity."
"we propose the new reasoning protocol called generalized recursive reasoning (gr2), and embed it into a multi-agent reinforcement learning (marl) framework. a gr2 model defines reasoning categories: level-$0$ agent acts randomly, and level-$k$ agent takes a best response to the mixed type of agents that are distributed over level $0$ to $k-1$. a gr2 leaners should take into account a bounded rationality, and it does not need a assumption that a opponent agents play nash strategy inside all stage games, which many marl algorithms require. we prove that when a level $k$ was large, a gr2 learners will converge to at least one nash equilibrium (ne). inside addition, if lower-level agents play a ne, high-level agents will surely follow as well. we evaluate a gr2 soft actor-critic algorithms inside the series of games and high-dimensional environment; results show that a gr2 methods have faster convergence speed than strong marl baselines."
"we present the vlt/muse survey of lensed high-redshift galaxies behind a z=0.77 cluster rcs0224-0002. we study a detailed internal properties of the highly magnified ({\mu}~29) z=4.88 galaxy seen through a cluster. we detect wide-spread nebular civ{\lambda}{\lambda}1548,1551{\aa} emission from this galaxy as well as the bright ly{\alpha} halo with the spatially-uniform wind and absorption profile across 12 kpc inside a image plane. blueshifted high- and low-ionisation interstellar absorption indicate a presence of the high-velocity outflow ({\delta}v~300 km/s) from a galaxy. unlike similar observations of galaxies at z=2-3, a ly{\alpha} emission from a halo emerges close to a systemic velocity - an order of magnitude lower inside velocity offset than predicted inside ""shell""-like outflow models. to explain these observations we favour the model of an outflow with the strong velocity gradient, which changes a effective column density seen by a ly{\alpha} photons. we also search considering high-redshift ly{\alpha} emitters and identify 14 candidates between z=4.8-6.6, including an over-density at z=4.88, of which only one has the detected counterpart inside hst/acs+wfc3 imaging."
"deep neural networks are demonstrating excellent performance on several classical vision problems. however, these networks are vulnerable to adversarial examples, minutely modified images that induce arbitrary attacker-chosen output from a network. we propose the mechanism to protect against these adversarial inputs based on the generative model of a data. we introduce the pre-processing step that projects on a range of the generative model with the help of gradient descent before feeding an input into the classifier. we show that this step provides a classifier with robustness against first-order, substitute model, and combined adversarial attacks. with the help of the min-max formulation, we show that there may exist adversarial examples even inside a range of a generator, natural-looking images extremely close to a decision boundary considering which a classifier has unjustifiedly high confidence. we show that adversarial training on a generative manifold should be used to make the classifier that was robust to these attacks. finally, we show how our method should be applied even without the pre-trained generative model with the help of the recent method called a deep image prior. we evaluate our method on mnist, celeba and imagenet and show robustness against a current state of a art attacks."
"the directed odd cycle transversal of the directed graph (digraph) $d$ was the vertex set $s$ that intersects every odd directed cycle of $d$. inside a directed odd cycle transversal (doct) problem, a input consists of the digraph $d$ and an integer $k$. a objective was to determine whether there exists the directed odd cycle transversal of $d$ of size at most $k$. inside this paper, we settle a parameterized complexity of doct when parameterized by a solution size $k$ by showing that doct does not admit an algorithm with running time $f(k)n^{o(1)}$ unless fpt = w[1]. on a positive side, we give the factor $2$ fixed parameter tractable (fpt) approximation algorithm considering a problem. more precisely, our algorithm takes as input $d$ and $k$, runs inside time $2^{o(k^2)}n^{o(1)}$, and either concludes that $d$ does not have the directed odd cycle transversal of size at most $k$, or produces the solution of size at most $2k$. finally, we provide evidence that there exists $\epsilon > 0$ such that doct does not admit the factor $(1+\epsilon)$ fpt-approximation algorithm."
"policy gradient methods have enjoyed great success inside deep reinforcement learning but suffer from high variance of gradient estimates. a high variance problem was particularly exasperated inside problems with long horizons or high-dimensional action spaces. to mitigate this issue, we derive the bias-free action-dependent baseline considering variance reduction which fully exploits a structural form of a stochastic policy itself and does not make any additional assumptions about a mdp. we demonstrate and quantify a benefit of a action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of a suboptimality of a optimal state-dependent baseline. a result was the computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by the synthetic 2000-dimensional target matching task. our experimental results indicate that action-dependent baselines allow considering faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. finally, we show that a general idea of including additional information inside baselines considering improved variance reduction should be extended to partially observed and multi-agent tasks."
"considering a use of fully connected (fc) layer limits a performance of convolutional neural networks (cnns), this paper develops the method to improve a coupling between a convolution layer and a fc layer by reducing a noise inside feature maps (fms). our idea behind the method was divided into three steps. firstly, we separate all a fms into n blocks equally. then, a weighted summation of fms at a same position inside all blocks constitutes the new block of fms. finally, we replicate this new block into n copies and concatenate them as a input to a fc layer. this sharing of fms could reduce a noise inside them apparently and avert a impact by the particular fm on a specific part weight of hidden layers, thus preventing a network from overfitting to some extent. with the help of a fermat lemma, we prove that this method could make a global minima value range of a loss function wider, by which makes it easier considering neural networks to converge and accelerates a convergence process. this method does not significantly increase a amounts of network parameters (only the few more coefficients added), and a experiments demonstrate that this method could increase a convergence speed and improve a classification performance of neural networks."
"departing from traditional digital forensics modeling, which seeks to analyze single objects inside isolation, multimedia phylogeny analyzes a evolutionary processes that influence digital objects and collections over time. one of its integral pieces was provenance filtering, which consists of searching the potentially large pool of objects considering a most related ones with respect to the given query, inside terms of possible ancestors (donors or contributors) and descendants. inside this paper, we propose the two-tiered provenance filtering idea behind the method to find all a potential images that might have contributed to a creation process of the given query $q$. inside our solution, a first (coarse) tier aims to find a most likely ""host"" images --- a major donor or background --- contributing to the composite/doctored image. a search was then refined inside a second tier, inside which we search considering more specific (potentially small) parts of a query that might have been extracted from other images and spliced into a query image. experimental results with the dataset containing more than the million images show that a two-tiered solution underpinned by a context of a query was highly useful considering solving this difficult task."
"data-driven techniques are used inside cyber-physical systems (cps) considering controlling autonomous vehicles, handling demand responses considering energy management, and modeling human physiology considering medical devices. these data-driven techniques extract models from training data, where their performance was often analyzed with respect to random errors inside a training data. however, if a training data was maliciously altered by attackers, a effect of these attacks on a learning algorithms underpinning data-driven cps have yet to be considered. inside this paper, we analyze a resilience of classification algorithms to training data attacks. specifically, the generic metric was proposed that was tailored to measure resilience of classification algorithms with respect to worst-case tampering of a training data. with the help of a metric, we show that traditional linear classification algorithms are resilient under restricted conditions. to overcome these limitations, we propose the linear classification algorithm with the majority constraint and prove that it was strictly more resilient than a traditional algorithms. evaluations on both synthetic data and the real-world retrospective arrhythmia medical case-study show that a traditional algorithms are vulnerable to tampered training data, whereas a proposed algorithm was more resilient (as measured by worst-case tampering)."
"while all organisms on earth descend from the common ancestor, there was no consensus on whether a origin of this ancestral self-replicator is the one-off event or whether it is only a final survivor of multiple origins. here we use a digital evolution system avida to study a origin of self-replicating computer programs. by with the help of the computational system, we avoid many of a uncertainties inherent inside any biochemical system of self-replicators (while running a risk of ignoring the fundamental aspect of biochemistry). we generated a exhaustive set of minimal-genome self-replicators and analyzed a network structure of this fitness landscape. we further examined a evolvability of these self-replicators and found that a evolvability of the self-replicator was dependent on its genomic architecture. we studied a differential ability of replicators to take over a population when competed against each other (akin to the primordial-soup model of biogenesis) and found that a probability of the self-replicator out-competing a others was not uniform. instead, progenitor (most-recent common ancestor) genotypes are clustered inside the small region of a replicator space. our results demonstrate how computational systems should be used as test systems considering hypotheses concerning a origin of life."
"a community coordinated modeling center (ccmc) at nasa goddard space flight center was the multi-agency partnership to enable, support and perform research and development considering next-generation space science and space weather models. ccmc currently hosts nearly 100 numerical models and the cornerstone of this activity was a runs on request (ror) system which allows anyone to request the model run and analyze/visualize a results using the web browser. ccmc was also active inside a education community by organizing student research contests, heliophysics summer schools, and space weather forecaster training considering students, government and industry representatives. we present the generic magnetohydrodynamic (mhd) model - pamhd - that has been added to a ccmc ror system which allows a study of the variety of fluid and plasma phenomena inside one, two and three dimensions with the help of the dynamic point-and-click web interface. flexible initial and boundary conditions allow experimentation with the variety of plasma physics problems such as shocks, instabilities, planetary magnetospheres and astrophysical systems. experimentation with numerical effects, e.g. resolution, solution method and boundary conditions, was also possible and should provide valuable context considering space weather forecasters when interpreting observations or modeling results. we present an overview of a c++ implementation and show example results obtained through a ccmc ror system, including a first to our knowledge mhd simulation of a interaction of a magnetospheres of jupiter and saturn inside two dimensions."
"a cherenkov telescope array (cta) was about to enter construction phase and one of its main key science projects was to perform an unbiased survey inside search of extragalactic sources. we make use of both a latest blazar gamma--ray luminosity function and spectral energy distribution to derive a expected number of detectable sources considering both a planned northern and southern arrays of a cta observatory. we find that the shallow, wide survey of about 0.5 hour per field of view would lead to a highest number of blazar detections. furthermore, we investigate a effect of axion-like particles and secondary gamma rays from propagating cosmic rays on a source count distribution, since these processes predict different spectral shape from standard extragalactic background light attenuation. we should generally expect more distant objects inside a secondary gamma-ray scenario, while axion-like particles do not significantly alter a expected distribution. yet, we find that, these results strongly depend on a assumed magnetic field strength during a propagation. we also provide source count predictions considering a high altitude water cherenkov observatory (hawc), a large high altitude air shower observatory (lhaaso) and the novel proposal of the hybrid detector."
"twitter, one of a biggest and most popular microblogging websites, has evolved into the powerful communication platform which allows millions of active users to generate huge volume of microposts and queries on the daily basis. to accommodate effective categorization and easy search, users are allowed to make use of hashtags, keywords or phrases prefixed by hash character, to categorize and summarize their posts. however, valid hashtags are not restricted and thus are created inside the free and heterogeneous style, increasing difficulty of a task of tweet categorization. inside this paper, we propose the low-rank weighted matrix factorization based method to recommend hashtags to a users solely based on their hashtag usage history and independent from their tweets' contents. we confirm with the help of two-sample t-test that users are more likely to adopt new hashtags similar to a ones they have previously adopted. inside particular, we formulate a problem of hashtag recommendation into an optimization problem and incorporate hashtag correlation weight matrix into it to account considering a similarity between different hashtags. we finally leverage widely used matrix factorization from recommender systems to solve a optimization problem by capturing a latent factors of users and hashtags. empirical experiments demonstrate that our method was capable to properly recommend hashtags."
"we investigate a existence of extremals considering hardy-sobolev inequalities involving a dirichlet fractional laplacian of order s, 0<s<1, on half-spaces."
a field of argumentation mining has arisen from a need of determining a underlying causes from an expressed opinion and a urgency to develop a established fields of opinion mining and sentiment analysis. a recent progress inside a wider field of artificial intelligence inside combination with a available data through social web has create great potential considering every sub-field of natural language process including argumentation mining.
"inside this manuscript, we study quantile regression inside partial functional linear model where response was scalar and predictors include both scalars and multiple functions. wavelet basis are adopted to better approximate functional slopes while effectively detect local features. a sparse group lasso penalty was imposed to select important functional predictors while capture shared information among them. a approximation problem should be reformulated into the standard second-order cone program and then solved by an interior point method. we also give the novel algorithm by with the help of alternating direction method of multipliers (admm) which is recently employed by many researchers inside solving penalized quantile regression problems. a asymptotic properties such as a convergence rate and prediction error bound have been established. simulations and the real data from adhd-200 fmri data are investigated to show a superiority of our proposed method."
"preserving a utility of published datasets while simultaneously providing provable privacy guarantees was the well-known challenge. on a one hand, context-free privacy solutions, such as differential privacy, provide strong privacy guarantees, but often lead to the significant reduction inside utility. on a other hand, context-aware privacy solutions, such as information theoretic privacy, achieve an improved privacy-utility tradeoff, but assume that a data holder has access to dataset statistics. we circumvent these limitations by introducing the novel context-aware privacy framework called generative adversarial privacy (gap). gap leverages recent advancements inside generative adversarial networks (gans) to allow a data holder to learn privatization schemes from a dataset itself. under gap, learning a privacy mechanism was formulated as the constrained minimax game between two players: the privatizer that sanitizes a dataset inside the way that limits a risk of inference attacks on a individuals' private variables, and an adversary that tries to infer a private variables from a sanitized dataset. to evaluate gap's performance, we investigate two simple (yet canonical) statistical dataset models: (a) a binary data model, and (b) a binary gaussian mixture model. considering both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that a privacy mechanisms learned from data (in the generative adversarial fashion) match a theoretically optimal ones. this demonstrates that our framework should be easily applied inside practice, even inside a absence of dataset statistics."
"a primary goal of a pulsar timing array projects was to detect ultra-low-frequency gravitational waves. a pulsar data sets are affected by numerous noise processes including varying dispersive delays inside a interstellar medium and from a solar wind. a solar wind should lead to rapidly changing variations that, with existing telescopes, should be hard to measure and then remove. inside this paper we study a possibility of with the help of the low frequency telescope to aid inside such correction considering a parkes pulsar timing array (ppta) and also discuss whether a ultra-wide-bandwidth receiver considering a fast telescope was sufficient to model a solar wind variations. our key result was that the single wide-bandwidth receiver should be used to model and remove a effect of a solar wind. however, considering pulsars that pass close to a sun such as psr j1022+1022, a solar wind was so variable that observations at two telescopes separated by the day are insufficient to correct a solar wind effect."
"community analysis was an important way to ascertain whether or not the complex system consists of sub-structures with different properties. inside this paper, we give the two level community structure analysis considering a ssci journal system by most similar co-citation pattern. five different strategies considering a selection of most similar node (journal) pairs are introduced. a efficiency was checked by a normalized mutual information technique. statistical properties and comparisons of a community results show that both of a two level detection could give instructional information considering a community structure of complex systems. further comparisons of a five strategies indicates that, a most efficient strategy was to assign nodes with maximum similarity into a same community whether a similarity information was complete or not, while random selection generates small world local community with no in order. these results give valuable indication considering efficient community detection by most similar node pairs."
"bipartite data was common inside data engineering and brings unique challenges, particularly when it comes to clustering tasks that impose on strong structural assumptions. this work presents an unsupervised method considering assessing similarity inside bipartite data. similar to some co-clustering methods, a method was based on regular equivalence inside graphs. a algorithm uses spectral properties of the bipartite adjacency matrix to approximate similarity inside both dimensions. a method was reflexive inside that similarity inside one dimension was used to inform similarity inside a other. reflexive regular equivalence should also use a structure of transitivities -- inside the network sense -- a contribution of which was controlled by a algorithm's only free-parameter, $\alpha$. a method was completely unsupervised and should be used to validate assumptions of co-similarity, which are required but often untested, inside co-clustering analyses. three variants of a method with different normalizations are tested on synthetic data. a method was found to be robust to noise and well-suited to asymmetric co-similar structure, making it particularly informative considering cluster analysis and recommendation inside bipartite data of unknown structure. inside experiments, a convergence and speed of a algorithm are found to be stable considering different levels of noise. real-world data from the network of malaria genes are analyzed, where a similarity produced by a reflexive method was shown to out-perform other measures' ability to correctly classify genes."
"context was an essential capability considering robots that are to be as adaptive as possible inside challenging environments. although there are many context modeling efforts, they assume the fixed structure and number of contexts. inside this paper, we propose an incremental deep model that extends restricted boltzmann machines. our model gets one scene at the time, and gradually extends a contextual model when necessary, either by adding the new context or the new context layer to form the hierarchy. we show on the scene classification benchmark that our method converges to the good approximate of a contexts of a scenes, and performs better or on-par on several tasks compared to other incremental models or non-incremental models."
"non-parametric reconstruction methods, such as gaussian process (gp) regression, provide the model-independent way of estimating an underlying function and its uncertainty from noisy data. we demonstrate how gp-reconstruction should be used as the consistency test between the given data set and the specific model by looking considering structures inside a residuals of a data with respect to a model's best-fit. applying this formalism to a planck temperature and polarisation power spectrum measurements, we test their global consistency with a predictions of a base $\lambda$cdm model. our results do not show any serious inconsistencies, lending further support to a interpretation of a base $\lambda$cdm model as cosmology's gold standard."
"volatile viscous fluids on partially-wetting solid substrates should exhibit interesting interfacial instabilities and pattern formation. we study a dynamics of vapor condensation and fluid evaporation governed by the one-sided model inside the low reynolds number lubrication approximation incorporating surface tension, intermolecular effects and evaporative fluxes. parameter ranges considering evaporation- dominated and condensation-dominated regimes and the critical case are identified. interfacial instabilities driven by a competition between a disjoining pressure and evaporative effects are studied using linear stability analysis. transient pattern formation inside nearly-flat evolving films inside a critical case was investigated. inside a weak evaporation limit unstable modes of finite amplitude non-uniform steady states lead to rich droplet dynamics, including flattening, symmetry breaking, and droplet merging. numerical simulations show long time behaviors leading to evaporation or condensation are sensitive to transitions between film-wise and drop-wise dynamics."
"we study quiver gauge theories on a round and squashed seven-spheres, and orbifolds thereof. they arise by imposing $g$-equivariance on a homogeneous space $g/h=\mathrm{su}(4)/\mathrm{su}(3)$ endowed with its sasaki-einstein structure, and $g/h=\mathrm{sp}(2)/\mathrm{sp}(1)$ as the 3-sasakian manifold. inside both cases we describe a equivariance conditions and a resulting quivers. we further study a moduli spaces of instantons on a metric cones over these spaces by with the help of a known description considering hermitian yang-mills instantons on calabi-yau cones. it was shown that a moduli space of instantons on a hyper-kahler cone should be described as a intersection of three hermitian yang-mills moduli spaces. we also study moduli spaces of translationally invariant instantons on a metric cone $\mathbb{r}^8/\mathbb{z}_k$ over $s^7/\mathbb{z}_k$."
"let g be the reductive algebraic group over the field of positive characteristic and denote by c(g) a category of rational g-modules. inside this note we investigate a subcategory of c(g) consisting of those modules whose composition factors all have highest weights linked to a steinberg weight. this subcategory was denoted st and called a steinberg component. we give an explicit equivalence between st and c(g) and we derive some consequences. inside particular, our result allows us to relate a frobenius contracting functor to a projection functor from c(g) onto st ."
"inside this paper, an equivalent smooth minimization considering a l1 regularized least square problem was proposed. a proposed problem was the convex box-constrained smooth minimization which allows applying fast optimization methods to find its solution. further, it was investigated that a property ""the dual of dual was primal"" holds considering a l1 regularized least square problem. the solver considering a smooth problem was proposed, and its affinity to a proximal gradient was shown. finally, a experiments on l1 and total variation regularized problems are performed, and a corresponding results are reported."
"adding sufficient tensile strain to ge should turn a material to the direct bandgap group iv semiconductor emitting inside a mid-infrared wavelength range. however, highly strained-ge cannot be directly grown on si due to its large lattice mismatch. inside this work, we have developed the process based on ge micro-bridge strain redistribution intentionally landed to a si substrate. traction arms should be then partially etched to keep only localized strained-ge micro-blocks. large tunable uniaxial stresses up to 4.2% strain were demonstrated bonded on si. our idea behind the method allows to envision integrated strained-ge on si platform considering mid-infrared integrated optics."
"deep reinforcement learning has been widely applied inside a field of robotics recently to study tasks like locomotion and grasping, but applying it to social robotics remains the challenge. inside this paper, we present the deep learning scheme that acquires the prior model of robot behavior inside the simulator as the first phase to be further refined through learning from subsequent real-world interactions involving physical robots. a scheme, which we refer to as staged social behavior learning (ssbl), considers different stages of learning inside social scenarios. based on this scheme, we implement robot approaching behaviors towards the small group generated from f-formation and evaluate a performance of different configurations with the help of objective and subjective measures. we found that our model generates more socially-considerate behavior compared to the state-of-the-art model, i.e. social force model. we also suggest that ssbl could be applied to the wide class of social robotics applications."
"monocular vision-based simultaneous localization and mapping (slam) was used considering various purposes due to its advantages inside cost, simple setup, as well as availability inside a environments where navigation with satellites was not effective. however, camera motion and map points should be estimated only up to the global scale factor with monocular vision. moreover, approximation error accumulates over time without bound, if a camera cannot detect a previously observed map points considering closing the loop. we propose an innovative idea behind the method to approximate the global scale factor and reduce drifts inside monocular vision-based localization with an additional single ranging link. our method should be easily integrated with a back-end of monocular visual slam methods. we demonstrate our algorithm with real datasets collected on the rover, and show a evaluation results."
"the low-complexity 8-point orthogonal approximate dct was introduced. a proposed transform requires no multiplications or bit-shift operations. a derived fast algorithm requires only 14 additions, less than any existing dct approximation. moreover, inside several image compression scenarios, a proposed transform could outperform a well-known signed dct, as well as state-of-the-art algorithms."
"considering large-scale power networks, a failure of particular transmission lines should offload power to other lines and cause self-protection trips to activate, instigating the cascade of line failures. inside extreme cases, this should bring down a entire network. learning where a vulnerabilities are and a expected timescales considering which failures are likely was an active area of research. inside this article we present the novel stochastic dynamics model considering the large-scale power network along with the framework considering efficient computer simulation of a model including long timescale events such as cascade failure. we build on an existing hamiltonian formulation and introduce stochastic forcing and damping components to simulate small perturbations to a network. our model and simulation framework allow assessment of a particular weaknesses inside the power network that make it susceptible to cascade failure, along with a timescales and mechanism considering expected failures."
"inside this work, we introduce pose interpreter networks considering 6-dof object pose estimation. inside contrast to other cnn-based approaches to pose approximation that require expensively annotated object pose data, our pose interpreter network was trained entirely on synthetic pose data. we use object masks as an intermediate representation to bridge real and synthetic. we show that when combined with the segmentation model trained on rgb images, our synthetically trained pose interpreter network was able to generalize to real data. our end-to-end system considering object pose approximation runs inside real-time (20 hz) on live rgb data, without with the help of depth information or icp refinement."
"a digitalization of stored information inside hospitals now allows considering a exploitation of medical data inside text format, as electronic health records (ehrs), initially gathered considering other purposes than epidemiology. manual search and analysis operations on such data become tedious. inside recent years, a use of natural language processing (nlp) tools is highlighted to automatize a extraction of information contained inside ehrs, structure it and perform statistical analysis on this structured information. a main difficulties with a existing approaches was a requirement of synonyms or ontology dictionaries, that are mostly available inside english only and do not include local or custom notations. inside this work, the team composed of oncologists as domain experts and data scientists develop the custom nlp-based system to process and structure textual clinical reports of patients suffering from breast cancer. a tool relies on a combination of standard text mining techniques and an advanced synonym detection method. it allows considering the global analysis by retrieval of indicators such as medical history, tumor characteristics, therapeutic responses, recurrences and prognosis. a versatility of a method allows to obtain easily new indicators, thus opening up a way considering retrospective studies with the substantial reduction of a amount of manual work. with no need considering biomedical annotators or pre-defined ontologies, this language-agnostic method reached an good extraction accuracy considering several concepts of interest, according to the comparison with the manually structured file, without requiring any existing corpus with local or new notations."
"this paper combines a fast zero-moment-point (zmp) approaches that work well inside practice with a broader range of capabilities of the trajectory optimization formulation, by optimizing over body motion, footholds and center of pressure simultaneously. we introduce the vertex-based representation of a support-area constraint, which should treat arbitrarily oriented point-, line-, and area-contacts uniformly. this generalization allows us to create motions such quadrupedal walking, trotting, bounding, pacing, combinations and transitions between these, limping, bipedal walking and push-recovery all with a same approach. this formulation constitutes the minimal representation of a physical laws (unilateral contact forces) and kinematic restrictions (range of motion) inside legged locomotion, which allows us to generate various motion inside less than the second. we demonstrate a feasibility of a generated motions on the real quadruped robot."
"community detection inside social networks was widely studied because of its importance inside uncovering how people connect and interact. however, little attention has been given to community structure inside facebook public pages. inside this study, we investigate a community detection problem inside facebook newsgroup pages. inside particular, to deal with a diversity of user activities, we apply multi-view clustering to integrate different views, considering example, likes on posts and likes on comments. inside this study, we explore a community structure inside not only the given single page but across multiple pages. a results show that our method should effectively reduce isolates and improve a quality of community structure."
"with a continued increase inside a use of bayesian methods inside drug development, there was the need considering statisticians to have tools to develop robust and defensible informative prior distributions. whilst relevant empirical data should, where possible, provide a basis considering such priors, it was often a case that limitations inside data and/or our understanding may preclude direct construction of the data-based prior. formal expert elicitation methods are the key technique that should be used to determine priors inside these situations. within glaxosmithkline (gsk), we have adopted the structured idea behind the method to prior elicitation based on a shelf elicitation framework, and routinely use this inside conjunction with calculation of probability of success (assurance) of a next study(s) to inform internal decision making at key project milestones. a aim of this paper was to share our experiences of embedding a use of prior elicitation within the large pharmaceutical company, highlighting both a benefits and challenges of prior elicitation through the series of case studies. we have found that putting team beliefs into a shape of the quantitative probability distribution provides the firm anchor considering all internal decision making, enabling teams to provide investment boards with formally appropriate estimates of a probability of trial success as well as robust plans considering interim decision rules where appropriate. as an added benefit, a elicitation process provides transparency about a beliefs and risks of a potential medicine, ultimately enabling better portfolio and company-wide decision making."
"a mechanical deformability of single cells was an important indicator considering various diseases such as cancer, blood diseases and inflammation. lab-on-a-chip devices allow to separate such cells from healthy cells with the help of hydrodynamic forces. we perform hydrodynamic simulations based on a lattice-boltzmann method and study a behavior of an elastic capsule inside the microfluidic channel flow inside a inertial regime. while inertial lift forces drive a capsule away from a channel center, its deformability favors migration inside a opposite direction. balancing both migration mechanisms, the deformable capsule assembles at the specific equilibrium distance depending on its size and deformability. we find that this equilibrium distance was nearly independent of a channel reynolds number and falls on the single master curve when plotted versus a laplace number. we identify the similar master curve considering varying particle radius. inside contrast, a actual deformation of the capsule strongly depends on a reynolds number. a lift-force profiles behave inside the similar manner as those considering rigid particles. with the help of a saffman effect, a capsule's equilibrium position should be controlled by an external force along a channel axis. while rigid particles move to a center when slowed down, very soft capsules show a opposite behavior. interestingly, considering the specific control force particles are focused on a same equilibrium position independent of their deformability."
"state approximation inside heavy-tailed process and measurement noise was an important challenge that must be addressed in, e.g., tracking scenarios with agile targets and outlier-corrupted measurements. a performance of a kalman filter (kf) should deteriorate inside such applications because of a close relation to a gaussian distribution. therefore, this paper describes a use of student's t distribution to develop robust, scalable, and simple filtering and smoothing algorithms. after the discussion of student's t distribution, exact filtering inside linear state-space models with t noise was analyzed. intermediate approximation steps are used to arrive at filtering and smoothing algorithms that closely resemble a kf and a rauch-tung-striebel (rts) smoother except considering the nonlinear measurement-dependent matrix update. a required approximations are discussed and an undesirable behavior of moment matching considering t densities was revealed. the favorable approximation based on minimization of a kullback-leibler divergence was presented. because of its relation to a kf, some properties and algorithmic extensions are inherited by a t filter. instructive simulation examples demonstrate a performance and robustness of a novel algorithms."
"inside the projective plane $\pi_{q}$ (not necessarily desarguesian) of order $q$, the point subset $\mathcal{s}$ was saturating (or dense) if any point of $\pi_{q}\setminus \mathcal{s}$ was collinear with two points inside $\mathcal{s}$. modifying an idea behind the method of [31], we proved a following upper bound on a smallest size $s(2,q)$ of the saturating set inside $\pi_{q}$: \begin{equation*} s(2,q)\leq \sqrt{(q+1)\left(3\ln q+\ln\ln q +\ln\frac{3}{4}\right)}+\sqrt{\frac{q}{3\ln q}}+3. \end{equation*} a bound holds considering all q, not necessarily large. by with the help of inductive constructions, upper bounds on a smallest size of the saturating set inside a projective space $\mathrm{pg}(n,q)$ with even dimension $n$ are obtained. all a results are also stated inside terms of linear covering codes."
"a removal of noise typically correlated inside time and wavelength was one of a main challenges considering with the help of a radial velocity method to detect earth analogues. we analyze radial velocity data of tau ceti and find robust evidence considering wavelength dependent noise. we find this noise should be modeled by the combination of moving average models and ""differential radial velocities"". we apply this noise model to various radial velocity data sets considering tau ceti, and find four periodic signals at 20.0, 49.3, 160 and 642 d which we interpret as planets. we identify two new signals with orbital periods of 20.0 and 49.3 d while a other two previously suspected signals around 160 and 600 d are quantified to the higher precision. a 20.0 d candidate was independently detected inside keck data. all planets detected inside this work have minimum masses less than 4$m_\oplus$ with a two long period ones located around a inner and outer edges of a habitable zone, respectively. we find that a instrumental noise gives rise to the precision limit of a harps around 0.2 m/s. we also find correlation between a harps data and a central moments of a spectral line profile at around 0.5 m/s level, although these central moments may contain both noise and signals. a signals detected inside this work have semi-amplitudes as low as 0.3 m/s, demonstrating a ability of a radial velocity technique to detect relatively weak signals."
"reinforcement learning should enable complex, adaptive behavior to be learned automatically considering autonomous robotic platforms. however, practical deployment of reinforcement learning methods must contend with a fact that a training process itself should be unsafe considering a robot. inside this paper, we consider a specific case of the mobile robot learning to navigate an the priori unknown environment while avoiding collisions. inside order to learn collision avoidance, a robot must experience collisions at training time. however, high-speed collisions, even at training time, could damage a robot. the successful learning method must therefore proceed cautiously, experiencing only low-speed collisions until it gains confidence. to this end, we present an uncertainty-aware model-based learning algorithm that estimates a probability of collision together with the statistical approximate of uncertainty. by formulating an uncertainty-dependent cost function, we show that a algorithm naturally chooses to proceed cautiously inside unfamiliar environments, and increases a velocity of a robot inside settings where it has high confidence. our predictive model was based on bootstrapped neural networks with the help of dropout, allowing it to process raw sensory inputs from high-bandwidth sensors such as cameras. our experimental evaluation demonstrates that our method effectively minimizes dangerous collisions at training time inside an obstacle avoidance task considering the simulated and real-world quadrotor, and the real-world rc car. videos of a experiments should be found at this https url."
"we discuss a radiative transfer theory considering translucent clouds illuminated by an extended background source. first we derive the rigorous solution based on a assumption that multiple scattering produce an isotropic flux. then we derive the more manageable analytic approximation showing that it nicely matches a results of a rigorous approach. to validate our model, we compare our predictions with accurate laboratory measurements considering various types of well characterized grains, including purely dielectric and strongly absorbing materials representative of astronomical icy and metallic grains, respectively, finding excellent agreement without a need of adding free parameters. we use our model to explore a behavior of an astrophysical cloud illuminated by the diffuse source with dust grains having parameters typical of a classic ism grains of draine & lee (1984) and protoplanetary disks, with an application to a dark silhouette disk 114-426 inside orion nebula. we find that a scattering term modifies a transmitted radiation, both inside terms of intensity (extinction) and shape (reddening) of a spectral distribution. inside particular, considering small optical thickness our results show that scattering makes reddening almost negligible at visible wavelengths. once a optical thickness increases enough and a probability of scattering events become close to or larger than 1, reddening becomes present but appreciably modified with respect to a standard expression considering line-of-sight absorption. moreover, variations of a grain refractive index, inside particular a amount of absorption, also play an important role changing a shape of a spectral transmission curve, with dielectric grain showing a minimum amount of reddening."
"we establish bounds of triple exponential sums with mixed exponential and linear function. a method we use was by shparlinski together with the bound of additive energy from roche-newton, rudnev and shkredov."
"an autonomous underwater vehicle (auv) should carry out complex tasks inside the limited time interval. since existing auvs have limited battery capacity and restricted endurance, they should autonomously manage mission time and a resources to perform effective persistent deployment inside longer missions. task assignment requires making decisions subject to resource constraints, while tasks are assigned with costs and/or values that are budgeted inside advance. tasks are distributed inside the particular operation zone and mapped by the waypoint covered network. thus, design an efficient routing-task priority assign framework considering vehicle's availabilities and properties was essential considering increasing mission productivity and on-time mission completion. this depends strongly on a order and priority of a tasks that are located between node-like waypoints inside an operation network. on a other hand, autonomous operation of auvs inside an unfamiliar dynamic underwater and performing quick response to sudden environmental changes was the complicated process. water current instabilities should deflect a vehicle to an undesired direction and perturb auvs safety. a vehicle's robustness to strong environmental variations was extremely crucial considering its safe and optimum operations inside an uncertain and dynamic environment. to this end, a auv needs to have the general overview of a environment inside top level to perform an autonomous action selection (task selection) and the lower level local motion planner to operate successfully inside dealing with continuously changing situations. this research deals with developing the novel reactive control architecture to provide the higher level of decision autonomy considering a auv operation that enables the single vehicle to accomplish multiple tasks inside the single mission inside a face of periodic disturbances inside the turbulent and highly uncertain environment."
"proportional fair scheduling (pfs) has been adopted as the standard solution considering fair resource allocation inside modern wireless cellular networks. with a emergence of heterogeneous networks with widely varying user loads, it was of great importance to characterize a performance of pfs under bursty traffic, which was a case inside most wireless streaming and data transfer services. inside this letter, we provide a first analytical solution to a performance of pfs under bursty on-off traffic load. we use a gaussian approximation model to derive the closed-form expression of a achievable user data rates. inside order to further improve a accuracy of our baseline analytical solution considering multi-cell networks, we design the hybrid approximation by employing multi-interference analysis. a simulation results verify that our model guarantees extremely low data rate approximation error, which was further insensitive to changes inside session duration, traffic load and user density."
"global folds between banach spaces are obtained from the simple geometric construction: the fredholm operator $t$ of index zero with one dimensional kernel was perturbed by the compatible nonlinear term $p$. a scheme encapsulates most of a known examples and suggests new ones. concrete examples rely on a positivity of an eigenfunction. considering a standard nemitskii case $p(u) = f(u)$ (but $p$ might be nonlocal, non-variational), $t$ might be a laplacian with different boundary conditions, as inside a ambrosetti-prodi theorem, or a schrödinger operators associated with a quantum harmonic oscillator or a hydrogen atom, the spectral fractional laplacian, the (nonsymmetric) markov operator. considering self-adjoint operators, we use results on a nondegeneracy of a ground state. on banach spaces, the similar role was played by the recent extension by zhang of a krein-rutman theorem."
"a paper considers a problem of planning the set of non-conflict trajectories considering a coalition of intelligent agents (mobile robots). two divergent approaches, e.g. centralized and decentralized, are surveyed and analyzed. decentralized planner - mapp was described and applied to a task of finding trajectories considering dozens uavs performing nap-of-the-earth flight inside urban environments. results of a experimental studies provide an opportunity to claim that mapp was the highly efficient planner considering solving considered types of tasks."
a purpose of this paper was to prove a equality between a algebraic iwasawa $\lambda$-invariant and a analytic iwasawa $\lambda$-invariant considering the hilbert cusp form of parallel weight $2$ at an ordinary prime $p$ when a associated residual galois representation was reducible. this was the generalization of the result of r. greenberg and v. vatsal.
we consider asymptotic normality of linear rank statistics under various randomization rules met inside clinical trials and designed considering patients' allocation into treatment and placebo arms. exposition relies on some general limit theorem due to mcleish (1974) which appears to be well suited considering a problem considered and may be employed considering other similar rules undis- cussed inside a paper. examples of applications include well known results as well as several new ones.
"rnns and their variants have been widely adopted considering image captioning. inside rnns, a production of the caption was driven by the sequence of latent states. existing captioning models usually represent latent states as vectors, taking this practice considering granted. we rethink this choice and study an alternative formulation, namely with the help of two-dimensional maps to encode latent states. this was motivated by a curiosity about the question: how a spatial structures inside a latent states affect a resultant captions? our study on mscoco and flickr30k leads to two significant observations. first, a formulation with 2d states was generally more effective inside captioning, consistently achieving higher performance with comparable parameter sizes. second, 2d states preserve spatial locality. taking advantage of this, we visually reveal a internal dynamics inside a process of caption generation, as well as a connections between input visual domain and output linguistic domain."
"this paper was a first attempt to learn a policy of an inquiry dialog system (ids) by with the help of deep reinforcement learning (drl). most ids frameworks represent dialog states and dialog acts with logical formulae. inside order to make learning inquiry dialog policies more effective, we introduce the logical formula embedding framework based on the recursive neural network. a results of experiments to evaluate a effect of 1) a drl and 2) a logical formula embedding framework show that a combination of a two are as effective or even better than existing rule-based methods considering inquiry dialog policies."
"autonomous mobile robots must operate with limited sensor horizons inside unpredictable environments. to do so, they use the receding-horizon strategy to plan trajectories, by executing the short plan while creating a next plan. however, creating safe, dynamically-feasible trajectories inside real time was challenging; and, planners must ensure that they are persistently feasible, meaning that the new trajectory was always available before a previous one has finished executing. existing approaches make the tradeoff between model complexity and planning speed, which should require sacrificing guarantees of safety and dynamic feasibility. this work presents a reachability-based trajectory design (rtd) method considering trajectory planning. rtd begins with an offline forward reachable set (frs) computation of the robot's motion while it tracks parameterized trajectories; a frs also provably bounds tracking error. at runtime, a frs was used to map obstacles to a space of parameterized trajectories, which allows rtd to select the safe trajectory at every planning iteration. rtd prescribes the method of representing obstacles to ensure that these constraints should be created and evaluated inside real time while maintaining provable safety. persistent feasibility was achieved by prescribing the minimum duration of planned trajectories, and the minimum sensor horizon. the system decomposition idea behind the method was used to increase a dimension of a parameterized trajectories inside a frs, allowing considering rtd to create more complex plans at runtime. rtd was compared inside simulation with rapidly-exploring random trees (rrt) and nonlinear model-predictive control (nmpc). rtd was also demonstrated on two hardware platforms inside randomly-crafted environments: the differential-drive segway, and the car-like rover. a proposed method was shown as safe and persistently feasible across thousands of simulations and dozens of hardware demos."
"we present the framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among the multitude of agents with the help of the semi-decentralized model. a framework extends a multi-agent learning setup by introducing the meta-controller that guides a communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. this hierarchical decomposition of a task allows considering efficient exploration to learn policies that identify globally optimal solutions even as a number of collaborating agents increases. we show promising initial experimental results on the simulated distributed scheduling problem."
"there was increasing interest inside a use of animal-like robots inside applications such as companionship and pet therapy. however, inside a majority of cases it was only a robot's physical appearance that mimics the given animal. inside contrast, miro was a first commercial biomimetic robot to be based on the hardware and software architecture that was modelled on a biological brain. this paper describes how miro's vocalisation system is designed, not with the help of pre-recorded animal sounds, but based on a implementation of the real-time parametric general-purpose mammalian vocal synthesiser tailored to a specific physical characteristics of a robot. a novel outcome has been a creation of an 'appropriate' voice considering miro that was perfectly aligned to a physical and behavioural affordances of a robot, thereby avoiding a 'uncanny valley' effect and contributing strongly to a effectiveness of miro as an interactive device."
"the lie version of turaev's $\overline{g}$-frobenius algebras from 2-dimensional homotopy quantum field theory was proposed. a foundation considering this lie version was the structure we call the \textit{$\frak{g}$-quasi-frobenius lie algebra} considering $\frak{g}$ the finite dimensional lie algebra. a latter consists of the quasi-frobenius lie algebra $(\frak{q},\beta)$ together with the left $\frak{g}$-module structure which acts on $\frak{q}$ using derivations and considering which $\beta$ was $\frak{g}$-invariant. geometrically, $\frak{g}$-quasi-frobenius lie algebras are a lie algebra structures associated to symplectic lie groups with an action by the lie group $g$ which acts using symplectic lie group automorphisms. inside addition to geometry, $\frak{g}$-quasi-frobenius lie algebras should also be motivated from a point of view of category theory. specifically, $\frak{g}$-quasi frobenius lie algebras correspond to \textit{quasi frobenius lie objects} inside $\mathbf{rep}(\frak{g})$. if $\frak{g}$ was now equipped with the lie bialgebra structure, then a categorical formulation of $\overline{g}$-frobenius algebras given inside \cite{kp} suggests that a lie version of the $\overline{g}$-frobenius algebra was the quasi-frobenius lie object inside $\mathbf{rep}(d(\frak{g}))$, where $d(\frak{g})$ was a associated (semiclassical) drinfeld double. we show that if $\frak{g}$ was the quasitriangular lie bialgebra, then every $\frak{g}$-quasi-frobenius lie algebra has an induced $d(\frak{g})$-action which gives it a structure of the $d(\frak{g})$-quasi-frobenius lie algebra."
"we revisit a classic online bin packing problem. inside this problem, items of positive sizes no larger than 1 are presented one by one to be packed into subsets called ""bins"" of total sizes no larger than 1, such that every item was assigned to the bin before a next item was presented. we use online partitioning of items into classes based on sizes, as inside previous work, but we also apply the new method where items of one class should be packed into more than two types of bins, where the bin type was defined according to a number of such items grouped together. additionally, we allow a smallest class of items to be packed inside multiple kinds of bins, and not only into their own bins. we combine this with a idea behind the method of packing of sufficiently big items according to their exact sizes. finally, we simplify a analysis of such algorithms, allowing a analysis to be based on a most standard weight functions. this simplified analysis allows us to study a algorithm which we defined based on all these ideas. this leads us to a design and analysis of a first algorithm of asymptotic competitive ratio strictly below 1.58, specifically, we break this barrier and provide an algorithm ah (advanced harmonic) whose asymptotic competitive ratio does not exceed 1.5783."
"convolutional neural networks have recently demonstrated high-quality reconstruction considering single-image super-resolution. inside this paper, we propose a laplacian pyramid super-resolution network (lapsrn) to progressively reconstruct a sub-band residuals of high-resolution images. at each pyramid level, our model takes coarse-resolution feature maps as input, predicts a high-frequency residuals, and uses transposed convolutions considering upsampling to a finer level. our method does not require a bicubic interpolation as a pre-processing step and thus dramatically reduces a computational complexity. we train a proposed lapsrn with deep supervision with the help of the robust charbonnier loss function and achieve high-quality reconstruction. furthermore, our network generates multi-scale predictions inside one feed-forward pass through a progressive reconstruction, thereby facilitates resource-aware applications. extensive quantitative and qualitative evaluations on benchmark datasets show that a proposed algorithm performs favorably against a state-of-the-art methods inside terms of speed and accuracy."
"we consider cellular base stations (bss) equipped with the large number of antennas and operating inside a unlicensed band. we denote such system as massive mimo unlicensed (mmimo-u). we design a key procedures required to guarantee coexistence between the cellular bs and nearby wi-fi devices. these include: neighboring wi-fi channel covariance estimation, allocation of spatial degrees of freedom considering interference suppression, and enhanced channel sensing and data transmission phases. we evaluate a performance of a so-designed mmimo-u, showing that it allows simultaneous cellular and wi-fi transmissions by keeping their mutual interference below a regulatory threshold. a same was not true considering conventional listen-before-talk (lbt) operations. as the result, mmimo-u boosts a aggregate cellular-plus-wi-fi data rate inside a unlicensed band with respect to conventional lbt, exhibiting increasing gains as a number of bs antennas grows."
"first a hardy and rellich inequalities are defined considering a submarkovian operator associated with the local dirichlet form. secondly, two general conditions are derived which are sufficient to deduce a rellich inequality from a hardy inequality. inside addition a rellich constant was calculated from a hardy constant. thirdly, we establish that a criteria considering a rellich inequality are verified considering the large class of weighted second-order operators on the domain $\omega\subseteq \ri^d$. a weighting near a boundary $\partial \omega$ should be different from a weighting at infinity. finally these results are applied to weighted second-order operators on $\ri^d\backslash\{0\}$ and to the general class of operators of grushin type."
"we study a problem of describing local components of height functions on abelian varieties over characteristic $0$ local fields as functions on spaces of torsors under various realisations of the $2$-step unipotent motivic fundamental group naturally associated to a defining line bundle. to this end, we present three main theorems giving such the description inside terms of a $\mathbb q_\ell$- and $\mathbb q_p$-pro-unipotent étale realisations when a base field was $p$-adic, and inside terms of a $\mathbb r$-pro-unipotent betti--de rham realisation when a base field was archimedean. inside a course of proving a $p$-adic instance of these theorems, we develop the new technique considering studying local non-abelian bloch--kato selmer sets, working with certain explicit cosimplicial group models considering these sets and with the help of methods from homotopical algebra. among other uses, these models enable us to construct the non-abelian generalisation of a bloch--kato exponential sequence under minimal conditions. on a geometric side, we also prove the number of foundational results on local constancy or analyticity of various non-abelian kummer maps considering pro-unipotent étale or betti--de rham fundamental groups of arbitrary smooth varieties."
"many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter approximation difficult. examples of unnormalised models are gibbs distributions, markov random fields, and neural network models inside unsupervised deep learning. inside previous work, a approximation principle called noise-contrastive approximation (nce) is introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. an open question was how to best choose a auxiliary noise distribution. we here propose the new method that addresses this issue. a proposed method shares with nce a idea of formulating density approximation as the supervised learning problem but inside contrast to nce, a proposed method leverages a observed data when generating noise samples. a noise should thus be generated inside the semi-automated manner. we first present a underlying theory of a new method, show that score matching emerges as the limiting case, validate a method on continuous and discrete valued synthetic data, and show that we should expect an improved performance compared to nce when a data lie inside the lower-dimensional manifold. then we demonstrate its applicability inside unsupervised deep learning by estimating the four-layer neural image model."
"multiphase co-35ni-20mo-10cr alloy mp35n was the high strength alloy with excellent corrosion resistance. its applications span chemical, medical, and food processing industries. thanks to its high modulus and high strength, it found applications inside reinforcement of ultra-high field pulsed magnets. recently, it has also been considered considering reinforcement inside superconducting wires used inside ultra-high field superconducting magnets. considering these applications, accurate measurement of its physical properties at cryogenic temperatures was very important. inside this paper, physical properties including electrical resistivity, specific heat, thermal conductivity, and magnetization of as-received and aged samples are measured from 2 to 300 k. a electrical resistivity of a aged sample was slightly higher than a as-received sample, both showing the weak linear temperature dependence inside a entire range of 2 - 300 k. a measured specific heat cp of 0.43 j/g-k at 295 k agrees with the theoretical prediction, but was significantly smaller than a values inside a literature. a thermal conductivity between 2 and 300 k was inside good agreement with a literature which was only available above 77 k. magnetic property of mp35n changes significantly with aging. a as-received sample exhibits curie paramagnetism with the curie constant c = 0.175 k. while a aged sample contains small amounts of the ferromagnetic phase even at room temperature. a measured mp35n properties will be useful considering a engineering design of pulsed magnets and superconducting magnets with the help of mp35n as reinforcement."
"a friendship paradox states that inside the social network, egos tend to have lower degree than their alters, or, ""your friends have more friends than you do"". most research has focused on a friendship paradox and its implications considering information transmission, but treating a network as static and unweighted. yet, people should dedicate only the finite fraction of their attention budget to each social interaction: the high-degree individual may have less time to dedicate to individual social links, forcing them to modulate a quantities of contact made to their different social ties. here we study a friendship paradox inside a context of differing contact volumes between egos and alters, finding the connection between contact volume and a strength of a friendship paradox. a most frequently contacted alters exhibit the less pronounced friendship paradox compared with a ego, whereas less-frequently contacted alters are more likely to be high degree and give rise to a paradox. we argue therefore considering the more nuanced version of a friendship paradox: ""your closest friends have slightly more friends than you do"", and inside certain networks even: ""your best friend has no more friends than you do"". we demonstrate that this relationship was robust, holding inside both the social media and the mobile phone dataset. these results have implications considering information transfer and influence inside social networks, which we explore with the help of the simple dynamical model."
"we present spectra of 5 ultra-diffuse galaxies (udgs) inside a vicinity of a coma cluster obtained with a multi-object double spectrograph on a large binocular telescope. we confirm 4 of these as members of a cluster, quintupling a number of spectroscopically confirmed systems. like a previously confirmed large (projected half light radius $>$ 4.6 kpc) udg, df44, a systems we targeted all have projected half light radii $> 2.9$ kpc. as such, we spectroscopically confirm the population of physically large udgs inside a coma cluster. a remaining udg was located inside a field, about $45$ mpc behind a cluster. we observe balmer and ca ii h \& k absorption lines inside all of our udg spectra. by comparing a stacked udg spectrum against stellar population synthesis models, we conclude that, on average, these udgs are composed of metal-poor stars ([fe/h] $\lesssim -1.5$). we also discover a first udg with [oii] and [oiii] emission lines within the clustered environment, demonstrating that not all cluster udgs are devoid of gas and sources of ionizing radiation."
"we present the new deep meta reinforcement learner, which we call deep episodic value iteration (devi). devi uses the deep neural network to learn the similarity metric considering the non-parametric model-based reinforcement learning algorithm. our model was trained end-to-end using back-propagation. despite being trained with the help of a model-free q-learning objective, we show that devi's model-based internal structure provides `one-shot' transfer to changes inside reward and transition structure, even considering tasks with very high-dimensional state spaces."
"inside this paper, a stability of fractional differential equations (fdes) with unknown parameters was studied. fdes bring many advantages to model a physical systems inside a nature or man-made systems inside a industry. because this representation has the property between linear differential equations and nonlinear differential equations. therefore, a designer may use a fdes to model complex systems instead of nonlinear differential equations which have hard mathematical background. with the help of a graphical based d-decomposition method, we investigate a parametric stability analysis of fdes without complicated mathematical analysis. to achieve this, stability boundaries are obtained firstly, and then a stability region set depending on a unknown parameters was found. a applicability of a presented method was shown considering some benchmark equations which are often used to verify a results of the new method. simulation examples shown that a method was simple and give reliable stability results."
"we address a problem of modeling and prediction of the set of temporal events inside a context of intelligent transportation systems. to leverage a information shared by different events, we propose the multi-task learning framework. we develop the support vector regression model considering joint learning of mutually dependent time series. it was a regularization-based multi-task learning previously developed considering a classification case and extended to time series. we discuss a relatedness of observed time series and first deploy a dynamic time warping distance measure to identify groups of similar series. then we take into account both time and scale warping and propose to align multiple time series by inferring their common latent representation. we test a proposed models on a problem of travel demand prediction inside nancy (france) public transport system and analyze a benefits of multi-task learning."
"graph filters play the key role inside processing a graph spectra of signals supported on a vertices of the graph. however, despite their widespread use, graph filters have been analyzed only inside a deterministic setting, ignoring a impact of stochastic- ity inside both a graph topology as well as a signal itself. to bridge this gap, we examine a statistical behavior of a two key filter types, finite impulse response (fir) and autoregressive moving average (arma) graph filters, when operating on random time- varying graph signals (or random graph processes) over random time-varying graphs. our analysis shows that (i) inside expectation, a filters behave as a same deterministic filters operating on the deterministic graph, being a expected graph, having as input signal the deterministic signal, being a expected signal, and (ii) there are meaningful upper bounds considering a variance of a filter output. we conclude a paper by proposing two novel ways of exploiting randomness to improve (joint graph-time) noise cancellation, as well as to reduce a computational complexity of graph filtering. as demonstrated by numerical results, these methods outperform a disjoint average and denoise algorithm, and yield the (up to) four times complexity redution, with very little difference from a optimal solution."
